{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-10-16T00:13:10.172683Z",
     "end_time": "2023-10-16T00:13:28.437098Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from Keys import openai_keys\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "openai.organization = openai_keys['organization']\n",
    "openai.api_key = openai_keys['api_key']\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "def leaky_relu(x):\n",
    "    return tf.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "def embed(X, embedding_model=\"text-embedding-ada-002\"):\n",
    "    curB = 0\n",
    "    Y = [0] * len(X)\n",
    "    while 2048 * curB < len(X):\n",
    "        Y_loc = get_embeddings(X[2048 * curB: min(2048 * (curB + 1), len(X))], embedding_model)\n",
    "        for i in range(len(Y_loc)):\n",
    "            Y[2048 * curB + i] = Y_loc[i]\n",
    "        curB += 1\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('Datasets/ForParsing/Clothing_and_Jewelry.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T19:22:36.957151Z",
     "end_time": "2023-09-25T19:22:45.375836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "with open('Datasets/ForParsing/CDs_and_Vinyl_500k.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "random.shuffle(data)\n",
    "df = pd.DataFrame(data)[['reviewText', 'overall', 'summary']]\n",
    "negatives = df[df['overall'] <= 2][:20000]\n",
    "positives = df[df['overall'] >= 4][:20000]\n",
    "neutrals = df[df['overall'] == 3][:10000]\n",
    "df = pd.concat([negatives, neutrals, positives], ignore_index=True)\n",
    "df = df[df['reviewText'].str.len() <= 15000]\n",
    "# df[\"embedding\"] = df.apply(lambda row: get_embedding(str(row['summary']) + \" : \" + str(row['reviewText']), engine=embedding_model), axis=1)\n",
    "df[\"embedding\"] = embed(list(df['reviewText'].astype(str)))\n",
    "df.to_csv(\"Datasets/Embeddings/CDs_and_Vinyl_Embeddings_50k.csv\")\n",
    "# display(df['overall'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T18:04:44.691646Z",
     "end_time": "2023-09-24T18:13:51.517446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/ForParsing/chatGPTSentiment.csv')\n",
    "df.dropna(axis=0)\n",
    "negatives = df[df['labels'] == 'bad'][:3000]\n",
    "positives = df[df['labels'] == 'good'][:3000]\n",
    "neutrals = df[df['labels'] == 'neutral'][:4000]\n",
    "df = pd.concat([negatives, neutrals, positives], ignore_index=True)\n",
    "df = df[df['tweets'].str.len() <= 15000]\n",
    "df[\"embedding\"] = embed(list(df['tweets'].astype(str)))\n",
    "df.to_csv(\"Datasets/Embeddings/chatGPTSentiment_embeddings_10k.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T17:49:01.857226Z",
     "end_time": "2023-09-24T17:50:49.496151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/ForParsing/financialData.csv')\n",
    "df = df[df['text'].str.len() <= 15000]\n",
    "df[\"embedding\"] = embed(list(df['text'].astype(str)), embedding_model)\n",
    "df.to_csv(\"Datasets/Embeddings/financeData1_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/ForParsing/AirlineSentiment.csv')\n",
    "df = df[df['airline_sentiment_confidence'] > 0.9]\n",
    "df = df[['airline_sentiment', 'text']]\n",
    "df = df[df['text'].str.len() <= 15000]\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"@VirginAmerica\", \"\"))\n",
    "df[\"embedding\"] = embed(list(df['text'].astype(str)), embedding_model)\n",
    "df.to_csv(\"Datasets/Embeddings/airline_twitter_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T18:19:04.953971Z",
     "end_time": "2023-09-24T18:20:55.809210Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/ForParsing/SentimentGOP.csv')\n",
    "df = df[df['sentiment_confidence'] > 0.9]\n",
    "df = df[['sentiment', 'text']]\n",
    "df = df[df['text'].str.len() <= 15000]\n",
    "df[\"embedding\"] = embed(list(df['text'].astype(str)), embedding_model)\n",
    "df.to_csv(\"Datasets/Embeddings/gop_twitter_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T18:23:38.860392Z",
     "end_time": "2023-09-24T18:24:28.406988Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "filepaths = ['Datasets/ForParsing/sentiment labelled sentences/yelp_labelled.txt',\n",
    "                 'Datasets/ForParsing/sentiment labelled sentences/amazon_cells_labelled.txt',\n",
    "                 'Datasets/ForParsing/sentiment labelled sentences/imdb_labelled.txt']\n",
    "\n",
    "df_list = []\n",
    "for filepath in filepaths:\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'sentiment'], sep='\\t')\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list)\n",
    "df[\"embedding\"] = embed(list(df['sentence'].astype(str)), embedding_model)\n",
    "df.to_csv(\"Datasets/Embeddings/HQ_sentences_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T18:40:42.041340Z",
     "end_time": "2023-09-24T18:41:08.868042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11276\\3345169400.py:4: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  series = pd.read_csv('Datasets/ForParsing/positive-words.txt', header=None, delimiter='¬', squeeze=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11276\\3345169400.py:4: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = pd.read_csv('Datasets/ForParsing/positive-words.txt', header=None, delimiter='¬', squeeze=True)\n"
     ]
    }
   ],
   "source": [
    "with open('Datasets/ForParsing/negative-words.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "series = pd.read_csv('Datasets/ForParsing/positive-words.txt', header=None, delimiter='¬', squeeze=True)\n",
    "series2 = pd.Series(lines)\n",
    "series2 = series2.str.replace('\\n', '')\n",
    "df1 = pd.DataFrame(series)\n",
    "df1['sentiment'] = 'positive'\n",
    "df2 = pd.DataFrame(series2)\n",
    "df2['sentiment'] = 'negative'\n",
    "df = pd.concat([df1, df2])\n",
    "df.rename(columns={0: 'word'}, inplace=True)\n",
    "df['embedding'] = embed(list(df['word']), embedding_model)\n",
    "df.to_csv(\"Datasets/Embeddings/keywords_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T17:40:12.486073Z",
     "end_time": "2023-09-24T17:41:27.125410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Embeddings/trip_advisor_embeddings_full.csv')\n",
    "df2 = pd.read_csv('Datasets/Embeddings/CDs_and_Vinyl_Embeddings_50k.csv')\n",
    "df3 = pd.read_csv('Datasets/Embeddings/chatGPTSentiment_embeddings_10k.csv')\n",
    "df4 = pd.read_csv('Datasets/Embeddings/financeData1_embeddings.csv')\n",
    "df5 = pd.read_csv('Datasets/Embeddings/financeData2_embeddings.csv')\n",
    "df6 = pd.read_csv(\"Datasets/Embeddings/keywords_embeddings.csv\")\n",
    "df7 = pd.read_csv(\"Datasets/Embeddings/airline_twitter_embeddings.csv\")\n",
    "df8 = pd.read_csv(\"Datasets/Embeddings/gop_twitter_embeddings.csv\")\n",
    "df9 = pd.read_csv(\"Datasets/Embeddings/HQ_sentences_embeddings.csv\")\n",
    "Y_df = pd.concat([df.Rating.apply(lambda x: 0 if x <= 3 else 2), df2.overall.apply(lambda x: 0 if x <= 3 else 2), df3.labels.apply(lambda x: 0 if x == \"bad\" else (1 if \"neutral\" else 2)), df4.sentiment.apply(lambda x: 0 if x == \"negative\" else (1 if \"neutral\" else 2)), df5.Sentiment.apply(lambda x: 0 if x == \"negative\" else (1 if \"neutral\" else 2)), df6.sentiment.apply(lambda x: 0 if x == \"negative\" else 2), df7.airline_sentiment.apply(lambda x: 0 if x == \"negative\" else (1 if \"neutral\" else 2)), df8.sentiment.apply(lambda x: 0 if x == \"Negative\" else (1 if \"Neutral\" else 2)), df9.sentiment.apply(lambda x: 0 if x == 0 else 2)])\n",
    "X_df = pd.concat([df.embedding, df2.embedding, df3.embedding, df4.embedding, df5.embedding, df6.embedding, df7.embedding, df8.embedding, df9.embedding])\n",
    "X_df = X_df.apply(literal_eval).apply(np.array)  # convert string to array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-16T00:13:28.445097Z",
     "end_time": "2023-10-16T00:33:50.452106Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(X_df.values), list(Y_df.values), test_size=0.05, random_state=69\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-16T01:38:00.785563Z",
     "end_time": "2023-10-16T01:38:00.862772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(n_estimators=1000)",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=1000)</pre></div></div></div></div></div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T02:44:42.008629Z",
     "end_time": "2023-09-20T07:20:21.686801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# joblib.dump(clf, 'models/embeddingsForest_4.pkl')\n",
    "\n",
    "clf = joblib.load('models/embeddingsForest_4.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T20:45:25.913857Z",
     "end_time": "2023-09-24T20:45:29.504397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      2231\n",
      "           1       0.85      0.93      0.89       842\n",
      "           2       0.90      0.88      0.89      1825\n",
      "\n",
      "    accuracy                           0.88      4898\n",
      "   macro avg       0.88      0.89      0.88      4898\n",
      "weighted avg       0.88      0.88      0.88      4898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T14:35:28.302912Z",
     "end_time": "2023-09-20T14:35:39.541318Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0], dtype=int64)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference\n",
    "clf.predict(np.array(get_embedding(\"This movie has some of the worst posters in years. This is such amateur composition and photoshopping.\", engine=embedding_model)).reshape(1, -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-03T15:57:58.278912Z",
     "end_time": "2023-09-03T15:57:59.024127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X_tensor_train = tf.convert_to_tensor(np.array(X_train), dtype=tf.float32)\n",
    "X_tensor_test = tf.convert_to_tensor(np.array(X_test), dtype=tf.float32)\n",
    "\n",
    "Y_tensor_train = tf.convert_to_tensor(pd.get_dummies(y_train).to_numpy())\n",
    "Y_tensor_test = tf.convert_to_tensor(pd.get_dummies(y_test).to_numpy())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-16T01:38:11.257803Z",
     "end_time": "2023-10-16T01:38:17.536061Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.6597 - categorical_accuracy: 0.7418\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.85962, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 5s 7ms/step - loss: 0.6577 - categorical_accuracy: 0.7428 - val_loss: 0.4150 - val_categorical_accuracy: 0.8596\n",
      "Epoch 2/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.3581 - categorical_accuracy: 0.8685\n",
      "Epoch 2: val_categorical_accuracy improved from 0.85962 to 0.87198, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.3581 - categorical_accuracy: 0.8685 - val_loss: 0.3283 - val_categorical_accuracy: 0.8720\n",
      "Epoch 3/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.3108 - categorical_accuracy: 0.8778\n",
      "Epoch 3: val_categorical_accuracy improved from 0.87198 to 0.87524, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.3108 - categorical_accuracy: 0.8778 - val_loss: 0.3052 - val_categorical_accuracy: 0.8752\n",
      "Epoch 4/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2931 - categorical_accuracy: 0.8828\n",
      "Epoch 4: val_categorical_accuracy improved from 0.87524 to 0.87815, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2931 - categorical_accuracy: 0.8828 - val_loss: 0.2942 - val_categorical_accuracy: 0.8782\n",
      "Epoch 5/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.2824 - categorical_accuracy: 0.8852\n",
      "Epoch 5: val_categorical_accuracy improved from 0.87815 to 0.88227, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2823 - categorical_accuracy: 0.8852 - val_loss: 0.2868 - val_categorical_accuracy: 0.8823\n",
      "Epoch 6/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2752 - categorical_accuracy: 0.8879\n",
      "Epoch 6: val_categorical_accuracy improved from 0.88227 to 0.88553, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2752 - categorical_accuracy: 0.8880 - val_loss: 0.2811 - val_categorical_accuracy: 0.8855\n",
      "Epoch 7/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2697 - categorical_accuracy: 0.8903\n",
      "Epoch 7: val_categorical_accuracy improved from 0.88553 to 0.88605, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2698 - categorical_accuracy: 0.8903 - val_loss: 0.2776 - val_categorical_accuracy: 0.8860\n",
      "Epoch 8/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2647 - categorical_accuracy: 0.8915\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.88605\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2647 - categorical_accuracy: 0.8915 - val_loss: 0.2733 - val_categorical_accuracy: 0.8855\n",
      "Epoch 9/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2606 - categorical_accuracy: 0.8932\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.88605\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2605 - categorical_accuracy: 0.8932 - val_loss: 0.2701 - val_categorical_accuracy: 0.8860\n",
      "Epoch 10/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2573 - categorical_accuracy: 0.8947\n",
      "Epoch 10: val_categorical_accuracy improved from 0.88605 to 0.88931, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2573 - categorical_accuracy: 0.8947 - val_loss: 0.2680 - val_categorical_accuracy: 0.8893\n",
      "Epoch 11/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.2542 - categorical_accuracy: 0.8956\n",
      "Epoch 11: val_categorical_accuracy improved from 0.88931 to 0.89102, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2543 - categorical_accuracy: 0.8955 - val_loss: 0.2648 - val_categorical_accuracy: 0.8910\n",
      "Epoch 12/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2513 - categorical_accuracy: 0.8971\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.89102\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2513 - categorical_accuracy: 0.8971 - val_loss: 0.2630 - val_categorical_accuracy: 0.8905\n",
      "Epoch 13/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2490 - categorical_accuracy: 0.8980\n",
      "Epoch 13: val_categorical_accuracy improved from 0.89102 to 0.89171, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2490 - categorical_accuracy: 0.8979 - val_loss: 0.2607 - val_categorical_accuracy: 0.8917\n",
      "Epoch 14/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2473 - categorical_accuracy: 0.8984\n",
      "Epoch 14: val_categorical_accuracy improved from 0.89171 to 0.89205, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2471 - categorical_accuracy: 0.8984 - val_loss: 0.2598 - val_categorical_accuracy: 0.8921\n",
      "Epoch 15/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2447 - categorical_accuracy: 0.8997\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.89205\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2449 - categorical_accuracy: 0.8996 - val_loss: 0.2575 - val_categorical_accuracy: 0.8917\n",
      "Epoch 16/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2428 - categorical_accuracy: 0.9009\n",
      "Epoch 16: val_categorical_accuracy improved from 0.89205 to 0.89377, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2428 - categorical_accuracy: 0.9009 - val_loss: 0.2555 - val_categorical_accuracy: 0.8938\n",
      "Epoch 17/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2409 - categorical_accuracy: 0.9017\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.89377\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2410 - categorical_accuracy: 0.9016 - val_loss: 0.2551 - val_categorical_accuracy: 0.8933\n",
      "Epoch 18/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2394 - categorical_accuracy: 0.9024\n",
      "Epoch 18: val_categorical_accuracy improved from 0.89377 to 0.89480, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2394 - categorical_accuracy: 0.9024 - val_loss: 0.2538 - val_categorical_accuracy: 0.8948\n",
      "Epoch 19/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2378 - categorical_accuracy: 0.9030\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.89480\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2378 - categorical_accuracy: 0.9030 - val_loss: 0.2525 - val_categorical_accuracy: 0.8941\n",
      "Epoch 20/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2360 - categorical_accuracy: 0.9037\n",
      "Epoch 20: val_categorical_accuracy did not improve from 0.89480\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2361 - categorical_accuracy: 0.9036 - val_loss: 0.2512 - val_categorical_accuracy: 0.8948\n",
      "Epoch 21/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2350 - categorical_accuracy: 0.9041\n",
      "Epoch 21: val_categorical_accuracy improved from 0.89480 to 0.89531, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2348 - categorical_accuracy: 0.9042 - val_loss: 0.2498 - val_categorical_accuracy: 0.8953\n",
      "Epoch 22/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2333 - categorical_accuracy: 0.9055\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.89531\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2333 - categorical_accuracy: 0.9055 - val_loss: 0.2490 - val_categorical_accuracy: 0.8941\n",
      "Epoch 23/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2321 - categorical_accuracy: 0.9055\n",
      "Epoch 23: val_categorical_accuracy did not improve from 0.89531\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2324 - categorical_accuracy: 0.9053 - val_loss: 0.2487 - val_categorical_accuracy: 0.8950\n",
      "Epoch 24/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.2308 - categorical_accuracy: 0.9061\n",
      "Epoch 24: val_categorical_accuracy improved from 0.89531 to 0.89617, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2308 - categorical_accuracy: 0.9060 - val_loss: 0.2474 - val_categorical_accuracy: 0.8962\n",
      "Epoch 25/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2299 - categorical_accuracy: 0.9063\n",
      "Epoch 25: val_categorical_accuracy improved from 0.89617 to 0.89755, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2299 - categorical_accuracy: 0.9063 - val_loss: 0.2468 - val_categorical_accuracy: 0.8975\n",
      "Epoch 26/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2289 - categorical_accuracy: 0.9066\n",
      "Epoch 26: val_categorical_accuracy did not improve from 0.89755\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2288 - categorical_accuracy: 0.9067 - val_loss: 0.2463 - val_categorical_accuracy: 0.8957\n",
      "Epoch 27/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2276 - categorical_accuracy: 0.9075\n",
      "Epoch 27: val_categorical_accuracy improved from 0.89755 to 0.89858, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2276 - categorical_accuracy: 0.9075 - val_loss: 0.2452 - val_categorical_accuracy: 0.8986\n",
      "Epoch 28/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2269 - categorical_accuracy: 0.9074\n",
      "Epoch 28: val_categorical_accuracy did not improve from 0.89858\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2269 - categorical_accuracy: 0.9074 - val_loss: 0.2446 - val_categorical_accuracy: 0.8975\n",
      "Epoch 29/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2262 - categorical_accuracy: 0.9079\n",
      "Epoch 29: val_categorical_accuracy did not improve from 0.89858\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2263 - categorical_accuracy: 0.9079 - val_loss: 0.2437 - val_categorical_accuracy: 0.8982\n",
      "Epoch 30/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2249 - categorical_accuracy: 0.9086\n",
      "Epoch 30: val_categorical_accuracy did not improve from 0.89858\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2247 - categorical_accuracy: 0.9088 - val_loss: 0.2434 - val_categorical_accuracy: 0.8963\n",
      "Epoch 31/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2239 - categorical_accuracy: 0.9088\n",
      "Epoch 31: val_categorical_accuracy did not improve from 0.89858\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2240 - categorical_accuracy: 0.9089 - val_loss: 0.2429 - val_categorical_accuracy: 0.8967\n",
      "Epoch 32/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2230 - categorical_accuracy: 0.9094\n",
      "Epoch 32: val_categorical_accuracy did not improve from 0.89858\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2229 - categorical_accuracy: 0.9094 - val_loss: 0.2435 - val_categorical_accuracy: 0.8969\n",
      "Epoch 33/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2218 - categorical_accuracy: 0.9098\n",
      "Epoch 33: val_categorical_accuracy improved from 0.89858 to 0.89875, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2221 - categorical_accuracy: 0.9097 - val_loss: 0.2416 - val_categorical_accuracy: 0.8987\n",
      "Epoch 34/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2213 - categorical_accuracy: 0.9099\n",
      "Epoch 34: val_categorical_accuracy did not improve from 0.89875\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2213 - categorical_accuracy: 0.9099 - val_loss: 0.2416 - val_categorical_accuracy: 0.8982\n",
      "Epoch 35/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2205 - categorical_accuracy: 0.9099\n",
      "Epoch 35: val_categorical_accuracy did not improve from 0.89875\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2205 - categorical_accuracy: 0.9099 - val_loss: 0.2411 - val_categorical_accuracy: 0.8967\n",
      "Epoch 36/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2199 - categorical_accuracy: 0.9103\n",
      "Epoch 36: val_categorical_accuracy improved from 0.89875 to 0.89892, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.2199 - categorical_accuracy: 0.9103 - val_loss: 0.2402 - val_categorical_accuracy: 0.8989\n",
      "Epoch 37/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2190 - categorical_accuracy: 0.9109\n",
      "Epoch 37: val_categorical_accuracy did not improve from 0.89892\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2190 - categorical_accuracy: 0.9108 - val_loss: 0.2398 - val_categorical_accuracy: 0.8970\n",
      "Epoch 38/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2178 - categorical_accuracy: 0.9114\n",
      "Epoch 38: val_categorical_accuracy improved from 0.89892 to 0.90081, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2181 - categorical_accuracy: 0.9113 - val_loss: 0.2396 - val_categorical_accuracy: 0.9008\n",
      "Epoch 39/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2176 - categorical_accuracy: 0.9112\n",
      "Epoch 39: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2175 - categorical_accuracy: 0.9113 - val_loss: 0.2390 - val_categorical_accuracy: 0.8986\n",
      "Epoch 40/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2167 - categorical_accuracy: 0.9121\n",
      "Epoch 40: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2166 - categorical_accuracy: 0.9122 - val_loss: 0.2397 - val_categorical_accuracy: 0.8981\n",
      "Epoch 41/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2160 - categorical_accuracy: 0.9124\n",
      "Epoch 41: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2162 - categorical_accuracy: 0.9123 - val_loss: 0.2391 - val_categorical_accuracy: 0.8974\n",
      "Epoch 42/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2157 - categorical_accuracy: 0.9126\n",
      "Epoch 42: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2155 - categorical_accuracy: 0.9127 - val_loss: 0.2381 - val_categorical_accuracy: 0.8981\n",
      "Epoch 43/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2143 - categorical_accuracy: 0.9128\n",
      "Epoch 43: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2143 - categorical_accuracy: 0.9128 - val_loss: 0.2378 - val_categorical_accuracy: 0.8989\n",
      "Epoch 44/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2138 - categorical_accuracy: 0.9129\n",
      "Epoch 44: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2138 - categorical_accuracy: 0.9129 - val_loss: 0.2372 - val_categorical_accuracy: 0.8986\n",
      "Epoch 45/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2130 - categorical_accuracy: 0.9132\n",
      "Epoch 45: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2132 - categorical_accuracy: 0.9132 - val_loss: 0.2371 - val_categorical_accuracy: 0.8981\n",
      "Epoch 46/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2122 - categorical_accuracy: 0.9138\n",
      "Epoch 46: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2122 - categorical_accuracy: 0.9138 - val_loss: 0.2368 - val_categorical_accuracy: 0.8994\n",
      "Epoch 47/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2116 - categorical_accuracy: 0.9137\n",
      "Epoch 47: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2117 - categorical_accuracy: 0.9137 - val_loss: 0.2369 - val_categorical_accuracy: 0.8977\n",
      "Epoch 48/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2118 - categorical_accuracy: 0.9139\n",
      "Epoch 48: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2117 - categorical_accuracy: 0.9139 - val_loss: 0.2363 - val_categorical_accuracy: 0.8991\n",
      "Epoch 49/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2110 - categorical_accuracy: 0.9139\n",
      "Epoch 49: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2108 - categorical_accuracy: 0.9141 - val_loss: 0.2358 - val_categorical_accuracy: 0.8987\n",
      "Epoch 50/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2102 - categorical_accuracy: 0.9145\n",
      "Epoch 50: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2104 - categorical_accuracy: 0.9143 - val_loss: 0.2354 - val_categorical_accuracy: 0.8993\n",
      "Epoch 51/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.2094 - categorical_accuracy: 0.9147\n",
      "Epoch 51: val_categorical_accuracy did not improve from 0.90081\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2094 - categorical_accuracy: 0.9147 - val_loss: 0.2358 - val_categorical_accuracy: 0.8999\n",
      "Epoch 52/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2087 - categorical_accuracy: 0.9148\n",
      "Epoch 52: val_categorical_accuracy improved from 0.90081 to 0.90252, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2092 - categorical_accuracy: 0.9146 - val_loss: 0.2361 - val_categorical_accuracy: 0.9025\n",
      "Epoch 53/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2086 - categorical_accuracy: 0.9151\n",
      "Epoch 53: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2087 - categorical_accuracy: 0.9150 - val_loss: 0.2347 - val_categorical_accuracy: 0.9003\n",
      "Epoch 54/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2080 - categorical_accuracy: 0.9153\n",
      "Epoch 54: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2078 - categorical_accuracy: 0.9153 - val_loss: 0.2353 - val_categorical_accuracy: 0.8998\n",
      "Epoch 55/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2075 - categorical_accuracy: 0.9158\n",
      "Epoch 55: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2076 - categorical_accuracy: 0.9157 - val_loss: 0.2348 - val_categorical_accuracy: 0.8994\n",
      "Epoch 56/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.2066 - categorical_accuracy: 0.9152\n",
      "Epoch 56: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2068 - categorical_accuracy: 0.9151 - val_loss: 0.2344 - val_categorical_accuracy: 0.9015\n",
      "Epoch 57/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2060 - categorical_accuracy: 0.9165\n",
      "Epoch 57: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2060 - categorical_accuracy: 0.9165 - val_loss: 0.2345 - val_categorical_accuracy: 0.9006\n",
      "Epoch 58/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2057 - categorical_accuracy: 0.9164\n",
      "Epoch 58: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2058 - categorical_accuracy: 0.9163 - val_loss: 0.2340 - val_categorical_accuracy: 0.9003\n",
      "Epoch 59/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2055 - categorical_accuracy: 0.9162\n",
      "Epoch 59: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2055 - categorical_accuracy: 0.9162 - val_loss: 0.2341 - val_categorical_accuracy: 0.9003\n",
      "Epoch 60/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2048 - categorical_accuracy: 0.9168\n",
      "Epoch 60: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2047 - categorical_accuracy: 0.9168 - val_loss: 0.2343 - val_categorical_accuracy: 0.9005\n",
      "Epoch 61/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2043 - categorical_accuracy: 0.9164\n",
      "Epoch 61: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2044 - categorical_accuracy: 0.9164 - val_loss: 0.2340 - val_categorical_accuracy: 0.9005\n",
      "Epoch 62/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2038 - categorical_accuracy: 0.9170\n",
      "Epoch 62: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2038 - categorical_accuracy: 0.9170 - val_loss: 0.2333 - val_categorical_accuracy: 0.9006\n",
      "Epoch 63/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.2032 - categorical_accuracy: 0.9175\n",
      "Epoch 63: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2031 - categorical_accuracy: 0.9175 - val_loss: 0.2338 - val_categorical_accuracy: 0.9010\n",
      "Epoch 64/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2030 - categorical_accuracy: 0.9174\n",
      "Epoch 64: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2030 - categorical_accuracy: 0.9174 - val_loss: 0.2332 - val_categorical_accuracy: 0.9018\n",
      "Epoch 65/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.2024 - categorical_accuracy: 0.9173\n",
      "Epoch 65: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2024 - categorical_accuracy: 0.9173 - val_loss: 0.2330 - val_categorical_accuracy: 0.9011\n",
      "Epoch 66/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.2017 - categorical_accuracy: 0.9184\n",
      "Epoch 66: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2017 - categorical_accuracy: 0.9184 - val_loss: 0.2333 - val_categorical_accuracy: 0.9013\n",
      "Epoch 67/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.2012 - categorical_accuracy: 0.9185\n",
      "Epoch 67: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2012 - categorical_accuracy: 0.9186 - val_loss: 0.2329 - val_categorical_accuracy: 0.9010\n",
      "Epoch 68/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.2011 - categorical_accuracy: 0.9179\n",
      "Epoch 68: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2010 - categorical_accuracy: 0.9180 - val_loss: 0.2330 - val_categorical_accuracy: 0.9005\n",
      "Epoch 69/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.2006 - categorical_accuracy: 0.9186\n",
      "Epoch 69: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.2005 - categorical_accuracy: 0.9187 - val_loss: 0.2326 - val_categorical_accuracy: 0.9006\n",
      "Epoch 70/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1999 - categorical_accuracy: 0.9189\n",
      "Epoch 70: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1997 - categorical_accuracy: 0.9190 - val_loss: 0.2327 - val_categorical_accuracy: 0.9006\n",
      "Epoch 71/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1999 - categorical_accuracy: 0.9187\n",
      "Epoch 71: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1999 - categorical_accuracy: 0.9187 - val_loss: 0.2340 - val_categorical_accuracy: 0.9015\n",
      "Epoch 72/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1993 - categorical_accuracy: 0.9190\n",
      "Epoch 72: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1994 - categorical_accuracy: 0.9189 - val_loss: 0.2316 - val_categorical_accuracy: 0.9017\n",
      "Epoch 73/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1988 - categorical_accuracy: 0.9188\n",
      "Epoch 73: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1987 - categorical_accuracy: 0.9189 - val_loss: 0.2317 - val_categorical_accuracy: 0.8999\n",
      "Epoch 74/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1981 - categorical_accuracy: 0.9195\n",
      "Epoch 74: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1981 - categorical_accuracy: 0.9194 - val_loss: 0.2323 - val_categorical_accuracy: 0.9018\n",
      "Epoch 75/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1977 - categorical_accuracy: 0.9198\n",
      "Epoch 75: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1977 - categorical_accuracy: 0.9198 - val_loss: 0.2319 - val_categorical_accuracy: 0.9006\n",
      "Epoch 76/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1975 - categorical_accuracy: 0.9198\n",
      "Epoch 76: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1974 - categorical_accuracy: 0.9198 - val_loss: 0.2314 - val_categorical_accuracy: 0.9017\n",
      "Epoch 77/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1972 - categorical_accuracy: 0.9195\n",
      "Epoch 77: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1971 - categorical_accuracy: 0.9195 - val_loss: 0.2322 - val_categorical_accuracy: 0.9008\n",
      "Epoch 78/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1961 - categorical_accuracy: 0.9201\n",
      "Epoch 78: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1961 - categorical_accuracy: 0.9200 - val_loss: 0.2316 - val_categorical_accuracy: 0.9011\n",
      "Epoch 79/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1956 - categorical_accuracy: 0.9206\n",
      "Epoch 79: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1956 - categorical_accuracy: 0.9206 - val_loss: 0.2319 - val_categorical_accuracy: 0.9018\n",
      "Epoch 80/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1956 - categorical_accuracy: 0.9209\n",
      "Epoch 80: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1958 - categorical_accuracy: 0.9209 - val_loss: 0.2310 - val_categorical_accuracy: 0.9018\n",
      "Epoch 81/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1953 - categorical_accuracy: 0.9207\n",
      "Epoch 81: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1952 - categorical_accuracy: 0.9207 - val_loss: 0.2312 - val_categorical_accuracy: 0.9015\n",
      "Epoch 82/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1943 - categorical_accuracy: 0.9211\n",
      "Epoch 82: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1947 - categorical_accuracy: 0.9211 - val_loss: 0.2313 - val_categorical_accuracy: 0.9017\n",
      "Epoch 83/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1946 - categorical_accuracy: 0.9211\n",
      "Epoch 83: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1946 - categorical_accuracy: 0.9210 - val_loss: 0.2308 - val_categorical_accuracy: 0.9017\n",
      "Epoch 84/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1939 - categorical_accuracy: 0.9215\n",
      "Epoch 84: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1938 - categorical_accuracy: 0.9216 - val_loss: 0.2309 - val_categorical_accuracy: 0.9018\n",
      "Epoch 85/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1934 - categorical_accuracy: 0.9212\n",
      "Epoch 85: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1933 - categorical_accuracy: 0.9213 - val_loss: 0.2312 - val_categorical_accuracy: 0.9022\n",
      "Epoch 86/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1926 - categorical_accuracy: 0.9221\n",
      "Epoch 86: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1927 - categorical_accuracy: 0.9219 - val_loss: 0.2314 - val_categorical_accuracy: 0.9025\n",
      "Epoch 87/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1930 - categorical_accuracy: 0.9216\n",
      "Epoch 87: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1928 - categorical_accuracy: 0.9217 - val_loss: 0.2304 - val_categorical_accuracy: 0.9017\n",
      "Epoch 88/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1928 - categorical_accuracy: 0.9220\n",
      "Epoch 88: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1928 - categorical_accuracy: 0.9220 - val_loss: 0.2311 - val_categorical_accuracy: 0.9024\n",
      "Epoch 89/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1924 - categorical_accuracy: 0.9224\n",
      "Epoch 89: val_categorical_accuracy did not improve from 0.90252\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1921 - categorical_accuracy: 0.9226 - val_loss: 0.2305 - val_categorical_accuracy: 0.9013\n",
      "Epoch 90/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1916 - categorical_accuracy: 0.9222\n",
      "Epoch 90: val_categorical_accuracy improved from 0.90252 to 0.90269, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1917 - categorical_accuracy: 0.9221 - val_loss: 0.2306 - val_categorical_accuracy: 0.9027\n",
      "Epoch 91/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1913 - categorical_accuracy: 0.9227\n",
      "Epoch 91: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1914 - categorical_accuracy: 0.9226 - val_loss: 0.2306 - val_categorical_accuracy: 0.9010\n",
      "Epoch 92/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1911 - categorical_accuracy: 0.9228\n",
      "Epoch 92: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1911 - categorical_accuracy: 0.9227 - val_loss: 0.2301 - val_categorical_accuracy: 0.9018\n",
      "Epoch 93/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1906 - categorical_accuracy: 0.9229\n",
      "Epoch 93: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1907 - categorical_accuracy: 0.9228 - val_loss: 0.2300 - val_categorical_accuracy: 0.9018\n",
      "Epoch 94/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1896 - categorical_accuracy: 0.9232\n",
      "Epoch 94: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1897 - categorical_accuracy: 0.9232 - val_loss: 0.2300 - val_categorical_accuracy: 0.9015\n",
      "Epoch 95/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1894 - categorical_accuracy: 0.9236\n",
      "Epoch 95: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1892 - categorical_accuracy: 0.9237 - val_loss: 0.2300 - val_categorical_accuracy: 0.9027\n",
      "Epoch 96/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1890 - categorical_accuracy: 0.9233\n",
      "Epoch 96: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1890 - categorical_accuracy: 0.9233 - val_loss: 0.2304 - val_categorical_accuracy: 0.9017\n",
      "Epoch 97/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1889 - categorical_accuracy: 0.9238\n",
      "Epoch 97: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1889 - categorical_accuracy: 0.9237 - val_loss: 0.2298 - val_categorical_accuracy: 0.9020\n",
      "Epoch 98/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1890 - categorical_accuracy: 0.9241\n",
      "Epoch 98: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1890 - categorical_accuracy: 0.9241 - val_loss: 0.2300 - val_categorical_accuracy: 0.9022\n",
      "Epoch 99/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1882 - categorical_accuracy: 0.9242\n",
      "Epoch 99: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1882 - categorical_accuracy: 0.9243 - val_loss: 0.2300 - val_categorical_accuracy: 0.9024\n",
      "Epoch 100/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1879 - categorical_accuracy: 0.9243\n",
      "Epoch 100: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1879 - categorical_accuracy: 0.9243 - val_loss: 0.2298 - val_categorical_accuracy: 0.9008\n",
      "Epoch 101/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1877 - categorical_accuracy: 0.9240\n",
      "Epoch 101: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1878 - categorical_accuracy: 0.9240 - val_loss: 0.2298 - val_categorical_accuracy: 0.9024\n",
      "Epoch 102/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1872 - categorical_accuracy: 0.9246\n",
      "Epoch 102: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1872 - categorical_accuracy: 0.9246 - val_loss: 0.2299 - val_categorical_accuracy: 0.9027\n",
      "Epoch 103/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1867 - categorical_accuracy: 0.9246\n",
      "Epoch 103: val_categorical_accuracy did not improve from 0.90269\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1868 - categorical_accuracy: 0.9246 - val_loss: 0.2303 - val_categorical_accuracy: 0.9027\n",
      "Epoch 104/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1867 - categorical_accuracy: 0.9241\n",
      "Epoch 104: val_categorical_accuracy improved from 0.90269 to 0.90287, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1867 - categorical_accuracy: 0.9241 - val_loss: 0.2298 - val_categorical_accuracy: 0.9029\n",
      "Epoch 105/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1862 - categorical_accuracy: 0.9253\n",
      "Epoch 105: val_categorical_accuracy did not improve from 0.90287\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1862 - categorical_accuracy: 0.9253 - val_loss: 0.2297 - val_categorical_accuracy: 0.9024\n",
      "Epoch 106/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1857 - categorical_accuracy: 0.9253\n",
      "Epoch 106: val_categorical_accuracy improved from 0.90287 to 0.90304, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1857 - categorical_accuracy: 0.9253 - val_loss: 0.2292 - val_categorical_accuracy: 0.9030\n",
      "Epoch 107/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1858 - categorical_accuracy: 0.9247\n",
      "Epoch 107: val_categorical_accuracy did not improve from 0.90304\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1857 - categorical_accuracy: 0.9247 - val_loss: 0.2296 - val_categorical_accuracy: 0.9025\n",
      "Epoch 108/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1852 - categorical_accuracy: 0.9250\n",
      "Epoch 108: val_categorical_accuracy did not improve from 0.90304\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1852 - categorical_accuracy: 0.9250 - val_loss: 0.2291 - val_categorical_accuracy: 0.9027\n",
      "Epoch 109/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1853 - categorical_accuracy: 0.9253\n",
      "Epoch 109: val_categorical_accuracy did not improve from 0.90304\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1853 - categorical_accuracy: 0.9253 - val_loss: 0.2295 - val_categorical_accuracy: 0.9029\n",
      "Epoch 110/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1845 - categorical_accuracy: 0.9262\n",
      "Epoch 110: val_categorical_accuracy improved from 0.90304 to 0.90355, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1844 - categorical_accuracy: 0.9262 - val_loss: 0.2296 - val_categorical_accuracy: 0.9036\n",
      "Epoch 111/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1843 - categorical_accuracy: 0.9259\n",
      "Epoch 111: val_categorical_accuracy did not improve from 0.90355\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1842 - categorical_accuracy: 0.9259 - val_loss: 0.2292 - val_categorical_accuracy: 0.9025\n",
      "Epoch 112/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1838 - categorical_accuracy: 0.9257\n",
      "Epoch 112: val_categorical_accuracy did not improve from 0.90355\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1838 - categorical_accuracy: 0.9257 - val_loss: 0.2307 - val_categorical_accuracy: 0.9036\n",
      "Epoch 113/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1838 - categorical_accuracy: 0.9254\n",
      "Epoch 113: val_categorical_accuracy did not improve from 0.90355\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1839 - categorical_accuracy: 0.9254 - val_loss: 0.2295 - val_categorical_accuracy: 0.9030\n",
      "Epoch 114/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1835 - categorical_accuracy: 0.9268\n",
      "Epoch 114: val_categorical_accuracy did not improve from 0.90355\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1835 - categorical_accuracy: 0.9268 - val_loss: 0.2298 - val_categorical_accuracy: 0.9034\n",
      "Epoch 115/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1823 - categorical_accuracy: 0.9268\n",
      "Epoch 115: val_categorical_accuracy did not improve from 0.90355\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1822 - categorical_accuracy: 0.9268 - val_loss: 0.2291 - val_categorical_accuracy: 0.9032\n",
      "Epoch 116/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1828 - categorical_accuracy: 0.9263\n",
      "Epoch 116: val_categorical_accuracy improved from 0.90355 to 0.90372, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1825 - categorical_accuracy: 0.9265 - val_loss: 0.2296 - val_categorical_accuracy: 0.9037\n",
      "Epoch 117/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1819 - categorical_accuracy: 0.9271\n",
      "Epoch 117: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1820 - categorical_accuracy: 0.9270 - val_loss: 0.2291 - val_categorical_accuracy: 0.9037\n",
      "Epoch 118/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1820 - categorical_accuracy: 0.9265\n",
      "Epoch 118: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1820 - categorical_accuracy: 0.9265 - val_loss: 0.2303 - val_categorical_accuracy: 0.9036\n",
      "Epoch 119/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1822 - categorical_accuracy: 0.9263\n",
      "Epoch 119: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1822 - categorical_accuracy: 0.9263 - val_loss: 0.2296 - val_categorical_accuracy: 0.9029\n",
      "Epoch 120/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1808 - categorical_accuracy: 0.9271\n",
      "Epoch 120: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1809 - categorical_accuracy: 0.9271 - val_loss: 0.2291 - val_categorical_accuracy: 0.9032\n",
      "Epoch 121/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1808 - categorical_accuracy: 0.9274\n",
      "Epoch 121: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1808 - categorical_accuracy: 0.9274 - val_loss: 0.2305 - val_categorical_accuracy: 0.9032\n",
      "Epoch 122/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1802 - categorical_accuracy: 0.9277\n",
      "Epoch 122: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1802 - categorical_accuracy: 0.9276 - val_loss: 0.2288 - val_categorical_accuracy: 0.9032\n",
      "Epoch 123/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1798 - categorical_accuracy: 0.9281\n",
      "Epoch 123: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1801 - categorical_accuracy: 0.9279 - val_loss: 0.2289 - val_categorical_accuracy: 0.9034\n",
      "Epoch 124/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1803 - categorical_accuracy: 0.9276\n",
      "Epoch 124: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1802 - categorical_accuracy: 0.9276 - val_loss: 0.2289 - val_categorical_accuracy: 0.9036\n",
      "Epoch 125/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1795 - categorical_accuracy: 0.9283\n",
      "Epoch 125: val_categorical_accuracy did not improve from 0.90372\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1795 - categorical_accuracy: 0.9283 - val_loss: 0.2291 - val_categorical_accuracy: 0.9037\n",
      "Epoch 126/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1793 - categorical_accuracy: 0.9279\n",
      "Epoch 126: val_categorical_accuracy improved from 0.90372 to 0.90510, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1793 - categorical_accuracy: 0.9279 - val_loss: 0.2284 - val_categorical_accuracy: 0.9051\n",
      "Epoch 127/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1790 - categorical_accuracy: 0.9284\n",
      "Epoch 127: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1788 - categorical_accuracy: 0.9285 - val_loss: 0.2290 - val_categorical_accuracy: 0.9029\n",
      "Epoch 128/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1790 - categorical_accuracy: 0.9279\n",
      "Epoch 128: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1791 - categorical_accuracy: 0.9279 - val_loss: 0.2290 - val_categorical_accuracy: 0.9027\n",
      "Epoch 129/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1776 - categorical_accuracy: 0.9288\n",
      "Epoch 129: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1776 - categorical_accuracy: 0.9288 - val_loss: 0.2285 - val_categorical_accuracy: 0.9044\n",
      "Epoch 130/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1780 - categorical_accuracy: 0.9288\n",
      "Epoch 130: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1780 - categorical_accuracy: 0.9288 - val_loss: 0.2295 - val_categorical_accuracy: 0.9046\n",
      "Epoch 131/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1784 - categorical_accuracy: 0.9282\n",
      "Epoch 131: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1782 - categorical_accuracy: 0.9281 - val_loss: 0.2290 - val_categorical_accuracy: 0.9032\n",
      "Epoch 132/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1771 - categorical_accuracy: 0.9298\n",
      "Epoch 132: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1771 - categorical_accuracy: 0.9298 - val_loss: 0.2291 - val_categorical_accuracy: 0.9030\n",
      "Epoch 133/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1775 - categorical_accuracy: 0.9286\n",
      "Epoch 133: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1775 - categorical_accuracy: 0.9285 - val_loss: 0.2290 - val_categorical_accuracy: 0.9034\n",
      "Epoch 134/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1770 - categorical_accuracy: 0.9288\n",
      "Epoch 134: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1770 - categorical_accuracy: 0.9289 - val_loss: 0.2289 - val_categorical_accuracy: 0.9042\n",
      "Epoch 135/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1764 - categorical_accuracy: 0.9291\n",
      "Epoch 135: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1764 - categorical_accuracy: 0.9291 - val_loss: 0.2288 - val_categorical_accuracy: 0.9034\n",
      "Epoch 136/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1760 - categorical_accuracy: 0.9291\n",
      "Epoch 136: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1760 - categorical_accuracy: 0.9292 - val_loss: 0.2293 - val_categorical_accuracy: 0.9034\n",
      "Epoch 137/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1754 - categorical_accuracy: 0.9300\n",
      "Epoch 137: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1754 - categorical_accuracy: 0.9300 - val_loss: 0.2291 - val_categorical_accuracy: 0.9034\n",
      "Epoch 138/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1755 - categorical_accuracy: 0.9301\n",
      "Epoch 138: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1755 - categorical_accuracy: 0.9301 - val_loss: 0.2288 - val_categorical_accuracy: 0.9048\n",
      "Epoch 139/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1755 - categorical_accuracy: 0.9300\n",
      "Epoch 139: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1754 - categorical_accuracy: 0.9300 - val_loss: 0.2285 - val_categorical_accuracy: 0.9046\n",
      "Epoch 140/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1750 - categorical_accuracy: 0.9304\n",
      "Epoch 140: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1750 - categorical_accuracy: 0.9304 - val_loss: 0.2293 - val_categorical_accuracy: 0.9039\n",
      "Epoch 141/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1749 - categorical_accuracy: 0.9299\n",
      "Epoch 141: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1748 - categorical_accuracy: 0.9299 - val_loss: 0.2292 - val_categorical_accuracy: 0.9032\n",
      "Epoch 142/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1745 - categorical_accuracy: 0.9306\n",
      "Epoch 142: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1745 - categorical_accuracy: 0.9306 - val_loss: 0.2291 - val_categorical_accuracy: 0.9041\n",
      "Epoch 143/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1734 - categorical_accuracy: 0.9313\n",
      "Epoch 143: val_categorical_accuracy did not improve from 0.90510\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1736 - categorical_accuracy: 0.9312 - val_loss: 0.2288 - val_categorical_accuracy: 0.9036\n",
      "Epoch 144/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1739 - categorical_accuracy: 0.9304\n",
      "Epoch 144: val_categorical_accuracy improved from 0.90510 to 0.90527, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1738 - categorical_accuracy: 0.9304 - val_loss: 0.2287 - val_categorical_accuracy: 0.9053\n",
      "Epoch 145/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1735 - categorical_accuracy: 0.9303\n",
      "Epoch 145: val_categorical_accuracy improved from 0.90527 to 0.90578, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 4s 8ms/step - loss: 0.1735 - categorical_accuracy: 0.9303 - val_loss: 0.2290 - val_categorical_accuracy: 0.9058\n",
      "Epoch 146/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1728 - categorical_accuracy: 0.9309\n",
      "Epoch 146: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1729 - categorical_accuracy: 0.9309 - val_loss: 0.2292 - val_categorical_accuracy: 0.9044\n",
      "Epoch 147/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9313\n",
      "Epoch 147: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1721 - categorical_accuracy: 0.9313 - val_loss: 0.2289 - val_categorical_accuracy: 0.9037\n",
      "Epoch 148/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1724 - categorical_accuracy: 0.9305\n",
      "Epoch 148: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1728 - categorical_accuracy: 0.9304 - val_loss: 0.2296 - val_categorical_accuracy: 0.9025\n",
      "Epoch 149/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1720 - categorical_accuracy: 0.9316\n",
      "Epoch 149: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1720 - categorical_accuracy: 0.9316 - val_loss: 0.2293 - val_categorical_accuracy: 0.9041\n",
      "Epoch 150/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1718 - categorical_accuracy: 0.9320\n",
      "Epoch 150: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1719 - categorical_accuracy: 0.9320 - val_loss: 0.2290 - val_categorical_accuracy: 0.9046\n",
      "Epoch 151/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1716 - categorical_accuracy: 0.9315\n",
      "Epoch 151: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1716 - categorical_accuracy: 0.9315 - val_loss: 0.2292 - val_categorical_accuracy: 0.9042\n",
      "Epoch 152/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1714 - categorical_accuracy: 0.9316\n",
      "Epoch 152: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1717 - categorical_accuracy: 0.9314 - val_loss: 0.2289 - val_categorical_accuracy: 0.9042\n",
      "Epoch 153/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1706 - categorical_accuracy: 0.9323\n",
      "Epoch 153: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1706 - categorical_accuracy: 0.9323 - val_loss: 0.2286 - val_categorical_accuracy: 0.9056\n",
      "Epoch 154/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1702 - categorical_accuracy: 0.9325\n",
      "Epoch 154: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1701 - categorical_accuracy: 0.9325 - val_loss: 0.2288 - val_categorical_accuracy: 0.9039\n",
      "Epoch 155/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1704 - categorical_accuracy: 0.9323\n",
      "Epoch 155: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1704 - categorical_accuracy: 0.9323 - val_loss: 0.2290 - val_categorical_accuracy: 0.9056\n",
      "Epoch 156/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1699 - categorical_accuracy: 0.9321\n",
      "Epoch 156: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1699 - categorical_accuracy: 0.9321 - val_loss: 0.2288 - val_categorical_accuracy: 0.9037\n",
      "Epoch 157/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1700 - categorical_accuracy: 0.9323\n",
      "Epoch 157: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1697 - categorical_accuracy: 0.9324 - val_loss: 0.2298 - val_categorical_accuracy: 0.9032\n",
      "Epoch 158/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1695 - categorical_accuracy: 0.9325\n",
      "Epoch 158: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1696 - categorical_accuracy: 0.9326 - val_loss: 0.2294 - val_categorical_accuracy: 0.9037\n",
      "Epoch 159/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1692 - categorical_accuracy: 0.9328\n",
      "Epoch 159: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1690 - categorical_accuracy: 0.9329 - val_loss: 0.2292 - val_categorical_accuracy: 0.9044\n",
      "Epoch 160/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1689 - categorical_accuracy: 0.9327\n",
      "Epoch 160: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1688 - categorical_accuracy: 0.9328 - val_loss: 0.2290 - val_categorical_accuracy: 0.9044\n",
      "Epoch 161/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1685 - categorical_accuracy: 0.9327\n",
      "Epoch 161: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1687 - categorical_accuracy: 0.9326 - val_loss: 0.2292 - val_categorical_accuracy: 0.9042\n",
      "Epoch 162/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1682 - categorical_accuracy: 0.9327\n",
      "Epoch 162: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1682 - categorical_accuracy: 0.9327 - val_loss: 0.2289 - val_categorical_accuracy: 0.9051\n",
      "Epoch 163/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1680 - categorical_accuracy: 0.9337\n",
      "Epoch 163: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1680 - categorical_accuracy: 0.9337 - val_loss: 0.2291 - val_categorical_accuracy: 0.9036\n",
      "Epoch 164/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1676 - categorical_accuracy: 0.9340\n",
      "Epoch 164: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1676 - categorical_accuracy: 0.9340 - val_loss: 0.2287 - val_categorical_accuracy: 0.9042\n",
      "Epoch 165/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1670 - categorical_accuracy: 0.9341\n",
      "Epoch 165: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1670 - categorical_accuracy: 0.9341 - val_loss: 0.2286 - val_categorical_accuracy: 0.9046\n",
      "Epoch 166/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1668 - categorical_accuracy: 0.9337\n",
      "Epoch 166: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1669 - categorical_accuracy: 0.9337 - val_loss: 0.2291 - val_categorical_accuracy: 0.9042\n",
      "Epoch 167/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1666 - categorical_accuracy: 0.9342\n",
      "Epoch 167: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1668 - categorical_accuracy: 0.9342 - val_loss: 0.2290 - val_categorical_accuracy: 0.9049\n",
      "Epoch 168/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1662 - categorical_accuracy: 0.9343\n",
      "Epoch 168: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1664 - categorical_accuracy: 0.9342 - val_loss: 0.2290 - val_categorical_accuracy: 0.9048\n",
      "Epoch 169/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1662 - categorical_accuracy: 0.9341\n",
      "Epoch 169: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1662 - categorical_accuracy: 0.9341 - val_loss: 0.2292 - val_categorical_accuracy: 0.9048\n",
      "Epoch 170/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1658 - categorical_accuracy: 0.9345\n",
      "Epoch 170: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1660 - categorical_accuracy: 0.9343 - val_loss: 0.2297 - val_categorical_accuracy: 0.9046\n",
      "Epoch 171/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1657 - categorical_accuracy: 0.9338\n",
      "Epoch 171: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1658 - categorical_accuracy: 0.9338 - val_loss: 0.2286 - val_categorical_accuracy: 0.9056\n",
      "Epoch 172/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1659 - categorical_accuracy: 0.9338\n",
      "Epoch 172: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1660 - categorical_accuracy: 0.9337 - val_loss: 0.2283 - val_categorical_accuracy: 0.9054\n",
      "Epoch 173/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1649 - categorical_accuracy: 0.9349\n",
      "Epoch 173: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1649 - categorical_accuracy: 0.9348 - val_loss: 0.2294 - val_categorical_accuracy: 0.9041\n",
      "Epoch 174/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1651 - categorical_accuracy: 0.9344\n",
      "Epoch 174: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1651 - categorical_accuracy: 0.9344 - val_loss: 0.2304 - val_categorical_accuracy: 0.9025\n",
      "Epoch 175/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1644 - categorical_accuracy: 0.9353\n",
      "Epoch 175: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1645 - categorical_accuracy: 0.9353 - val_loss: 0.2296 - val_categorical_accuracy: 0.9039\n",
      "Epoch 176/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1641 - categorical_accuracy: 0.9351\n",
      "Epoch 176: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1642 - categorical_accuracy: 0.9351 - val_loss: 0.2291 - val_categorical_accuracy: 0.9044\n",
      "Epoch 177/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1643 - categorical_accuracy: 0.9354\n",
      "Epoch 177: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1644 - categorical_accuracy: 0.9353 - val_loss: 0.2296 - val_categorical_accuracy: 0.9046\n",
      "Epoch 178/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1633 - categorical_accuracy: 0.9358\n",
      "Epoch 178: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1633 - categorical_accuracy: 0.9358 - val_loss: 0.2288 - val_categorical_accuracy: 0.9051\n",
      "Epoch 179/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1638 - categorical_accuracy: 0.9354\n",
      "Epoch 179: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1634 - categorical_accuracy: 0.9356 - val_loss: 0.2313 - val_categorical_accuracy: 0.9042\n",
      "Epoch 180/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1632 - categorical_accuracy: 0.9360\n",
      "Epoch 180: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1631 - categorical_accuracy: 0.9361 - val_loss: 0.2293 - val_categorical_accuracy: 0.9054\n",
      "Epoch 181/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1630 - categorical_accuracy: 0.9357\n",
      "Epoch 181: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1630 - categorical_accuracy: 0.9356 - val_loss: 0.2302 - val_categorical_accuracy: 0.9039\n",
      "Epoch 182/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1624 - categorical_accuracy: 0.9361\n",
      "Epoch 182: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1624 - categorical_accuracy: 0.9361 - val_loss: 0.2296 - val_categorical_accuracy: 0.9049\n",
      "Epoch 183/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1625 - categorical_accuracy: 0.9357\n",
      "Epoch 183: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1625 - categorical_accuracy: 0.9357 - val_loss: 0.2291 - val_categorical_accuracy: 0.9048\n",
      "Epoch 184/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1623 - categorical_accuracy: 0.9369\n",
      "Epoch 184: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1621 - categorical_accuracy: 0.9369 - val_loss: 0.2291 - val_categorical_accuracy: 0.9049\n",
      "Epoch 185/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1623 - categorical_accuracy: 0.9359\n",
      "Epoch 185: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1621 - categorical_accuracy: 0.9360 - val_loss: 0.2294 - val_categorical_accuracy: 0.9048\n",
      "Epoch 186/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1609 - categorical_accuracy: 0.9362\n",
      "Epoch 186: val_categorical_accuracy did not improve from 0.90578\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1612 - categorical_accuracy: 0.9360 - val_loss: 0.2295 - val_categorical_accuracy: 0.9048\n",
      "Epoch 187/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1609 - categorical_accuracy: 0.9364\n",
      "Epoch 187: val_categorical_accuracy improved from 0.90578 to 0.90630, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1610 - categorical_accuracy: 0.9364 - val_loss: 0.2293 - val_categorical_accuracy: 0.9063\n",
      "Epoch 188/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1615 - categorical_accuracy: 0.9367\n",
      "Epoch 188: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1614 - categorical_accuracy: 0.9367 - val_loss: 0.2300 - val_categorical_accuracy: 0.9046\n",
      "Epoch 189/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1605 - categorical_accuracy: 0.9372\n",
      "Epoch 189: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1605 - categorical_accuracy: 0.9372 - val_loss: 0.2292 - val_categorical_accuracy: 0.9048\n",
      "Epoch 190/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1608 - categorical_accuracy: 0.9371\n",
      "Epoch 190: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1608 - categorical_accuracy: 0.9371 - val_loss: 0.2298 - val_categorical_accuracy: 0.9056\n",
      "Epoch 191/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1606 - categorical_accuracy: 0.9370\n",
      "Epoch 191: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1608 - categorical_accuracy: 0.9370 - val_loss: 0.2295 - val_categorical_accuracy: 0.9048\n",
      "Epoch 192/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1599 - categorical_accuracy: 0.9368\n",
      "Epoch 192: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1599 - categorical_accuracy: 0.9367 - val_loss: 0.2297 - val_categorical_accuracy: 0.9060\n",
      "Epoch 193/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1597 - categorical_accuracy: 0.9372\n",
      "Epoch 193: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1597 - categorical_accuracy: 0.9372 - val_loss: 0.2295 - val_categorical_accuracy: 0.9046\n",
      "Epoch 194/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1593 - categorical_accuracy: 0.9376\n",
      "Epoch 194: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1593 - categorical_accuracy: 0.9376 - val_loss: 0.2295 - val_categorical_accuracy: 0.9051\n",
      "Epoch 195/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1594 - categorical_accuracy: 0.9377\n",
      "Epoch 195: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1593 - categorical_accuracy: 0.9377 - val_loss: 0.2297 - val_categorical_accuracy: 0.9049\n",
      "Epoch 196/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1584 - categorical_accuracy: 0.9376\n",
      "Epoch 196: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1584 - categorical_accuracy: 0.9376 - val_loss: 0.2299 - val_categorical_accuracy: 0.9054\n",
      "Epoch 197/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1584 - categorical_accuracy: 0.9381\n",
      "Epoch 197: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1585 - categorical_accuracy: 0.9380 - val_loss: 0.2299 - val_categorical_accuracy: 0.9041\n",
      "Epoch 198/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1580 - categorical_accuracy: 0.9378\n",
      "Epoch 198: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1580 - categorical_accuracy: 0.9378 - val_loss: 0.2303 - val_categorical_accuracy: 0.9053\n",
      "Epoch 199/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1578 - categorical_accuracy: 0.9384\n",
      "Epoch 199: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1576 - categorical_accuracy: 0.9384 - val_loss: 0.2305 - val_categorical_accuracy: 0.9039\n",
      "Epoch 200/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1573 - categorical_accuracy: 0.9380\n",
      "Epoch 200: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1574 - categorical_accuracy: 0.9380 - val_loss: 0.2306 - val_categorical_accuracy: 0.9041\n",
      "Epoch 201/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1573 - categorical_accuracy: 0.9382\n",
      "Epoch 201: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1572 - categorical_accuracy: 0.9382 - val_loss: 0.2297 - val_categorical_accuracy: 0.9053\n",
      "Epoch 202/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9387\n",
      "Epoch 202: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1572 - categorical_accuracy: 0.9387 - val_loss: 0.2298 - val_categorical_accuracy: 0.9051\n",
      "Epoch 203/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1565 - categorical_accuracy: 0.9387\n",
      "Epoch 203: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1565 - categorical_accuracy: 0.9387 - val_loss: 0.2303 - val_categorical_accuracy: 0.9049\n",
      "Epoch 204/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9378\n",
      "Epoch 204: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1571 - categorical_accuracy: 0.9379 - val_loss: 0.2299 - val_categorical_accuracy: 0.9056\n",
      "Epoch 205/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1566 - categorical_accuracy: 0.9390\n",
      "Epoch 205: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1566 - categorical_accuracy: 0.9389 - val_loss: 0.2304 - val_categorical_accuracy: 0.9041\n",
      "Epoch 206/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1564 - categorical_accuracy: 0.9391\n",
      "Epoch 206: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1564 - categorical_accuracy: 0.9391 - val_loss: 0.2303 - val_categorical_accuracy: 0.9037\n",
      "Epoch 207/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1563 - categorical_accuracy: 0.9387\n",
      "Epoch 207: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1562 - categorical_accuracy: 0.9387 - val_loss: 0.2302 - val_categorical_accuracy: 0.9051\n",
      "Epoch 208/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1552 - categorical_accuracy: 0.9395\n",
      "Epoch 208: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1550 - categorical_accuracy: 0.9395 - val_loss: 0.2300 - val_categorical_accuracy: 0.9046\n",
      "Epoch 209/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1549 - categorical_accuracy: 0.9400\n",
      "Epoch 209: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1550 - categorical_accuracy: 0.9400 - val_loss: 0.2300 - val_categorical_accuracy: 0.9061\n",
      "Epoch 210/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9398\n",
      "Epoch 210: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1550 - categorical_accuracy: 0.9398 - val_loss: 0.2306 - val_categorical_accuracy: 0.9058\n",
      "Epoch 211/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9388\n",
      "Epoch 211: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1552 - categorical_accuracy: 0.9388 - val_loss: 0.2309 - val_categorical_accuracy: 0.9048\n",
      "Epoch 212/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1540 - categorical_accuracy: 0.9403\n",
      "Epoch 212: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1541 - categorical_accuracy: 0.9403 - val_loss: 0.2297 - val_categorical_accuracy: 0.9048\n",
      "Epoch 213/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1543 - categorical_accuracy: 0.9396\n",
      "Epoch 213: val_categorical_accuracy did not improve from 0.90630\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1543 - categorical_accuracy: 0.9396 - val_loss: 0.2304 - val_categorical_accuracy: 0.9049\n",
      "Epoch 214/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1536 - categorical_accuracy: 0.9402\n",
      "Epoch 214: val_categorical_accuracy improved from 0.90630 to 0.90681, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1537 - categorical_accuracy: 0.9402 - val_loss: 0.2307 - val_categorical_accuracy: 0.9068\n",
      "Epoch 215/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1541 - categorical_accuracy: 0.9393\n",
      "Epoch 215: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1542 - categorical_accuracy: 0.9392 - val_loss: 0.2304 - val_categorical_accuracy: 0.9061\n",
      "Epoch 216/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1542 - categorical_accuracy: 0.9395\n",
      "Epoch 216: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1541 - categorical_accuracy: 0.9395 - val_loss: 0.2308 - val_categorical_accuracy: 0.9042\n",
      "Epoch 217/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1535 - categorical_accuracy: 0.9402\n",
      "Epoch 217: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1535 - categorical_accuracy: 0.9402 - val_loss: 0.2302 - val_categorical_accuracy: 0.9065\n",
      "Epoch 218/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1530 - categorical_accuracy: 0.9410\n",
      "Epoch 218: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1529 - categorical_accuracy: 0.9410 - val_loss: 0.2306 - val_categorical_accuracy: 0.9066\n",
      "Epoch 219/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1525 - categorical_accuracy: 0.9409\n",
      "Epoch 219: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1524 - categorical_accuracy: 0.9411 - val_loss: 0.2308 - val_categorical_accuracy: 0.9058\n",
      "Epoch 220/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1524 - categorical_accuracy: 0.9410\n",
      "Epoch 220: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1524 - categorical_accuracy: 0.9410 - val_loss: 0.2314 - val_categorical_accuracy: 0.9049\n",
      "Epoch 221/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1528 - categorical_accuracy: 0.9403\n",
      "Epoch 221: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1528 - categorical_accuracy: 0.9404 - val_loss: 0.2310 - val_categorical_accuracy: 0.9058\n",
      "Epoch 222/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1518 - categorical_accuracy: 0.9407\n",
      "Epoch 222: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1517 - categorical_accuracy: 0.9408 - val_loss: 0.2305 - val_categorical_accuracy: 0.9044\n",
      "Epoch 223/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1517 - categorical_accuracy: 0.9409\n",
      "Epoch 223: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1517 - categorical_accuracy: 0.9409 - val_loss: 0.2305 - val_categorical_accuracy: 0.9056\n",
      "Epoch 224/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1518 - categorical_accuracy: 0.9409\n",
      "Epoch 224: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1519 - categorical_accuracy: 0.9409 - val_loss: 0.2306 - val_categorical_accuracy: 0.9051\n",
      "Epoch 225/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1511 - categorical_accuracy: 0.9411\n",
      "Epoch 225: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1513 - categorical_accuracy: 0.9410 - val_loss: 0.2304 - val_categorical_accuracy: 0.9063\n",
      "Epoch 226/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1514 - categorical_accuracy: 0.9414\n",
      "Epoch 226: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1514 - categorical_accuracy: 0.9414 - val_loss: 0.2308 - val_categorical_accuracy: 0.9068\n",
      "Epoch 227/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1503 - categorical_accuracy: 0.9413\n",
      "Epoch 227: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1503 - categorical_accuracy: 0.9413 - val_loss: 0.2308 - val_categorical_accuracy: 0.9060\n",
      "Epoch 228/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1508 - categorical_accuracy: 0.9418\n",
      "Epoch 228: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1507 - categorical_accuracy: 0.9418 - val_loss: 0.2315 - val_categorical_accuracy: 0.9051\n",
      "Epoch 229/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1502 - categorical_accuracy: 0.9413\n",
      "Epoch 229: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1502 - categorical_accuracy: 0.9413 - val_loss: 0.2311 - val_categorical_accuracy: 0.9063\n",
      "Epoch 230/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1501 - categorical_accuracy: 0.9416\n",
      "Epoch 230: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1501 - categorical_accuracy: 0.9416 - val_loss: 0.2307 - val_categorical_accuracy: 0.9053\n",
      "Epoch 231/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1501 - categorical_accuracy: 0.9417\n",
      "Epoch 231: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1502 - categorical_accuracy: 0.9416 - val_loss: 0.2306 - val_categorical_accuracy: 0.9049\n",
      "Epoch 232/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1497 - categorical_accuracy: 0.9419\n",
      "Epoch 232: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1497 - categorical_accuracy: 0.9418 - val_loss: 0.2307 - val_categorical_accuracy: 0.9056\n",
      "Epoch 233/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1485 - categorical_accuracy: 0.9428\n",
      "Epoch 233: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1485 - categorical_accuracy: 0.9427 - val_loss: 0.2311 - val_categorical_accuracy: 0.9054\n",
      "Epoch 234/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1491 - categorical_accuracy: 0.9427\n",
      "Epoch 234: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1490 - categorical_accuracy: 0.9427 - val_loss: 0.2315 - val_categorical_accuracy: 0.9065\n",
      "Epoch 235/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1483 - categorical_accuracy: 0.9421\n",
      "Epoch 235: val_categorical_accuracy did not improve from 0.90681\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1483 - categorical_accuracy: 0.9421 - val_loss: 0.2309 - val_categorical_accuracy: 0.9061\n",
      "Epoch 236/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1489 - categorical_accuracy: 0.9422\n",
      "Epoch 236: val_categorical_accuracy improved from 0.90681 to 0.90801, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1488 - categorical_accuracy: 0.9423 - val_loss: 0.2317 - val_categorical_accuracy: 0.9080\n",
      "Epoch 237/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1485 - categorical_accuracy: 0.9427\n",
      "Epoch 237: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1485 - categorical_accuracy: 0.9427 - val_loss: 0.2329 - val_categorical_accuracy: 0.9060\n",
      "Epoch 238/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1478 - categorical_accuracy: 0.9430\n",
      "Epoch 238: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1479 - categorical_accuracy: 0.9430 - val_loss: 0.2315 - val_categorical_accuracy: 0.9056\n",
      "Epoch 239/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1470 - categorical_accuracy: 0.9429\n",
      "Epoch 239: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1471 - categorical_accuracy: 0.9428 - val_loss: 0.2317 - val_categorical_accuracy: 0.9054\n",
      "Epoch 240/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1475 - categorical_accuracy: 0.9431\n",
      "Epoch 240: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1475 - categorical_accuracy: 0.9431 - val_loss: 0.2318 - val_categorical_accuracy: 0.9068\n",
      "Epoch 241/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1469 - categorical_accuracy: 0.9427\n",
      "Epoch 241: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1468 - categorical_accuracy: 0.9428 - val_loss: 0.2317 - val_categorical_accuracy: 0.9061\n",
      "Epoch 242/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1470 - categorical_accuracy: 0.9430\n",
      "Epoch 242: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1470 - categorical_accuracy: 0.9430 - val_loss: 0.2323 - val_categorical_accuracy: 0.9061\n",
      "Epoch 243/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1467 - categorical_accuracy: 0.9435\n",
      "Epoch 243: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1467 - categorical_accuracy: 0.9435 - val_loss: 0.2321 - val_categorical_accuracy: 0.9049\n",
      "Epoch 244/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1463 - categorical_accuracy: 0.9433\n",
      "Epoch 244: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1464 - categorical_accuracy: 0.9432 - val_loss: 0.2313 - val_categorical_accuracy: 0.9051\n",
      "Epoch 245/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1455 - categorical_accuracy: 0.9434\n",
      "Epoch 245: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 4s 8ms/step - loss: 0.1455 - categorical_accuracy: 0.9435 - val_loss: 0.2313 - val_categorical_accuracy: 0.9061\n",
      "Epoch 246/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1462 - categorical_accuracy: 0.9434\n",
      "Epoch 246: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1462 - categorical_accuracy: 0.9433 - val_loss: 0.2316 - val_categorical_accuracy: 0.9046\n",
      "Epoch 247/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1454 - categorical_accuracy: 0.9433\n",
      "Epoch 247: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1454 - categorical_accuracy: 0.9433 - val_loss: 0.2317 - val_categorical_accuracy: 0.9046\n",
      "Epoch 248/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1450 - categorical_accuracy: 0.9445\n",
      "Epoch 248: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1451 - categorical_accuracy: 0.9444 - val_loss: 0.2320 - val_categorical_accuracy: 0.9042\n",
      "Epoch 249/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1449 - categorical_accuracy: 0.9437\n",
      "Epoch 249: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 4s 8ms/step - loss: 0.1447 - categorical_accuracy: 0.9438 - val_loss: 0.2321 - val_categorical_accuracy: 0.9066\n",
      "Epoch 250/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1443 - categorical_accuracy: 0.9440\n",
      "Epoch 250: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1442 - categorical_accuracy: 0.9440 - val_loss: 0.2324 - val_categorical_accuracy: 0.9080\n",
      "Epoch 251/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1441 - categorical_accuracy: 0.9443\n",
      "Epoch 251: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1441 - categorical_accuracy: 0.9443 - val_loss: 0.2331 - val_categorical_accuracy: 0.9051\n",
      "Epoch 252/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1443 - categorical_accuracy: 0.9442\n",
      "Epoch 252: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1443 - categorical_accuracy: 0.9442 - val_loss: 0.2326 - val_categorical_accuracy: 0.9049\n",
      "Epoch 253/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1443 - categorical_accuracy: 0.9446\n",
      "Epoch 253: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1442 - categorical_accuracy: 0.9447 - val_loss: 0.2325 - val_categorical_accuracy: 0.9054\n",
      "Epoch 254/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1432 - categorical_accuracy: 0.9453\n",
      "Epoch 254: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1432 - categorical_accuracy: 0.9453 - val_loss: 0.2320 - val_categorical_accuracy: 0.9060\n",
      "Epoch 255/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1437 - categorical_accuracy: 0.9446\n",
      "Epoch 255: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1437 - categorical_accuracy: 0.9446 - val_loss: 0.2328 - val_categorical_accuracy: 0.9046\n",
      "Epoch 256/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1433 - categorical_accuracy: 0.9453\n",
      "Epoch 256: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1432 - categorical_accuracy: 0.9453 - val_loss: 0.2318 - val_categorical_accuracy: 0.9061\n",
      "Epoch 257/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1427 - categorical_accuracy: 0.9447\n",
      "Epoch 257: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1427 - categorical_accuracy: 0.9447 - val_loss: 0.2321 - val_categorical_accuracy: 0.9066\n",
      "Epoch 258/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1425 - categorical_accuracy: 0.9457\n",
      "Epoch 258: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1424 - categorical_accuracy: 0.9458 - val_loss: 0.2324 - val_categorical_accuracy: 0.9061\n",
      "Epoch 259/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1421 - categorical_accuracy: 0.9455\n",
      "Epoch 259: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1421 - categorical_accuracy: 0.9454 - val_loss: 0.2319 - val_categorical_accuracy: 0.9054\n",
      "Epoch 260/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1422 - categorical_accuracy: 0.9453\n",
      "Epoch 260: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1424 - categorical_accuracy: 0.9453 - val_loss: 0.2321 - val_categorical_accuracy: 0.9053\n",
      "Epoch 261/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1418 - categorical_accuracy: 0.9452\n",
      "Epoch 261: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 4s 8ms/step - loss: 0.1418 - categorical_accuracy: 0.9452 - val_loss: 0.2323 - val_categorical_accuracy: 0.9068\n",
      "Epoch 262/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1421 - categorical_accuracy: 0.9457\n",
      "Epoch 262: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1418 - categorical_accuracy: 0.9458 - val_loss: 0.2327 - val_categorical_accuracy: 0.9061\n",
      "Epoch 263/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1416 - categorical_accuracy: 0.9459\n",
      "Epoch 263: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1416 - categorical_accuracy: 0.9459 - val_loss: 0.2332 - val_categorical_accuracy: 0.9053\n",
      "Epoch 264/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1415 - categorical_accuracy: 0.9457\n",
      "Epoch 264: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1415 - categorical_accuracy: 0.9457 - val_loss: 0.2317 - val_categorical_accuracy: 0.9068\n",
      "Epoch 265/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1402 - categorical_accuracy: 0.9470\n",
      "Epoch 265: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1402 - categorical_accuracy: 0.9470 - val_loss: 0.2327 - val_categorical_accuracy: 0.9051\n",
      "Epoch 266/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1406 - categorical_accuracy: 0.9458\n",
      "Epoch 266: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1406 - categorical_accuracy: 0.9457 - val_loss: 0.2329 - val_categorical_accuracy: 0.9061\n",
      "Epoch 267/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1400 - categorical_accuracy: 0.9466\n",
      "Epoch 267: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1400 - categorical_accuracy: 0.9466 - val_loss: 0.2327 - val_categorical_accuracy: 0.9054\n",
      "Epoch 268/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1407 - categorical_accuracy: 0.9463\n",
      "Epoch 268: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1407 - categorical_accuracy: 0.9463 - val_loss: 0.2330 - val_categorical_accuracy: 0.9072\n",
      "Epoch 269/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1399 - categorical_accuracy: 0.9467\n",
      "Epoch 269: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1399 - categorical_accuracy: 0.9467 - val_loss: 0.2334 - val_categorical_accuracy: 0.9051\n",
      "Epoch 270/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1396 - categorical_accuracy: 0.9468\n",
      "Epoch 270: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1396 - categorical_accuracy: 0.9467 - val_loss: 0.2334 - val_categorical_accuracy: 0.9061\n",
      "Epoch 271/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1395 - categorical_accuracy: 0.9465\n",
      "Epoch 271: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1395 - categorical_accuracy: 0.9465 - val_loss: 0.2332 - val_categorical_accuracy: 0.9048\n",
      "Epoch 272/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1390 - categorical_accuracy: 0.9466\n",
      "Epoch 272: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1390 - categorical_accuracy: 0.9467 - val_loss: 0.2334 - val_categorical_accuracy: 0.9042\n",
      "Epoch 273/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1392 - categorical_accuracy: 0.9470\n",
      "Epoch 273: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1392 - categorical_accuracy: 0.9470 - val_loss: 0.2332 - val_categorical_accuracy: 0.9073\n",
      "Epoch 274/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1387 - categorical_accuracy: 0.9474\n",
      "Epoch 274: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1386 - categorical_accuracy: 0.9474 - val_loss: 0.2335 - val_categorical_accuracy: 0.9066\n",
      "Epoch 275/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1386 - categorical_accuracy: 0.9468\n",
      "Epoch 275: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1385 - categorical_accuracy: 0.9469 - val_loss: 0.2328 - val_categorical_accuracy: 0.9063\n",
      "Epoch 276/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1386 - categorical_accuracy: 0.9470\n",
      "Epoch 276: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1386 - categorical_accuracy: 0.9470 - val_loss: 0.2340 - val_categorical_accuracy: 0.9072\n",
      "Epoch 277/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1384 - categorical_accuracy: 0.9469\n",
      "Epoch 277: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1384 - categorical_accuracy: 0.9469 - val_loss: 0.2330 - val_categorical_accuracy: 0.9060\n",
      "Epoch 278/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1378 - categorical_accuracy: 0.9473\n",
      "Epoch 278: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1380 - categorical_accuracy: 0.9473 - val_loss: 0.2332 - val_categorical_accuracy: 0.9051\n",
      "Epoch 279/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1375 - categorical_accuracy: 0.9469\n",
      "Epoch 279: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1376 - categorical_accuracy: 0.9469 - val_loss: 0.2335 - val_categorical_accuracy: 0.9051\n",
      "Epoch 280/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1373 - categorical_accuracy: 0.9480\n",
      "Epoch 280: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1371 - categorical_accuracy: 0.9481 - val_loss: 0.2341 - val_categorical_accuracy: 0.9046\n",
      "Epoch 281/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1370 - categorical_accuracy: 0.9480\n",
      "Epoch 281: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1370 - categorical_accuracy: 0.9480 - val_loss: 0.2333 - val_categorical_accuracy: 0.9063\n",
      "Epoch 282/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1369 - categorical_accuracy: 0.9484\n",
      "Epoch 282: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1369 - categorical_accuracy: 0.9483 - val_loss: 0.2340 - val_categorical_accuracy: 0.9066\n",
      "Epoch 283/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1367 - categorical_accuracy: 0.9477\n",
      "Epoch 283: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1367 - categorical_accuracy: 0.9477 - val_loss: 0.2343 - val_categorical_accuracy: 0.9049\n",
      "Epoch 284/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1361 - categorical_accuracy: 0.9485\n",
      "Epoch 284: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1361 - categorical_accuracy: 0.9484 - val_loss: 0.2339 - val_categorical_accuracy: 0.9037\n",
      "Epoch 285/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1359 - categorical_accuracy: 0.9484\n",
      "Epoch 285: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1359 - categorical_accuracy: 0.9484 - val_loss: 0.2342 - val_categorical_accuracy: 0.9049\n",
      "Epoch 286/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1361 - categorical_accuracy: 0.9479\n",
      "Epoch 286: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1362 - categorical_accuracy: 0.9478 - val_loss: 0.2342 - val_categorical_accuracy: 0.9063\n",
      "Epoch 287/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1359 - categorical_accuracy: 0.9481\n",
      "Epoch 287: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1360 - categorical_accuracy: 0.9480 - val_loss: 0.2352 - val_categorical_accuracy: 0.9061\n",
      "Epoch 288/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1357 - categorical_accuracy: 0.9484\n",
      "Epoch 288: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1357 - categorical_accuracy: 0.9484 - val_loss: 0.2338 - val_categorical_accuracy: 0.9058\n",
      "Epoch 289/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1352 - categorical_accuracy: 0.9487\n",
      "Epoch 289: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1352 - categorical_accuracy: 0.9488 - val_loss: 0.2345 - val_categorical_accuracy: 0.9061\n",
      "Epoch 290/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1341 - categorical_accuracy: 0.9494\n",
      "Epoch 290: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1342 - categorical_accuracy: 0.9494 - val_loss: 0.2346 - val_categorical_accuracy: 0.9061\n",
      "Epoch 291/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1345 - categorical_accuracy: 0.9483\n",
      "Epoch 291: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1345 - categorical_accuracy: 0.9483 - val_loss: 0.2341 - val_categorical_accuracy: 0.9056\n",
      "Epoch 292/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1343 - categorical_accuracy: 0.9491\n",
      "Epoch 292: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1342 - categorical_accuracy: 0.9491 - val_loss: 0.2336 - val_categorical_accuracy: 0.9072\n",
      "Epoch 293/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1335 - categorical_accuracy: 0.9494\n",
      "Epoch 293: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1336 - categorical_accuracy: 0.9494 - val_loss: 0.2345 - val_categorical_accuracy: 0.9041\n",
      "Epoch 294/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1340 - categorical_accuracy: 0.9492\n",
      "Epoch 294: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1339 - categorical_accuracy: 0.9492 - val_loss: 0.2341 - val_categorical_accuracy: 0.9056\n",
      "Epoch 295/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1343 - categorical_accuracy: 0.9490\n",
      "Epoch 295: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1343 - categorical_accuracy: 0.9490 - val_loss: 0.2342 - val_categorical_accuracy: 0.9068\n",
      "Epoch 296/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1332 - categorical_accuracy: 0.9494\n",
      "Epoch 296: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1332 - categorical_accuracy: 0.9494 - val_loss: 0.2344 - val_categorical_accuracy: 0.9065\n",
      "Epoch 297/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1332 - categorical_accuracy: 0.9491\n",
      "Epoch 297: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1332 - categorical_accuracy: 0.9491 - val_loss: 0.2341 - val_categorical_accuracy: 0.9061\n",
      "Epoch 298/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1328 - categorical_accuracy: 0.9497\n",
      "Epoch 298: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1328 - categorical_accuracy: 0.9496 - val_loss: 0.2356 - val_categorical_accuracy: 0.9060\n",
      "Epoch 299/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1328 - categorical_accuracy: 0.9499\n",
      "Epoch 299: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1328 - categorical_accuracy: 0.9499 - val_loss: 0.2359 - val_categorical_accuracy: 0.9060\n",
      "Epoch 300/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1325 - categorical_accuracy: 0.9501\n",
      "Epoch 300: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1325 - categorical_accuracy: 0.9501 - val_loss: 0.2349 - val_categorical_accuracy: 0.9061\n",
      "Epoch 301/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1328 - categorical_accuracy: 0.9498\n",
      "Epoch 301: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1327 - categorical_accuracy: 0.9498 - val_loss: 0.2357 - val_categorical_accuracy: 0.9049\n",
      "Epoch 302/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1319 - categorical_accuracy: 0.9500\n",
      "Epoch 302: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1319 - categorical_accuracy: 0.9500 - val_loss: 0.2346 - val_categorical_accuracy: 0.9061\n",
      "Epoch 303/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1320 - categorical_accuracy: 0.9502\n",
      "Epoch 303: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1320 - categorical_accuracy: 0.9502 - val_loss: 0.2345 - val_categorical_accuracy: 0.9063\n",
      "Epoch 304/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1319 - categorical_accuracy: 0.9502\n",
      "Epoch 304: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1319 - categorical_accuracy: 0.9502 - val_loss: 0.2349 - val_categorical_accuracy: 0.9048\n",
      "Epoch 305/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1320 - categorical_accuracy: 0.9495\n",
      "Epoch 305: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1321 - categorical_accuracy: 0.9495 - val_loss: 0.2344 - val_categorical_accuracy: 0.9063\n",
      "Epoch 306/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1313 - categorical_accuracy: 0.9500\n",
      "Epoch 306: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1313 - categorical_accuracy: 0.9500 - val_loss: 0.2356 - val_categorical_accuracy: 0.9051\n",
      "Epoch 307/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1315 - categorical_accuracy: 0.9504\n",
      "Epoch 307: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1314 - categorical_accuracy: 0.9505 - val_loss: 0.2355 - val_categorical_accuracy: 0.9054\n",
      "Epoch 308/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1310 - categorical_accuracy: 0.9505\n",
      "Epoch 308: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1309 - categorical_accuracy: 0.9505 - val_loss: 0.2351 - val_categorical_accuracy: 0.9073\n",
      "Epoch 309/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1302 - categorical_accuracy: 0.9511\n",
      "Epoch 309: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1302 - categorical_accuracy: 0.9512 - val_loss: 0.2356 - val_categorical_accuracy: 0.9054\n",
      "Epoch 310/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1307 - categorical_accuracy: 0.9508\n",
      "Epoch 310: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1307 - categorical_accuracy: 0.9508 - val_loss: 0.2357 - val_categorical_accuracy: 0.9063\n",
      "Epoch 311/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1305 - categorical_accuracy: 0.9505\n",
      "Epoch 311: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1303 - categorical_accuracy: 0.9505 - val_loss: 0.2354 - val_categorical_accuracy: 0.9054\n",
      "Epoch 312/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1289 - categorical_accuracy: 0.9509\n",
      "Epoch 312: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1289 - categorical_accuracy: 0.9509 - val_loss: 0.2355 - val_categorical_accuracy: 0.9058\n",
      "Epoch 313/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1295 - categorical_accuracy: 0.9514\n",
      "Epoch 313: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1295 - categorical_accuracy: 0.9514 - val_loss: 0.2356 - val_categorical_accuracy: 0.9060\n",
      "Epoch 314/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1294 - categorical_accuracy: 0.9510\n",
      "Epoch 314: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1295 - categorical_accuracy: 0.9510 - val_loss: 0.2352 - val_categorical_accuracy: 0.9066\n",
      "Epoch 315/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1293 - categorical_accuracy: 0.9512\n",
      "Epoch 315: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1293 - categorical_accuracy: 0.9513 - val_loss: 0.2357 - val_categorical_accuracy: 0.9061\n",
      "Epoch 316/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1283 - categorical_accuracy: 0.9521\n",
      "Epoch 316: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1283 - categorical_accuracy: 0.9521 - val_loss: 0.2358 - val_categorical_accuracy: 0.9056\n",
      "Epoch 317/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1282 - categorical_accuracy: 0.9518\n",
      "Epoch 317: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1280 - categorical_accuracy: 0.9519 - val_loss: 0.2358 - val_categorical_accuracy: 0.9053\n",
      "Epoch 318/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1278 - categorical_accuracy: 0.9516\n",
      "Epoch 318: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1277 - categorical_accuracy: 0.9516 - val_loss: 0.2356 - val_categorical_accuracy: 0.9053\n",
      "Epoch 319/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1285 - categorical_accuracy: 0.9519\n",
      "Epoch 319: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1284 - categorical_accuracy: 0.9520 - val_loss: 0.2359 - val_categorical_accuracy: 0.9061\n",
      "Epoch 320/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1287 - categorical_accuracy: 0.9515\n",
      "Epoch 320: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1287 - categorical_accuracy: 0.9515 - val_loss: 0.2357 - val_categorical_accuracy: 0.9061\n",
      "Epoch 321/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1277 - categorical_accuracy: 0.9523\n",
      "Epoch 321: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1278 - categorical_accuracy: 0.9522 - val_loss: 0.2366 - val_categorical_accuracy: 0.9058\n",
      "Epoch 322/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1273 - categorical_accuracy: 0.9520\n",
      "Epoch 322: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1274 - categorical_accuracy: 0.9519 - val_loss: 0.2368 - val_categorical_accuracy: 0.9063\n",
      "Epoch 323/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1268 - categorical_accuracy: 0.9520\n",
      "Epoch 323: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1268 - categorical_accuracy: 0.9520 - val_loss: 0.2366 - val_categorical_accuracy: 0.9054\n",
      "Epoch 324/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1275 - categorical_accuracy: 0.9517\n",
      "Epoch 324: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1275 - categorical_accuracy: 0.9517 - val_loss: 0.2366 - val_categorical_accuracy: 0.9065\n",
      "Epoch 325/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1268 - categorical_accuracy: 0.9523\n",
      "Epoch 325: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1268 - categorical_accuracy: 0.9523 - val_loss: 0.2362 - val_categorical_accuracy: 0.9058\n",
      "Epoch 326/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1264 - categorical_accuracy: 0.9523\n",
      "Epoch 326: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1265 - categorical_accuracy: 0.9523 - val_loss: 0.2371 - val_categorical_accuracy: 0.9058\n",
      "Epoch 327/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1270 - categorical_accuracy: 0.9527\n",
      "Epoch 327: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1269 - categorical_accuracy: 0.9527 - val_loss: 0.2369 - val_categorical_accuracy: 0.9070\n",
      "Epoch 328/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1261 - categorical_accuracy: 0.9525\n",
      "Epoch 328: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1260 - categorical_accuracy: 0.9525 - val_loss: 0.2369 - val_categorical_accuracy: 0.9070\n",
      "Epoch 329/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1261 - categorical_accuracy: 0.9532\n",
      "Epoch 329: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1261 - categorical_accuracy: 0.9532 - val_loss: 0.2366 - val_categorical_accuracy: 0.9042\n",
      "Epoch 330/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1253 - categorical_accuracy: 0.9529\n",
      "Epoch 330: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1255 - categorical_accuracy: 0.9528 - val_loss: 0.2376 - val_categorical_accuracy: 0.9066\n",
      "Epoch 331/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1255 - categorical_accuracy: 0.9530\n",
      "Epoch 331: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1254 - categorical_accuracy: 0.9530 - val_loss: 0.2388 - val_categorical_accuracy: 0.9056\n",
      "Epoch 332/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1252 - categorical_accuracy: 0.9533\n",
      "Epoch 332: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1252 - categorical_accuracy: 0.9532 - val_loss: 0.2382 - val_categorical_accuracy: 0.9060\n",
      "Epoch 333/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1254 - categorical_accuracy: 0.9528\n",
      "Epoch 333: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1254 - categorical_accuracy: 0.9528 - val_loss: 0.2378 - val_categorical_accuracy: 0.9061\n",
      "Epoch 334/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1246 - categorical_accuracy: 0.9533\n",
      "Epoch 334: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1245 - categorical_accuracy: 0.9534 - val_loss: 0.2367 - val_categorical_accuracy: 0.9068\n",
      "Epoch 335/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1244 - categorical_accuracy: 0.9533\n",
      "Epoch 335: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1245 - categorical_accuracy: 0.9533 - val_loss: 0.2379 - val_categorical_accuracy: 0.9065\n",
      "Epoch 336/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1248 - categorical_accuracy: 0.9534\n",
      "Epoch 336: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1248 - categorical_accuracy: 0.9534 - val_loss: 0.2376 - val_categorical_accuracy: 0.9061\n",
      "Epoch 337/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1245 - categorical_accuracy: 0.9536\n",
      "Epoch 337: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1244 - categorical_accuracy: 0.9536 - val_loss: 0.2375 - val_categorical_accuracy: 0.9066\n",
      "Epoch 338/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1232 - categorical_accuracy: 0.9542\n",
      "Epoch 338: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1235 - categorical_accuracy: 0.9541 - val_loss: 0.2372 - val_categorical_accuracy: 0.9060\n",
      "Epoch 339/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1237 - categorical_accuracy: 0.9530\n",
      "Epoch 339: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1238 - categorical_accuracy: 0.9530 - val_loss: 0.2372 - val_categorical_accuracy: 0.9054\n",
      "Epoch 340/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1240 - categorical_accuracy: 0.9537\n",
      "Epoch 340: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1241 - categorical_accuracy: 0.9537 - val_loss: 0.2390 - val_categorical_accuracy: 0.9066\n",
      "Epoch 341/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1232 - categorical_accuracy: 0.9540\n",
      "Epoch 341: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1233 - categorical_accuracy: 0.9539 - val_loss: 0.2384 - val_categorical_accuracy: 0.9060\n",
      "Epoch 342/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1230 - categorical_accuracy: 0.9539\n",
      "Epoch 342: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1231 - categorical_accuracy: 0.9539 - val_loss: 0.2371 - val_categorical_accuracy: 0.9054\n",
      "Epoch 343/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1226 - categorical_accuracy: 0.9539\n",
      "Epoch 343: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1227 - categorical_accuracy: 0.9539 - val_loss: 0.2382 - val_categorical_accuracy: 0.9054\n",
      "Epoch 344/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1219 - categorical_accuracy: 0.9543\n",
      "Epoch 344: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1220 - categorical_accuracy: 0.9542 - val_loss: 0.2380 - val_categorical_accuracy: 0.9061\n",
      "Epoch 345/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1221 - categorical_accuracy: 0.9542\n",
      "Epoch 345: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1222 - categorical_accuracy: 0.9541 - val_loss: 0.2378 - val_categorical_accuracy: 0.9060\n",
      "Epoch 346/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1214 - categorical_accuracy: 0.9549\n",
      "Epoch 346: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1214 - categorical_accuracy: 0.9549 - val_loss: 0.2386 - val_categorical_accuracy: 0.9070\n",
      "Epoch 347/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1213 - categorical_accuracy: 0.9547\n",
      "Epoch 347: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1213 - categorical_accuracy: 0.9547 - val_loss: 0.2385 - val_categorical_accuracy: 0.9068\n",
      "Epoch 348/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1219 - categorical_accuracy: 0.9552\n",
      "Epoch 348: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1219 - categorical_accuracy: 0.9552 - val_loss: 0.2384 - val_categorical_accuracy: 0.9060\n",
      "Epoch 349/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1216 - categorical_accuracy: 0.9545\n",
      "Epoch 349: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1217 - categorical_accuracy: 0.9545 - val_loss: 0.2379 - val_categorical_accuracy: 0.9049\n",
      "Epoch 350/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1207 - categorical_accuracy: 0.9552\n",
      "Epoch 350: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1207 - categorical_accuracy: 0.9552 - val_loss: 0.2385 - val_categorical_accuracy: 0.9077\n",
      "Epoch 351/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1208 - categorical_accuracy: 0.9554\n",
      "Epoch 351: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1208 - categorical_accuracy: 0.9554 - val_loss: 0.2401 - val_categorical_accuracy: 0.9068\n",
      "Epoch 352/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1197 - categorical_accuracy: 0.9554\n",
      "Epoch 352: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1197 - categorical_accuracy: 0.9554 - val_loss: 0.2387 - val_categorical_accuracy: 0.9075\n",
      "Epoch 353/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1203 - categorical_accuracy: 0.9552\n",
      "Epoch 353: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1203 - categorical_accuracy: 0.9552 - val_loss: 0.2398 - val_categorical_accuracy: 0.9063\n",
      "Epoch 354/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1196 - categorical_accuracy: 0.9558\n",
      "Epoch 354: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1197 - categorical_accuracy: 0.9558 - val_loss: 0.2393 - val_categorical_accuracy: 0.9058\n",
      "Epoch 355/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1193 - categorical_accuracy: 0.9553\n",
      "Epoch 355: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1195 - categorical_accuracy: 0.9553 - val_loss: 0.2384 - val_categorical_accuracy: 0.9061\n",
      "Epoch 356/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1210 - categorical_accuracy: 0.9550\n",
      "Epoch 356: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1211 - categorical_accuracy: 0.9550 - val_loss: 0.2387 - val_categorical_accuracy: 0.9060\n",
      "Epoch 357/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1187 - categorical_accuracy: 0.9559\n",
      "Epoch 357: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1188 - categorical_accuracy: 0.9559 - val_loss: 0.2395 - val_categorical_accuracy: 0.9060\n",
      "Epoch 358/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1197 - categorical_accuracy: 0.9557\n",
      "Epoch 358: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1197 - categorical_accuracy: 0.9557 - val_loss: 0.2389 - val_categorical_accuracy: 0.9060\n",
      "Epoch 359/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1193 - categorical_accuracy: 0.9555\n",
      "Epoch 359: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1193 - categorical_accuracy: 0.9555 - val_loss: 0.2396 - val_categorical_accuracy: 0.9073\n",
      "Epoch 360/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1191 - categorical_accuracy: 0.9562\n",
      "Epoch 360: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1192 - categorical_accuracy: 0.9561 - val_loss: 0.2401 - val_categorical_accuracy: 0.9058\n",
      "Epoch 361/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1182 - categorical_accuracy: 0.9560\n",
      "Epoch 361: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1181 - categorical_accuracy: 0.9560 - val_loss: 0.2404 - val_categorical_accuracy: 0.9048\n",
      "Epoch 362/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1184 - categorical_accuracy: 0.9560\n",
      "Epoch 362: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1186 - categorical_accuracy: 0.9559 - val_loss: 0.2403 - val_categorical_accuracy: 0.9048\n",
      "Epoch 363/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1182 - categorical_accuracy: 0.9563\n",
      "Epoch 363: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1182 - categorical_accuracy: 0.9563 - val_loss: 0.2403 - val_categorical_accuracy: 0.9053\n",
      "Epoch 364/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1173 - categorical_accuracy: 0.9569\n",
      "Epoch 364: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1172 - categorical_accuracy: 0.9568 - val_loss: 0.2393 - val_categorical_accuracy: 0.9058\n",
      "Epoch 365/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1180 - categorical_accuracy: 0.9566\n",
      "Epoch 365: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1179 - categorical_accuracy: 0.9566 - val_loss: 0.2399 - val_categorical_accuracy: 0.9070\n",
      "Epoch 366/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1172 - categorical_accuracy: 0.9560\n",
      "Epoch 366: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1175 - categorical_accuracy: 0.9559 - val_loss: 0.2400 - val_categorical_accuracy: 0.9053\n",
      "Epoch 367/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1178 - categorical_accuracy: 0.9567\n",
      "Epoch 367: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1178 - categorical_accuracy: 0.9566 - val_loss: 0.2403 - val_categorical_accuracy: 0.9049\n",
      "Epoch 368/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1169 - categorical_accuracy: 0.9570\n",
      "Epoch 368: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1169 - categorical_accuracy: 0.9570 - val_loss: 0.2409 - val_categorical_accuracy: 0.9051\n",
      "Epoch 369/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1168 - categorical_accuracy: 0.9563\n",
      "Epoch 369: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1169 - categorical_accuracy: 0.9563 - val_loss: 0.2390 - val_categorical_accuracy: 0.9065\n",
      "Epoch 370/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1172 - categorical_accuracy: 0.9567\n",
      "Epoch 370: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1170 - categorical_accuracy: 0.9568 - val_loss: 0.2401 - val_categorical_accuracy: 0.9065\n",
      "Epoch 371/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1166 - categorical_accuracy: 0.9570\n",
      "Epoch 371: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 8ms/step - loss: 0.1167 - categorical_accuracy: 0.9569 - val_loss: 0.2415 - val_categorical_accuracy: 0.9072\n",
      "Epoch 372/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1162 - categorical_accuracy: 0.9569\n",
      "Epoch 372: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1161 - categorical_accuracy: 0.9569 - val_loss: 0.2402 - val_categorical_accuracy: 0.9060\n",
      "Epoch 373/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1164 - categorical_accuracy: 0.9572\n",
      "Epoch 373: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1164 - categorical_accuracy: 0.9572 - val_loss: 0.2408 - val_categorical_accuracy: 0.9070\n",
      "Epoch 374/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1153 - categorical_accuracy: 0.9573\n",
      "Epoch 374: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1153 - categorical_accuracy: 0.9573 - val_loss: 0.2400 - val_categorical_accuracy: 0.9068\n",
      "Epoch 375/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1152 - categorical_accuracy: 0.9578\n",
      "Epoch 375: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1152 - categorical_accuracy: 0.9578 - val_loss: 0.2407 - val_categorical_accuracy: 0.9056\n",
      "Epoch 376/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1161 - categorical_accuracy: 0.9566\n",
      "Epoch 376: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1160 - categorical_accuracy: 0.9566 - val_loss: 0.2401 - val_categorical_accuracy: 0.9066\n",
      "Epoch 377/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1151 - categorical_accuracy: 0.9573\n",
      "Epoch 377: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1151 - categorical_accuracy: 0.9573 - val_loss: 0.2416 - val_categorical_accuracy: 0.9061\n",
      "Epoch 378/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1147 - categorical_accuracy: 0.9575\n",
      "Epoch 378: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1148 - categorical_accuracy: 0.9575 - val_loss: 0.2406 - val_categorical_accuracy: 0.9061\n",
      "Epoch 379/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1153 - categorical_accuracy: 0.9576\n",
      "Epoch 379: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1153 - categorical_accuracy: 0.9576 - val_loss: 0.2415 - val_categorical_accuracy: 0.9049\n",
      "Epoch 380/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1146 - categorical_accuracy: 0.9580\n",
      "Epoch 380: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1146 - categorical_accuracy: 0.9580 - val_loss: 0.2402 - val_categorical_accuracy: 0.9058\n",
      "Epoch 381/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1142 - categorical_accuracy: 0.9580\n",
      "Epoch 381: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1143 - categorical_accuracy: 0.9578 - val_loss: 0.2417 - val_categorical_accuracy: 0.9070\n",
      "Epoch 382/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1147 - categorical_accuracy: 0.9580\n",
      "Epoch 382: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1146 - categorical_accuracy: 0.9580 - val_loss: 0.2415 - val_categorical_accuracy: 0.9075\n",
      "Epoch 383/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1141 - categorical_accuracy: 0.9575\n",
      "Epoch 383: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1140 - categorical_accuracy: 0.9575 - val_loss: 0.2424 - val_categorical_accuracy: 0.9060\n",
      "Epoch 384/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1133 - categorical_accuracy: 0.9579\n",
      "Epoch 384: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1135 - categorical_accuracy: 0.9577 - val_loss: 0.2416 - val_categorical_accuracy: 0.9065\n",
      "Epoch 385/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1137 - categorical_accuracy: 0.9583\n",
      "Epoch 385: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1137 - categorical_accuracy: 0.9584 - val_loss: 0.2427 - val_categorical_accuracy: 0.9065\n",
      "Epoch 386/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1138 - categorical_accuracy: 0.9586\n",
      "Epoch 386: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1137 - categorical_accuracy: 0.9586 - val_loss: 0.2411 - val_categorical_accuracy: 0.9066\n",
      "Epoch 387/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1138 - categorical_accuracy: 0.9577\n",
      "Epoch 387: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1136 - categorical_accuracy: 0.9577 - val_loss: 0.2414 - val_categorical_accuracy: 0.9061\n",
      "Epoch 388/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1124 - categorical_accuracy: 0.9588\n",
      "Epoch 388: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1125 - categorical_accuracy: 0.9588 - val_loss: 0.2414 - val_categorical_accuracy: 0.9063\n",
      "Epoch 389/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1127 - categorical_accuracy: 0.9590\n",
      "Epoch 389: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1127 - categorical_accuracy: 0.9590 - val_loss: 0.2417 - val_categorical_accuracy: 0.9056\n",
      "Epoch 390/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1125 - categorical_accuracy: 0.9583\n",
      "Epoch 390: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1124 - categorical_accuracy: 0.9584 - val_loss: 0.2419 - val_categorical_accuracy: 0.9078\n",
      "Epoch 391/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1131 - categorical_accuracy: 0.9588\n",
      "Epoch 391: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1130 - categorical_accuracy: 0.9589 - val_loss: 0.2418 - val_categorical_accuracy: 0.9056\n",
      "Epoch 392/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1128 - categorical_accuracy: 0.9587\n",
      "Epoch 392: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1128 - categorical_accuracy: 0.9588 - val_loss: 0.2430 - val_categorical_accuracy: 0.9066\n",
      "Epoch 393/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1125 - categorical_accuracy: 0.9589\n",
      "Epoch 393: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1124 - categorical_accuracy: 0.9589 - val_loss: 0.2421 - val_categorical_accuracy: 0.9072\n",
      "Epoch 394/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1124 - categorical_accuracy: 0.9586\n",
      "Epoch 394: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1125 - categorical_accuracy: 0.9586 - val_loss: 0.2437 - val_categorical_accuracy: 0.9063\n",
      "Epoch 395/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1122 - categorical_accuracy: 0.9588\n",
      "Epoch 395: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1120 - categorical_accuracy: 0.9588 - val_loss: 0.2436 - val_categorical_accuracy: 0.9065\n",
      "Epoch 396/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1115 - categorical_accuracy: 0.9586\n",
      "Epoch 396: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1114 - categorical_accuracy: 0.9586 - val_loss: 0.2434 - val_categorical_accuracy: 0.9068\n",
      "Epoch 397/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1118 - categorical_accuracy: 0.9594\n",
      "Epoch 397: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1118 - categorical_accuracy: 0.9593 - val_loss: 0.2431 - val_categorical_accuracy: 0.9060\n",
      "Epoch 398/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1114 - categorical_accuracy: 0.9595\n",
      "Epoch 398: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1114 - categorical_accuracy: 0.9595 - val_loss: 0.2436 - val_categorical_accuracy: 0.9060\n",
      "Epoch 399/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1108 - categorical_accuracy: 0.9590\n",
      "Epoch 399: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1108 - categorical_accuracy: 0.9589 - val_loss: 0.2426 - val_categorical_accuracy: 0.9049\n",
      "Epoch 400/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1105 - categorical_accuracy: 0.9593\n",
      "Epoch 400: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1106 - categorical_accuracy: 0.9593 - val_loss: 0.2429 - val_categorical_accuracy: 0.9066\n",
      "Epoch 401/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1106 - categorical_accuracy: 0.9591\n",
      "Epoch 401: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1106 - categorical_accuracy: 0.9591 - val_loss: 0.2432 - val_categorical_accuracy: 0.9063\n",
      "Epoch 402/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1091 - categorical_accuracy: 0.9598\n",
      "Epoch 402: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1092 - categorical_accuracy: 0.9598 - val_loss: 0.2435 - val_categorical_accuracy: 0.9072\n",
      "Epoch 403/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1090 - categorical_accuracy: 0.9603\n",
      "Epoch 403: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1092 - categorical_accuracy: 0.9602 - val_loss: 0.2428 - val_categorical_accuracy: 0.9056\n",
      "Epoch 404/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1098 - categorical_accuracy: 0.9599\n",
      "Epoch 404: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1098 - categorical_accuracy: 0.9599 - val_loss: 0.2445 - val_categorical_accuracy: 0.9063\n",
      "Epoch 405/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1087 - categorical_accuracy: 0.9605\n",
      "Epoch 405: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1088 - categorical_accuracy: 0.9604 - val_loss: 0.2436 - val_categorical_accuracy: 0.9073\n",
      "Epoch 406/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1091 - categorical_accuracy: 0.9601\n",
      "Epoch 406: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1089 - categorical_accuracy: 0.9602 - val_loss: 0.2436 - val_categorical_accuracy: 0.9063\n",
      "Epoch 407/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1089 - categorical_accuracy: 0.9596\n",
      "Epoch 407: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1090 - categorical_accuracy: 0.9596 - val_loss: 0.2440 - val_categorical_accuracy: 0.9063\n",
      "Epoch 408/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1089 - categorical_accuracy: 0.9605\n",
      "Epoch 408: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1090 - categorical_accuracy: 0.9605 - val_loss: 0.2440 - val_categorical_accuracy: 0.9063\n",
      "Epoch 409/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1092 - categorical_accuracy: 0.9598\n",
      "Epoch 409: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1091 - categorical_accuracy: 0.9598 - val_loss: 0.2428 - val_categorical_accuracy: 0.9063\n",
      "Epoch 410/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1084 - categorical_accuracy: 0.9598\n",
      "Epoch 410: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1084 - categorical_accuracy: 0.9598 - val_loss: 0.2438 - val_categorical_accuracy: 0.9072\n",
      "Epoch 411/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1084 - categorical_accuracy: 0.9607\n",
      "Epoch 411: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1085 - categorical_accuracy: 0.9606 - val_loss: 0.2439 - val_categorical_accuracy: 0.9060\n",
      "Epoch 412/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1086 - categorical_accuracy: 0.9604\n",
      "Epoch 412: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1085 - categorical_accuracy: 0.9605 - val_loss: 0.2450 - val_categorical_accuracy: 0.9056\n",
      "Epoch 413/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1081 - categorical_accuracy: 0.9601\n",
      "Epoch 413: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1082 - categorical_accuracy: 0.9600 - val_loss: 0.2446 - val_categorical_accuracy: 0.9073\n",
      "Epoch 414/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1078 - categorical_accuracy: 0.9608\n",
      "Epoch 414: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1079 - categorical_accuracy: 0.9607 - val_loss: 0.2453 - val_categorical_accuracy: 0.9070\n",
      "Epoch 415/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1074 - categorical_accuracy: 0.9614\n",
      "Epoch 415: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1074 - categorical_accuracy: 0.9613 - val_loss: 0.2438 - val_categorical_accuracy: 0.9073\n",
      "Epoch 416/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1075 - categorical_accuracy: 0.9606\n",
      "Epoch 416: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1075 - categorical_accuracy: 0.9607 - val_loss: 0.2453 - val_categorical_accuracy: 0.9073\n",
      "Epoch 417/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1077 - categorical_accuracy: 0.9606\n",
      "Epoch 417: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1078 - categorical_accuracy: 0.9605 - val_loss: 0.2448 - val_categorical_accuracy: 0.9053\n",
      "Epoch 418/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1073 - categorical_accuracy: 0.9607\n",
      "Epoch 418: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1073 - categorical_accuracy: 0.9607 - val_loss: 0.2452 - val_categorical_accuracy: 0.9063\n",
      "Epoch 419/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1062 - categorical_accuracy: 0.9615\n",
      "Epoch 419: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1062 - categorical_accuracy: 0.9614 - val_loss: 0.2450 - val_categorical_accuracy: 0.9053\n",
      "Epoch 420/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1072 - categorical_accuracy: 0.9606\n",
      "Epoch 420: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1070 - categorical_accuracy: 0.9607 - val_loss: 0.2449 - val_categorical_accuracy: 0.9066\n",
      "Epoch 421/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1063 - categorical_accuracy: 0.9618\n",
      "Epoch 421: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1062 - categorical_accuracy: 0.9618 - val_loss: 0.2448 - val_categorical_accuracy: 0.9058\n",
      "Epoch 422/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1061 - categorical_accuracy: 0.9610\n",
      "Epoch 422: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1060 - categorical_accuracy: 0.9610 - val_loss: 0.2458 - val_categorical_accuracy: 0.9054\n",
      "Epoch 423/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1054 - categorical_accuracy: 0.9616\n",
      "Epoch 423: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1054 - categorical_accuracy: 0.9616 - val_loss: 0.2453 - val_categorical_accuracy: 0.9063\n",
      "Epoch 424/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1067 - categorical_accuracy: 0.9611\n",
      "Epoch 424: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1066 - categorical_accuracy: 0.9612 - val_loss: 0.2460 - val_categorical_accuracy: 0.9072\n",
      "Epoch 425/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1059 - categorical_accuracy: 0.9616\n",
      "Epoch 425: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1059 - categorical_accuracy: 0.9616 - val_loss: 0.2453 - val_categorical_accuracy: 0.9065\n",
      "Epoch 426/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1055 - categorical_accuracy: 0.9613\n",
      "Epoch 426: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1055 - categorical_accuracy: 0.9612 - val_loss: 0.2453 - val_categorical_accuracy: 0.9054\n",
      "Epoch 427/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1060 - categorical_accuracy: 0.9616\n",
      "Epoch 427: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1061 - categorical_accuracy: 0.9615 - val_loss: 0.2455 - val_categorical_accuracy: 0.9061\n",
      "Epoch 428/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1051 - categorical_accuracy: 0.9620\n",
      "Epoch 428: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1050 - categorical_accuracy: 0.9620 - val_loss: 0.2459 - val_categorical_accuracy: 0.9063\n",
      "Epoch 429/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1049 - categorical_accuracy: 0.9621\n",
      "Epoch 429: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1048 - categorical_accuracy: 0.9621 - val_loss: 0.2452 - val_categorical_accuracy: 0.9061\n",
      "Epoch 430/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1052 - categorical_accuracy: 0.9619\n",
      "Epoch 430: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1052 - categorical_accuracy: 0.9619 - val_loss: 0.2453 - val_categorical_accuracy: 0.9061\n",
      "Epoch 431/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1044 - categorical_accuracy: 0.9625\n",
      "Epoch 431: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1045 - categorical_accuracy: 0.9624 - val_loss: 0.2461 - val_categorical_accuracy: 0.9060\n",
      "Epoch 432/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1039 - categorical_accuracy: 0.9621\n",
      "Epoch 432: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1038 - categorical_accuracy: 0.9621 - val_loss: 0.2459 - val_categorical_accuracy: 0.9061\n",
      "Epoch 433/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1041 - categorical_accuracy: 0.9623\n",
      "Epoch 433: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1042 - categorical_accuracy: 0.9623 - val_loss: 0.2459 - val_categorical_accuracy: 0.9065\n",
      "Epoch 434/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1033 - categorical_accuracy: 0.9626\n",
      "Epoch 434: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1033 - categorical_accuracy: 0.9626 - val_loss: 0.2465 - val_categorical_accuracy: 0.9061\n",
      "Epoch 435/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1040 - categorical_accuracy: 0.9622\n",
      "Epoch 435: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1040 - categorical_accuracy: 0.9621 - val_loss: 0.2475 - val_categorical_accuracy: 0.9070\n",
      "Epoch 436/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1036 - categorical_accuracy: 0.9625\n",
      "Epoch 436: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1037 - categorical_accuracy: 0.9625 - val_loss: 0.2467 - val_categorical_accuracy: 0.9066\n",
      "Epoch 437/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1029 - categorical_accuracy: 0.9630\n",
      "Epoch 437: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1029 - categorical_accuracy: 0.9630 - val_loss: 0.2461 - val_categorical_accuracy: 0.9073\n",
      "Epoch 438/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1031 - categorical_accuracy: 0.9629\n",
      "Epoch 438: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1030 - categorical_accuracy: 0.9629 - val_loss: 0.2465 - val_categorical_accuracy: 0.9063\n",
      "Epoch 439/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1023 - categorical_accuracy: 0.9633\n",
      "Epoch 439: val_categorical_accuracy did not improve from 0.90801\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1025 - categorical_accuracy: 0.9633 - val_loss: 0.2470 - val_categorical_accuracy: 0.9065\n",
      "Epoch 440/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1030 - categorical_accuracy: 0.9630\n",
      "Epoch 440: val_categorical_accuracy improved from 0.90801 to 0.90819, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1031 - categorical_accuracy: 0.9629 - val_loss: 0.2465 - val_categorical_accuracy: 0.9082\n",
      "Epoch 441/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1031 - categorical_accuracy: 0.9626\n",
      "Epoch 441: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1032 - categorical_accuracy: 0.9626 - val_loss: 0.2466 - val_categorical_accuracy: 0.9063\n",
      "Epoch 442/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.1029 - categorical_accuracy: 0.9629\n",
      "Epoch 442: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1029 - categorical_accuracy: 0.9629 - val_loss: 0.2468 - val_categorical_accuracy: 0.9073\n",
      "Epoch 443/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1027 - categorical_accuracy: 0.9625\n",
      "Epoch 443: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1027 - categorical_accuracy: 0.9625 - val_loss: 0.2471 - val_categorical_accuracy: 0.9066\n",
      "Epoch 444/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1025 - categorical_accuracy: 0.9621\n",
      "Epoch 444: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1026 - categorical_accuracy: 0.9622 - val_loss: 0.2471 - val_categorical_accuracy: 0.9077\n",
      "Epoch 445/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.1023 - categorical_accuracy: 0.9628\n",
      "Epoch 445: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1023 - categorical_accuracy: 0.9628 - val_loss: 0.2474 - val_categorical_accuracy: 0.9077\n",
      "Epoch 446/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.1017 - categorical_accuracy: 0.9628\n",
      "Epoch 446: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1018 - categorical_accuracy: 0.9628 - val_loss: 0.2469 - val_categorical_accuracy: 0.9058\n",
      "Epoch 447/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1009 - categorical_accuracy: 0.9633\n",
      "Epoch 447: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1010 - categorical_accuracy: 0.9634 - val_loss: 0.2474 - val_categorical_accuracy: 0.9061\n",
      "Epoch 448/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.1010 - categorical_accuracy: 0.9634\n",
      "Epoch 448: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1011 - categorical_accuracy: 0.9634 - val_loss: 0.2479 - val_categorical_accuracy: 0.9065\n",
      "Epoch 449/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.1011 - categorical_accuracy: 0.9639\n",
      "Epoch 449: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1011 - categorical_accuracy: 0.9638 - val_loss: 0.2473 - val_categorical_accuracy: 0.9068\n",
      "Epoch 450/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1013 - categorical_accuracy: 0.9630\n",
      "Epoch 450: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1013 - categorical_accuracy: 0.9630 - val_loss: 0.2472 - val_categorical_accuracy: 0.9065\n",
      "Epoch 451/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.1004 - categorical_accuracy: 0.9639\n",
      "Epoch 451: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1004 - categorical_accuracy: 0.9639 - val_loss: 0.2489 - val_categorical_accuracy: 0.9060\n",
      "Epoch 452/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0999 - categorical_accuracy: 0.9639\n",
      "Epoch 452: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0999 - categorical_accuracy: 0.9639 - val_loss: 0.2491 - val_categorical_accuracy: 0.9078\n",
      "Epoch 453/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.1004 - categorical_accuracy: 0.9645\n",
      "Epoch 453: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1004 - categorical_accuracy: 0.9645 - val_loss: 0.2481 - val_categorical_accuracy: 0.9046\n",
      "Epoch 454/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.1004 - categorical_accuracy: 0.9638\n",
      "Epoch 454: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.1003 - categorical_accuracy: 0.9638 - val_loss: 0.2482 - val_categorical_accuracy: 0.9063\n",
      "Epoch 455/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0999 - categorical_accuracy: 0.9642\n",
      "Epoch 455: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0999 - categorical_accuracy: 0.9642 - val_loss: 0.2483 - val_categorical_accuracy: 0.9070\n",
      "Epoch 456/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0997 - categorical_accuracy: 0.9640\n",
      "Epoch 456: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0997 - categorical_accuracy: 0.9640 - val_loss: 0.2494 - val_categorical_accuracy: 0.9075\n",
      "Epoch 457/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0998 - categorical_accuracy: 0.9639\n",
      "Epoch 457: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0998 - categorical_accuracy: 0.9639 - val_loss: 0.2486 - val_categorical_accuracy: 0.9061\n",
      "Epoch 458/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.0999 - categorical_accuracy: 0.9644\n",
      "Epoch 458: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0998 - categorical_accuracy: 0.9643 - val_loss: 0.2490 - val_categorical_accuracy: 0.9063\n",
      "Epoch 459/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0989 - categorical_accuracy: 0.9648\n",
      "Epoch 459: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0988 - categorical_accuracy: 0.9648 - val_loss: 0.2494 - val_categorical_accuracy: 0.9065\n",
      "Epoch 460/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0986 - categorical_accuracy: 0.9645\n",
      "Epoch 460: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0987 - categorical_accuracy: 0.9645 - val_loss: 0.2494 - val_categorical_accuracy: 0.9072\n",
      "Epoch 461/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0987 - categorical_accuracy: 0.9645\n",
      "Epoch 461: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0987 - categorical_accuracy: 0.9645 - val_loss: 0.2498 - val_categorical_accuracy: 0.9077\n",
      "Epoch 462/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.0995 - categorical_accuracy: 0.9639\n",
      "Epoch 462: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0994 - categorical_accuracy: 0.9640 - val_loss: 0.2486 - val_categorical_accuracy: 0.9066\n",
      "Epoch 463/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0982 - categorical_accuracy: 0.9649\n",
      "Epoch 463: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0982 - categorical_accuracy: 0.9649 - val_loss: 0.2491 - val_categorical_accuracy: 0.9056\n",
      "Epoch 464/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0985 - categorical_accuracy: 0.9650\n",
      "Epoch 464: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0986 - categorical_accuracy: 0.9650 - val_loss: 0.2491 - val_categorical_accuracy: 0.9070\n",
      "Epoch 465/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0985 - categorical_accuracy: 0.9647\n",
      "Epoch 465: val_categorical_accuracy did not improve from 0.90819\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0985 - categorical_accuracy: 0.9647 - val_loss: 0.2511 - val_categorical_accuracy: 0.9078\n",
      "Epoch 466/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0975 - categorical_accuracy: 0.9650\n",
      "Epoch 466: val_categorical_accuracy improved from 0.90819 to 0.90853, saving model to models/temp\\NNmodel.h5\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0975 - categorical_accuracy: 0.9649 - val_loss: 0.2508 - val_categorical_accuracy: 0.9085\n",
      "Epoch 467/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0984 - categorical_accuracy: 0.9647\n",
      "Epoch 467: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0985 - categorical_accuracy: 0.9647 - val_loss: 0.2491 - val_categorical_accuracy: 0.9058\n",
      "Epoch 468/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.0980 - categorical_accuracy: 0.9643\n",
      "Epoch 468: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0980 - categorical_accuracy: 0.9642 - val_loss: 0.2505 - val_categorical_accuracy: 0.9072\n",
      "Epoch 469/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0977 - categorical_accuracy: 0.9652\n",
      "Epoch 469: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0977 - categorical_accuracy: 0.9652 - val_loss: 0.2511 - val_categorical_accuracy: 0.9077\n",
      "Epoch 470/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.0977 - categorical_accuracy: 0.9652\n",
      "Epoch 470: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0978 - categorical_accuracy: 0.9652 - val_loss: 0.2507 - val_categorical_accuracy: 0.9060\n",
      "Epoch 471/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.0962 - categorical_accuracy: 0.9658\n",
      "Epoch 471: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0963 - categorical_accuracy: 0.9657 - val_loss: 0.2503 - val_categorical_accuracy: 0.9054\n",
      "Epoch 472/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0971 - categorical_accuracy: 0.9652\n",
      "Epoch 472: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0970 - categorical_accuracy: 0.9653 - val_loss: 0.2504 - val_categorical_accuracy: 0.9072\n",
      "Epoch 473/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0970 - categorical_accuracy: 0.9653\n",
      "Epoch 473: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0970 - categorical_accuracy: 0.9653 - val_loss: 0.2509 - val_categorical_accuracy: 0.9065\n",
      "Epoch 474/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0973 - categorical_accuracy: 0.9649\n",
      "Epoch 474: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0973 - categorical_accuracy: 0.9649 - val_loss: 0.2508 - val_categorical_accuracy: 0.9056\n",
      "Epoch 475/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.0963 - categorical_accuracy: 0.9652\n",
      "Epoch 475: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0963 - categorical_accuracy: 0.9652 - val_loss: 0.2511 - val_categorical_accuracy: 0.9075\n",
      "Epoch 476/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0959 - categorical_accuracy: 0.9658\n",
      "Epoch 476: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0960 - categorical_accuracy: 0.9658 - val_loss: 0.2512 - val_categorical_accuracy: 0.9070\n",
      "Epoch 477/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.0963 - categorical_accuracy: 0.9656\n",
      "Epoch 477: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0963 - categorical_accuracy: 0.9655 - val_loss: 0.2507 - val_categorical_accuracy: 0.9072\n",
      "Epoch 478/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0965 - categorical_accuracy: 0.9655\n",
      "Epoch 478: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0964 - categorical_accuracy: 0.9655 - val_loss: 0.2514 - val_categorical_accuracy: 0.9060\n",
      "Epoch 479/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0966 - categorical_accuracy: 0.9656\n",
      "Epoch 479: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0966 - categorical_accuracy: 0.9656 - val_loss: 0.2502 - val_categorical_accuracy: 0.9061\n",
      "Epoch 480/500\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.0949 - categorical_accuracy: 0.9656\n",
      "Epoch 480: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0949 - categorical_accuracy: 0.9656 - val_loss: 0.2519 - val_categorical_accuracy: 0.9068\n",
      "Epoch 481/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.0953 - categorical_accuracy: 0.9662\n",
      "Epoch 481: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0951 - categorical_accuracy: 0.9663 - val_loss: 0.2514 - val_categorical_accuracy: 0.9077\n",
      "Epoch 482/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0960 - categorical_accuracy: 0.9655\n",
      "Epoch 482: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0961 - categorical_accuracy: 0.9655 - val_loss: 0.2524 - val_categorical_accuracy: 0.9078\n",
      "Epoch 483/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0954 - categorical_accuracy: 0.9662\n",
      "Epoch 483: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0955 - categorical_accuracy: 0.9662 - val_loss: 0.2522 - val_categorical_accuracy: 0.9065\n",
      "Epoch 484/500\n",
      "431/433 [============================>.] - ETA: 0s - loss: 0.0947 - categorical_accuracy: 0.9662\n",
      "Epoch 484: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0947 - categorical_accuracy: 0.9661 - val_loss: 0.2520 - val_categorical_accuracy: 0.9060\n",
      "Epoch 485/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0951 - categorical_accuracy: 0.9663\n",
      "Epoch 485: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0951 - categorical_accuracy: 0.9662 - val_loss: 0.2524 - val_categorical_accuracy: 0.9063\n",
      "Epoch 486/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0941 - categorical_accuracy: 0.9668\n",
      "Epoch 486: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0941 - categorical_accuracy: 0.9668 - val_loss: 0.2506 - val_categorical_accuracy: 0.9078\n",
      "Epoch 487/500\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.0937 - categorical_accuracy: 0.9667\n",
      "Epoch 487: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0937 - categorical_accuracy: 0.9667 - val_loss: 0.2512 - val_categorical_accuracy: 0.9078\n",
      "Epoch 488/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.0948 - categorical_accuracy: 0.9655\n",
      "Epoch 488: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0950 - categorical_accuracy: 0.9655 - val_loss: 0.2521 - val_categorical_accuracy: 0.9063\n",
      "Epoch 489/500\n",
      "426/433 [============================>.] - ETA: 0s - loss: 0.0944 - categorical_accuracy: 0.9662\n",
      "Epoch 489: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0943 - categorical_accuracy: 0.9662 - val_loss: 0.2525 - val_categorical_accuracy: 0.9061\n",
      "Epoch 490/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0933 - categorical_accuracy: 0.9668\n",
      "Epoch 490: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0935 - categorical_accuracy: 0.9667 - val_loss: 0.2523 - val_categorical_accuracy: 0.9072\n",
      "Epoch 491/500\n",
      "427/433 [============================>.] - ETA: 0s - loss: 0.0942 - categorical_accuracy: 0.9665\n",
      "Epoch 491: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0942 - categorical_accuracy: 0.9664 - val_loss: 0.2529 - val_categorical_accuracy: 0.9065\n",
      "Epoch 492/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0931 - categorical_accuracy: 0.9674\n",
      "Epoch 492: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0931 - categorical_accuracy: 0.9674 - val_loss: 0.2528 - val_categorical_accuracy: 0.9072\n",
      "Epoch 493/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0926 - categorical_accuracy: 0.9668\n",
      "Epoch 493: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0927 - categorical_accuracy: 0.9668 - val_loss: 0.2524 - val_categorical_accuracy: 0.9066\n",
      "Epoch 494/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0928 - categorical_accuracy: 0.9674\n",
      "Epoch 494: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0929 - categorical_accuracy: 0.9674 - val_loss: 0.2530 - val_categorical_accuracy: 0.9060\n",
      "Epoch 495/500\n",
      "429/433 [============================>.] - ETA: 0s - loss: 0.0921 - categorical_accuracy: 0.9670\n",
      "Epoch 495: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0920 - categorical_accuracy: 0.9670 - val_loss: 0.2521 - val_categorical_accuracy: 0.9065\n",
      "Epoch 496/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0927 - categorical_accuracy: 0.9675\n",
      "Epoch 496: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0927 - categorical_accuracy: 0.9675 - val_loss: 0.2538 - val_categorical_accuracy: 0.9065\n",
      "Epoch 497/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0924 - categorical_accuracy: 0.9673\n",
      "Epoch 497: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0925 - categorical_accuracy: 0.9672 - val_loss: 0.2527 - val_categorical_accuracy: 0.9063\n",
      "Epoch 498/500\n",
      "430/433 [============================>.] - ETA: 0s - loss: 0.0925 - categorical_accuracy: 0.9673\n",
      "Epoch 498: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0925 - categorical_accuracy: 0.9672 - val_loss: 0.2543 - val_categorical_accuracy: 0.9068\n",
      "Epoch 499/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0923 - categorical_accuracy: 0.9670\n",
      "Epoch 499: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0923 - categorical_accuracy: 0.9670 - val_loss: 0.2549 - val_categorical_accuracy: 0.9046\n",
      "Epoch 500/500\n",
      "428/433 [============================>.] - ETA: 0s - loss: 0.0923 - categorical_accuracy: 0.9670\n",
      "Epoch 500: val_categorical_accuracy did not improve from 0.90853\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.0922 - categorical_accuracy: 0.9670 - val_loss: 0.2541 - val_categorical_accuracy: 0.9078\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2541 - categorical_accuracy: 0.9078\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.2541351914405823, 0.9078428149223328]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hyperOptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, weight_decay=0, amsgrad=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='models/temp/NNmodel.h5',\n",
    "                             monitor='val_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "NNmodel = tf.keras.Sequential([tf.keras.layers.Dense(200, activation=tf.keras.activations.relu), tf.keras.layers.Dropout(0.3),  tf.keras.layers.Dense(3)])\n",
    "NNmodel.compile(optimizer=hyperOptimizer, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=(tf.keras.metrics.CategoricalAccuracy()))\n",
    "history = NNmodel.fit(X_tensor_train, Y_tensor_train, batch_size=256, epochs=500, validation_data=(X_tensor_test, Y_tensor_test), callbacks=[checkpoint])\n",
    "NNmodel.evaluate(X_tensor_test, Y_tensor_test, verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-16T01:45:41.257925Z",
     "end_time": "2023-10-16T02:11:38.613952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "217/217 [==============================] - 9s 20ms/step - loss: 0.3713 - categorical_accuracy: 0.8442 - val_loss: 0.2810 - val_categorical_accuracy: 0.8862\n",
      "Epoch 2/25\n",
      "217/217 [==============================] - 5s 22ms/step - loss: 0.2606 - categorical_accuracy: 0.8920 - val_loss: 0.2666 - val_categorical_accuracy: 0.8903\n",
      "Epoch 3/25\n",
      "217/217 [==============================] - 5s 22ms/step - loss: 0.2478 - categorical_accuracy: 0.8973 - val_loss: 0.2602 - val_categorical_accuracy: 0.8909\n",
      "Epoch 4/25\n",
      "217/217 [==============================] - 4s 17ms/step - loss: 0.2400 - categorical_accuracy: 0.9004 - val_loss: 0.2639 - val_categorical_accuracy: 0.8900\n",
      "Epoch 5/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2341 - categorical_accuracy: 0.9034 - val_loss: 0.2549 - val_categorical_accuracy: 0.8945\n",
      "Epoch 6/25\n",
      "217/217 [==============================] - 4s 17ms/step - loss: 0.2295 - categorical_accuracy: 0.9065 - val_loss: 0.2496 - val_categorical_accuracy: 0.8962\n",
      "Epoch 7/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.2245 - categorical_accuracy: 0.9085 - val_loss: 0.2517 - val_categorical_accuracy: 0.8943\n",
      "Epoch 8/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2213 - categorical_accuracy: 0.9096 - val_loss: 0.2458 - val_categorical_accuracy: 0.9003\n",
      "Epoch 9/25\n",
      "217/217 [==============================] - 4s 17ms/step - loss: 0.2179 - categorical_accuracy: 0.9107 - val_loss: 0.2441 - val_categorical_accuracy: 0.8969\n",
      "Epoch 10/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2157 - categorical_accuracy: 0.9114 - val_loss: 0.2445 - val_categorical_accuracy: 0.9013\n",
      "Epoch 11/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.2143 - categorical_accuracy: 0.9116 - val_loss: 0.2447 - val_categorical_accuracy: 0.8993\n",
      "Epoch 12/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2117 - categorical_accuracy: 0.9127 - val_loss: 0.2420 - val_categorical_accuracy: 0.9017\n",
      "Epoch 13/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.2102 - categorical_accuracy: 0.9139 - val_loss: 0.2449 - val_categorical_accuracy: 0.9001\n",
      "Epoch 14/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.2094 - categorical_accuracy: 0.9143 - val_loss: 0.2435 - val_categorical_accuracy: 0.8981\n",
      "Epoch 15/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2076 - categorical_accuracy: 0.9145 - val_loss: 0.2437 - val_categorical_accuracy: 0.9025\n",
      "Epoch 16/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2053 - categorical_accuracy: 0.9156 - val_loss: 0.2402 - val_categorical_accuracy: 0.9003\n",
      "Epoch 17/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2040 - categorical_accuracy: 0.9159 - val_loss: 0.2408 - val_categorical_accuracy: 0.9024\n",
      "Epoch 18/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.2024 - categorical_accuracy: 0.9173 - val_loss: 0.2400 - val_categorical_accuracy: 0.9010\n",
      "Epoch 19/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.2025 - categorical_accuracy: 0.9165 - val_loss: 0.2383 - val_categorical_accuracy: 0.9017\n",
      "Epoch 20/25\n",
      "217/217 [==============================] - 3s 16ms/step - loss: 0.1998 - categorical_accuracy: 0.9181 - val_loss: 0.2402 - val_categorical_accuracy: 0.9003\n",
      "Epoch 21/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.1986 - categorical_accuracy: 0.9184 - val_loss: 0.2375 - val_categorical_accuracy: 0.9024\n",
      "Epoch 22/25\n",
      "217/217 [==============================] - 4s 17ms/step - loss: 0.1980 - categorical_accuracy: 0.9191 - val_loss: 0.2369 - val_categorical_accuracy: 0.9027\n",
      "Epoch 23/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.1968 - categorical_accuracy: 0.9195 - val_loss: 0.2374 - val_categorical_accuracy: 0.9027\n",
      "Epoch 24/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.1955 - categorical_accuracy: 0.9197 - val_loss: 0.2385 - val_categorical_accuracy: 0.9008\n",
      "Epoch 25/25\n",
      "217/217 [==============================] - 4s 16ms/step - loss: 0.1943 - categorical_accuracy: 0.9199 - val_loss: 0.2414 - val_categorical_accuracy: 0.9027\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.24140934646129608, 0.9026943445205688]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperOptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='models/temp/ResNetmodel.h5',\n",
    "                             monitor='val_categorical_accuracy',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "\n",
    "def generateModel():\n",
    "    inputs = keras.Input(shape=(1536, ), name=\"emb\")\n",
    "    x = layers.Dense(100, activation=keras.activations.swish)(inputs)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Activation(keras.activations.swish)(layers.Dense(100)(x) + layers.Dense(100)(inputs))\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "    # x = keras.activations.relu(layers.Dense(100)(x) + baseModel_out)\n",
    "\n",
    "    outputs = layers.Dense(3)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "ResNetmodel = generateModel()\n",
    "# ResNetmodel.summary()\n",
    "ResNetmodel.compile(optimizer=hyperOptimizer, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=(tf.keras.metrics.CategoricalAccuracy()))\n",
    "history = ResNetmodel.fit(X_tensor_train, Y_tensor_train, batch_size=512, epochs=25, validation_data=(X_tensor_test, Y_tensor_test), callbacks=[checkpoint], verbose=1)\n",
    "ResNetmodel.evaluate(X_tensor_test, Y_tensor_test, verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T19:50:20.322603Z",
     "end_time": "2023-10-06T19:51:57.259711Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "433/433 [==============================] - 55s 123ms/step - loss: 0.3593 - categorical_accuracy: 0.8487 - val_loss: 0.3182 - val_categorical_accuracy: 0.8711\n",
      "Epoch 2/10\n",
      "433/433 [==============================] - 66s 153ms/step - loss: 0.2996 - categorical_accuracy: 0.8784 - val_loss: 0.3129 - val_categorical_accuracy: 0.8764\n",
      "Epoch 3/10\n",
      "433/433 [==============================] - 66s 152ms/step - loss: 0.2898 - categorical_accuracy: 0.8821 - val_loss: 0.3077 - val_categorical_accuracy: 0.8773\n",
      "Epoch 4/10\n",
      "433/433 [==============================] - 61s 140ms/step - loss: 0.2865 - categorical_accuracy: 0.8839 - val_loss: 0.3017 - val_categorical_accuracy: 0.8785\n",
      "Epoch 5/10\n",
      "433/433 [==============================] - 63s 145ms/step - loss: 0.2801 - categorical_accuracy: 0.8855 - val_loss: 0.3041 - val_categorical_accuracy: 0.8721\n",
      "Epoch 6/10\n",
      "433/433 [==============================] - 59s 137ms/step - loss: 0.2800 - categorical_accuracy: 0.8867 - val_loss: 0.2948 - val_categorical_accuracy: 0.8831\n",
      "Epoch 7/10\n",
      "433/433 [==============================] - 60s 140ms/step - loss: 0.2759 - categorical_accuracy: 0.8881 - val_loss: 0.2927 - val_categorical_accuracy: 0.8826\n",
      "Epoch 8/10\n",
      "433/433 [==============================] - 61s 141ms/step - loss: 0.2764 - categorical_accuracy: 0.8880 - val_loss: 0.2938 - val_categorical_accuracy: 0.8824\n",
      "Epoch 9/10\n",
      "433/433 [==============================] - 66s 152ms/step - loss: 0.2741 - categorical_accuracy: 0.8885 - val_loss: 0.3222 - val_categorical_accuracy: 0.8715\n",
      "Epoch 10/10\n",
      "433/433 [==============================] - 66s 152ms/step - loss: 0.2750 - categorical_accuracy: 0.8884 - val_loss: 0.2925 - val_categorical_accuracy: 0.8788\n",
      "183/183 - 2s - loss: 0.2925 - categorical_accuracy: 0.8788 - 2s/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.29252496361732483, 0.878839910030365]"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperOptimizer = tf.keras.optimizers.Adam(learning_rate=0.01, weight_decay=0)\n",
    "CNNmodel = tf.keras.Sequential([tf.keras.layers.Conv1D(filters=25, kernel_size=16, input_shape=(1536, 1)), tf.keras.layers.MaxPool1D(pool_size=2, strides=2), tf.keras.layers.Conv1D(filters=5, kernel_size=8), tf.keras.layers.AvgPool1D(pool_size=2, strides=2), tf.keras.layers.Conv1D(filters=1, kernel_size=4), tf.keras.layers.Flatten(), tf.keras.layers.Dense(3)])\n",
    "# print(CNNmodel.summary())\n",
    "CNNmodel.compile(optimizer=hyperOptimizer, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=(tf.keras.metrics.CategoricalAccuracy()))\n",
    "history = CNNmodel.fit(X_tensor_train, Y_tensor_train, batch_size=256, epochs=10, validation_data=(X_tensor_test, Y_tensor_test))\n",
    "\n",
    "CNNmodel.evaluate(X_tensor_test, Y_tensor_test, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:24:27.875568Z",
     "end_time": "2023-09-25T01:34:53.018481Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      2883\n",
      "           1       0.89      0.92      0.91      1044\n",
      "           2       0.90      0.92      0.91      1900\n",
      "\n",
      "    accuracy                           0.91      5827\n",
      "   macro avg       0.91      0.91      0.91      5827\n",
      "weighted avg       0.91      0.91      0.91      5827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NNmodel_inf = tf.keras.Sequential([NNmodel, tf.keras.layers.Softmax()])\n",
    "report = classification_report(np.argmax(Y_tensor_test, axis=1), np.argmax(NNmodel_inf.predict(X_tensor_test), axis=1))\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T02:31:18.521925Z",
     "end_time": "2023-09-25T02:31:18.944940Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# NNmodel.save(\"models/embeddingsNN\")\n",
    "NNmodel = tf.keras.models.load_model(\"models/temp/NNmodel.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T03:19:19.408778Z",
     "end_time": "2023-09-25T03:19:20.491292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Train Loss",
         "y": [
          0.8221728205680847,
          0.4059525728225708,
          0.31078198552131653,
          0.29186147451400757,
          0.28236639499664307,
          0.2759021818637848,
          0.2707113027572632,
          0.26674535870552063,
          0.26342669129371643,
          0.26063841581344604,
          0.25824013352394104,
          0.2561529278755188,
          0.254269003868103,
          0.25274550914764404,
          0.2511916756629944,
          0.2498110681772232,
          0.24846859276294708,
          0.24731658399105072,
          0.24639160931110382,
          0.2454918771982193,
          0.24434448778629303,
          0.2434232085943222,
          0.2427108883857727,
          0.24181587994098663,
          0.24127411842346191,
          0.24056899547576904,
          0.23981860280036926,
          0.2391965687274933,
          0.23865613341331482,
          0.23805108666419983,
          0.237343892455101,
          0.236976757645607,
          0.23643182218074799,
          0.236032173037529,
          0.23537977039813995,
          0.23486967384815216,
          0.2343938946723938,
          0.23413637280464172,
          0.23374223709106445,
          0.23343409597873688,
          0.23298580944538116,
          0.23239447176456451,
          0.23203301429748535,
          0.2317318320274353,
          0.2313278168439865,
          0.23083806037902832,
          0.230381578207016,
          0.2299567312002182,
          0.22986143827438354,
          0.2291860729455948,
          0.22892990708351135,
          0.22849000990390778,
          0.22834400832653046,
          0.22786585986614227,
          0.22734853625297546,
          0.2270345836877823,
          0.22693973779678345,
          0.22661158442497253,
          0.22615006566047668,
          0.22569778561592102,
          0.2253211885690689,
          0.22505679726600647,
          0.22453247010707855,
          0.22456566989421844,
          0.2240161895751953,
          0.22362029552459717,
          0.22340315580368042,
          0.22332081198692322,
          0.22278481721878052,
          0.22241441905498505,
          0.22195777297019958,
          0.22162143886089325,
          0.22121110558509827,
          0.22082926332950592,
          0.22072385251522064,
          0.2205130010843277,
          0.22003690898418427,
          0.22012782096862793,
          0.21970795094966888,
          0.21915145218372345,
          0.2193886637687683,
          0.21870042383670807,
          0.21863853931427002,
          0.2180650383234024,
          0.21770597994327545,
          0.2173834592103958,
          0.21744653582572937,
          0.2170257568359375,
          0.2169177085161209,
          0.21659225225448608,
          0.21655331552028656,
          0.2160089612007141,
          0.2163783609867096,
          0.21575085818767548,
          0.2156670242547989,
          0.21502315998077393,
          0.2150454968214035,
          0.2147701233625412,
          0.21444790065288544,
          0.2143961638212204,
          0.21431979537010193,
          0.21402588486671448,
          0.21376082301139832,
          0.21335890889167786,
          0.21325618028640747,
          0.21300272643566132,
          0.21285808086395264,
          0.2127775251865387,
          0.21265809237957,
          0.2125522643327713,
          0.2120668590068817,
          0.2121511846780777,
          0.21196569502353668,
          0.21155212819576263,
          0.211497500538826,
          0.21121129393577576,
          0.21119947731494904,
          0.2110278159379959,
          0.21080586314201355,
          0.21087238192558289,
          0.21068710088729858,
          0.21007849276065826,
          0.21018356084823608,
          0.2098621129989624,
          0.20987896621227264,
          0.20943163335323334,
          0.20947134494781494,
          0.20934981107711792,
          0.20928551256656647,
          0.2088765650987625,
          0.20881035923957825,
          0.20873546600341797,
          0.20862329006195068,
          0.2082928866147995,
          0.2080586850643158,
          0.20812851190567017,
          0.20797109603881836,
          0.20798443257808685,
          0.20757798850536346,
          0.2074957937002182,
          0.20741324126720428,
          0.20697933435440063,
          0.20715513825416565,
          0.20713622868061066,
          0.206980362534523,
          0.20673242211341858,
          0.2065526247024536,
          0.2067699283361435,
          0.20649421215057373,
          0.20622608065605164,
          0.20614954829216003,
          0.2059309184551239,
          0.2058289796113968,
          0.20581229031085968,
          0.2058461755514145,
          0.20551341772079468,
          0.2053167074918747,
          0.20517660677433014,
          0.20484896004199982,
          0.2049075961112976,
          0.2050129473209381,
          0.2048403024673462,
          0.20465676486492157,
          0.20440416038036346,
          0.20421089231967926,
          0.20442815124988556,
          0.20379303395748138,
          0.2044626921415329,
          0.20386071503162384,
          0.20391640067100525,
          0.20358014106750488,
          0.20353640615940094,
          0.20345494151115417,
          0.20334498584270477,
          0.20348885655403137,
          0.20303848385810852,
          0.20304645597934723,
          0.20277175307273865,
          0.20264045894145966,
          0.20282170176506042,
          0.20277801156044006,
          0.2026168256998062,
          0.20250782370567322,
          0.20211873948574066,
          0.20236998796463013,
          0.20196323096752167,
          0.20185397565364838,
          0.2019326090812683,
          0.20163534581661224,
          0.201531320810318,
          0.2014874666929245,
          0.20148572325706482,
          0.2013835310935974,
          0.20125772058963776,
          0.20100848376750946,
          0.20100805163383484,
          0.20091544091701508,
          0.20089936256408691,
          0.20062975585460663,
          0.20068712532520294,
          0.2005259245634079,
          0.2003924399614334,
          0.20035569369792938,
          0.20026451349258423,
          0.19998575747013092,
          0.2000523954629898,
          0.1998215615749359,
          0.1997440755367279,
          0.19987133145332336,
          0.19949087500572205,
          0.19927164912223816,
          0.19951428472995758,
          0.19926513731479645,
          0.19926360249519348,
          0.199395090341568,
          0.1987391859292984,
          0.19883251190185547,
          0.1988038718700409,
          0.19878461956977844,
          0.19877523183822632,
          0.19870589673519135,
          0.19838587939739227,
          0.19834265112876892,
          0.19843333959579468,
          0.198016956448555,
          0.19811813533306122,
          0.1979479342699051,
          0.19819138944149017,
          0.19765225052833557,
          0.19787028431892395,
          0.1976456493139267,
          0.19763614237308502,
          0.1973777562379837,
          0.1973324418067932,
          0.1972142904996872,
          0.19708067178726196,
          0.19721689820289612,
          0.1970321387052536,
          0.19712430238723755,
          0.19677898287773132,
          0.1967153698205948,
          0.19675768911838531,
          0.1966724395751953,
          0.1966055929660797,
          0.1964256763458252,
          0.19617055356502533,
          0.19610510766506195,
          0.19619694352149963,
          0.19618044793605804,
          0.19603079557418823,
          0.19573445618152618,
          0.1959293633699417,
          0.19633205235004425,
          0.19600066542625427,
          0.19551897048950195,
          0.19550757110118866,
          0.1956358551979065,
          0.19517236948013306,
          0.19516348838806152,
          0.19533678889274597,
          0.19496695697307587,
          0.19544151425361633,
          0.19495247304439545,
          0.19477921724319458,
          0.19468531012535095,
          0.19460278749465942,
          0.19488316774368286,
          0.19454476237297058,
          0.1944044679403305,
          0.19447064399719238,
          0.19434453547000885,
          0.1942211389541626,
          0.19415751099586487,
          0.19415099918842316,
          0.1940993219614029,
          0.1939845085144043,
          0.1936611831188202,
          0.19386574625968933,
          0.19349785149097443,
          0.1935928761959076,
          0.1932675987482071,
          0.19344347715377808,
          0.19331413507461548,
          0.19324949383735657,
          0.19315950572490692,
          0.19300174713134766,
          0.19303205609321594,
          0.19337943196296692,
          0.19289329648017883,
          0.19288547337055206,
          0.1927497833967209,
          0.19266827404499054,
          0.19261611998081207,
          0.19246919453144073,
          0.19250603020191193,
          0.19245688617229462,
          0.1922166645526886,
          0.19258105754852295,
          0.19209612905979156,
          0.19198009371757507,
          0.19205673038959503,
          0.1920361965894699,
          0.1918594241142273,
          0.19161513447761536,
          0.19168715178966522,
          0.1916830986738205,
          0.19148816168308258,
          0.19143186509609222,
          0.19144009053707123,
          0.19145594537258148,
          0.1914013773202896,
          0.19108176231384277,
          0.19106657803058624,
          0.19108663499355316,
          0.19095470011234283,
          0.19083911180496216,
          0.1908699870109558,
          0.19068214297294617,
          0.19077008962631226,
          0.1907690465450287,
          0.19046732783317566,
          0.19052454829216003,
          0.19035464525222778,
          0.19017529487609863,
          0.19033144414424896,
          0.19018302857875824,
          0.18995048105716705,
          0.1901429444551468,
          0.18973688781261444,
          0.18990841507911682,
          0.18985465168952942,
          0.1897931694984436,
          0.1894555240869522,
          0.18934376537799835,
          0.18945254385471344,
          0.18961548805236816,
          0.18925373256206512,
          0.1894080638885498,
          0.1893010437488556,
          0.18961898982524872,
          0.1891530603170395,
          0.1888938546180725,
          0.1890168935060501,
          0.188785120844841,
          0.188979372382164,
          0.18881867825984955,
          0.18887656927108765,
          0.18855087459087372,
          0.18854784965515137,
          0.18847769498825073,
          0.188505157828331,
          0.18831993639469147,
          0.1880989670753479,
          0.18817193806171417,
          0.18812139332294464,
          0.188202902674675,
          0.187850683927536,
          0.18784616887569427,
          0.1878642737865448,
          0.18769419193267822,
          0.18767580389976501,
          0.18744401633739471,
          0.1874568611383438,
          0.18729446828365326,
          0.18754151463508606,
          0.1875758171081543,
          0.18730683624744415,
          0.18692496418952942,
          0.18730799853801727,
          0.1869579702615738,
          0.18705397844314575,
          0.18686193227767944,
          0.18675154447555542,
          0.1866496503353119,
          0.1865449845790863,
          0.18664765357971191,
          0.18627652525901794,
          0.1864786297082901,
          0.18650849163532257,
          0.18611837923526764,
          0.1861928105354309,
          0.18637089431285858,
          0.18598659336566925,
          0.18589146435260773,
          0.18631619215011597,
          0.1859777569770813,
          0.18561099469661713,
          0.18565733730793,
          0.185951828956604,
          0.18562549352645874,
          0.1854996383190155,
          0.18543867766857147,
          0.18540287017822266,
          0.18533003330230713,
          0.18522456288337708,
          0.18546706438064575,
          0.18505345284938812,
          0.18477050960063934,
          0.1851121038198471,
          0.18480221927165985,
          0.184715136885643,
          0.18458892405033112,
          0.18475934863090515,
          0.18475094437599182,
          0.18446004390716553,
          0.18435533344745636,
          0.18424679338932037,
          0.18408890068531036,
          0.18418550491333008,
          0.1840265542268753,
          0.18422330915927887,
          0.18403054773807526,
          0.18385159969329834,
          0.18358014523983002,
          0.18386730551719666,
          0.18350443243980408,
          0.18357045948505402,
          0.18363109230995178,
          0.18318741023540497,
          0.18354560434818268,
          0.1833462417125702,
          0.18355512619018555,
          0.18332205712795258,
          0.18322525918483734,
          0.18316683173179626,
          0.183038592338562,
          0.1831560879945755,
          0.18313919007778168,
          0.18301613628864288,
          0.18263964354991913,
          0.1827450543642044,
          0.18244077265262604,
          0.1824265718460083,
          0.18242600560188293,
          0.18226441740989685,
          0.18220610916614532,
          0.1819526106119156,
          0.18210436403751373,
          0.18192598223686218,
          0.18196256458759308,
          0.18195036053657532,
          0.1818268895149231,
          0.18193843960762024,
          0.1816374659538269,
          0.18170198798179626,
          0.18157000839710236,
          0.1814054697751999,
          0.18168655037879944,
          0.1813826709985733,
          0.18136627972126007,
          0.1810387670993805,
          0.18098187446594238,
          0.18105614185333252,
          0.18082834780216217,
          0.1811251938343048,
          0.18084374070167542,
          0.1807972937822342,
          0.18053437769412994,
          0.18049339950084686,
          0.18061193823814392,
          0.18030104041099548,
          0.1803983896970749,
          0.18017776310443878,
          0.1801966279745102,
          0.18041323125362396,
          0.18016429245471954,
          0.1801329106092453,
          0.1798977106809616,
          0.17995218932628632,
          0.179923415184021,
          0.17990167438983917,
          0.17968407273292542,
          0.1798962652683258,
          0.17972812056541443,
          0.17939677834510803,
          0.1795443892478943,
          0.17934221029281616,
          0.17937947809696198,
          0.17930862307548523,
          0.179496631026268,
          0.17937350273132324,
          0.17938651144504547,
          0.179181769490242,
          0.17898455262184143,
          0.17879317700862885,
          0.17870379984378815,
          0.17885945737361908,
          0.17871378362178802,
          0.17857462167739868,
          0.17827706038951874,
          0.17859168350696564,
          0.1784277856349945,
          0.17870068550109863,
          0.17812201380729675,
          0.17795541882514954,
          0.17813953757286072,
          0.1782209426164627,
          0.17802003026008606,
          0.17802925407886505,
          0.17778785526752472,
          0.17772795259952545,
          0.177851140499115,
          0.17752374708652496,
          0.17732536792755127,
          0.1776110678911209,
          0.17761681973934174,
          0.17719754576683044,
          0.1771892011165619,
          0.1773756891489029,
          0.17711776494979858,
          0.17716072499752045,
          0.1770150512456894,
          0.1769539713859558,
          0.1767461895942688,
          0.176866814494133,
          0.17658811807632446,
          0.17652693390846252,
          0.17661452293395996,
          0.1764616221189499,
          0.17658086121082306,
          0.1764504462480545,
          0.176239475607872,
          0.17636610567569733,
          0.17613449692726135,
          0.17618831992149353,
          0.17614296078681946,
          0.1760607212781906,
          0.1760086566209793,
          0.1757209599018097,
          0.1759290248155594,
          0.17614957690238953,
          0.17563802003860474,
          0.17566394805908203,
          0.17552322149276733,
          0.17544914782047272,
          0.17540563642978668,
          0.17539571225643158,
          0.17523059248924255,
          0.17499473690986633,
          0.17524434626102448,
          0.1757640540599823,
          0.1753702163696289,
          0.17520463466644287,
          0.17480389773845673,
          0.1748666763305664,
          0.17470122873783112,
          0.17446769773960114,
          0.1745785027742386,
          0.17441348731517792,
          0.17436803877353668,
          0.17441149055957794,
          0.17449802160263062,
          0.1743655651807785,
          0.1745399683713913,
          0.17418022453784943,
          0.17442020773887634,
          0.17409034073352814,
          0.17395292222499847,
          0.1742236316204071,
          0.17379987239837646,
          0.1739835888147354,
          0.1737849861383438,
          0.17362095415592194,
          0.1735178828239441,
          0.1734132617712021,
          0.17356754839420319,
          0.1737482100725174,
          0.17342615127563477,
          0.1729782223701477,
          0.17358991503715515,
          0.17323759198188782,
          0.17307166755199432,
          0.17331823706626892,
          0.17288614809513092,
          0.17294053733348846,
          0.17304790019989014,
          0.17283940315246582,
          0.17275750637054443,
          0.17283740639686584,
          0.17275846004486084,
          0.17247748374938965,
          0.1723453551530838,
          0.17228588461875916,
          0.17261871695518494,
          0.1721222847700119,
          0.17214316129684448,
          0.17199446260929108,
          0.1720491647720337,
          0.17224282026290894,
          0.1718662977218628,
          0.1723165065050125,
          0.17186017334461212,
          0.17185981571674347,
          0.17153732478618622,
          0.1716652810573578,
          0.17157208919525146,
          0.1716357171535492,
          0.1713705211877823,
          0.17159639298915863,
          0.17137227952480316,
          0.171129509806633,
          0.17112493515014648,
          0.17125585675239563,
          0.171095609664917,
          0.1710016131401062,
          0.17088624835014343,
          0.17089784145355225,
          0.17086394131183624,
          0.17088617384433746,
          0.17060807347297668,
          0.1706649512052536,
          0.17064817249774933,
          0.17041893303394318,
          0.17052792012691498,
          0.17042690515518188,
          0.17068374156951904,
          0.17029353976249695,
          0.16998641192913055,
          0.17020753026008606,
          0.16996529698371887,
          0.1698819249868393,
          0.16980686783790588,
          0.16992877423763275,
          0.169857919216156,
          0.16984374821186066,
          0.16976255178451538,
          0.16946722567081451,
          0.1696070283651352,
          0.16949225962162018,
          0.16935376822948456,
          0.16981105506420135,
          0.16932623088359833,
          0.16907106339931488,
          0.1689741611480713,
          0.16891105473041534,
          0.1689406782388687,
          0.16900798678398132,
          0.16901203989982605,
          0.168913334608078,
          0.16896510124206543,
          0.16891339421272278,
          0.1686224788427353,
          0.16867870092391968,
          0.16849714517593384,
          0.1687769889831543,
          0.16818644106388092,
          0.16838820278644562,
          0.16835272312164307,
          0.16811947524547577,
          0.16813446581363678,
          0.16816070675849915,
          0.16800183057785034,
          0.16808383166790009,
          0.16792498528957367,
          0.16781528294086456,
          0.16783419251441956,
          0.1674661785364151,
          0.16778546571731567,
          0.16781963407993317,
          0.16769178211688995,
          0.167479008436203,
          0.16754098236560822,
          0.16734036803245544,
          0.1674221158027649,
          0.16761894524097443,
          0.16728292405605316,
          0.16720545291900635,
          0.1667855679988861,
          0.1669044941663742,
          0.16694410145282745,
          0.1667417585849762,
          0.16685177385807037,
          0.16649824380874634,
          0.16685369610786438,
          0.16654245555400848,
          0.16665492951869965,
          0.16653381288051605,
          0.166391059756279,
          0.1663346141576767,
          0.1662931889295578,
          0.16618163883686066,
          0.16624991595745087,
          0.16619402170181274,
          0.16618938744068146,
          0.16594982147216797,
          0.166133850812912,
          0.16594743728637695,
          0.16582027077674866,
          0.1656438410282135,
          0.16543704271316528,
          0.16551344096660614,
          0.16558541357517242,
          0.16547800600528717,
          0.1652754694223404,
          0.16534923017024994,
          0.16535329818725586,
          0.16521289944648743,
          0.16519634425640106,
          0.16530193388462067,
          0.16536936163902283,
          0.16500604152679443,
          0.1648981273174286,
          0.16481854021549225,
          0.16476672887802124,
          0.16458339989185333,
          0.16472987830638885,
          0.16464580595493317,
          0.16452105343341827,
          0.16453230381011963,
          0.1642839014530182,
          0.16428923606872559,
          0.16473719477653503,
          0.16427624225616455,
          0.16438350081443787,
          0.1641593724489212,
          0.1641572117805481,
          0.16409815847873688,
          0.16394779086112976,
          0.1638767272233963,
          0.16375377774238586,
          0.1636190116405487,
          0.16355954110622406,
          0.1639067828655243,
          0.16361860930919647,
          0.16347958147525787,
          0.16363027691841125,
          0.163334921002388,
          0.16329242289066315,
          0.16347916424274445,
          0.16322094202041626,
          0.16302764415740967,
          0.1629590094089508,
          0.16301245987415314,
          0.16277636587619781,
          0.1628677099943161,
          0.16268238425254822,
          0.16274255514144897,
          0.16262957453727722,
          0.1625186651945114,
          0.1627340316772461,
          0.16255952417850494,
          0.16236983239650726,
          0.162686288356781,
          0.16200415790081024,
          0.16252875328063965,
          0.16234491765499115,
          0.1621490865945816,
          0.16213232278823853,
          0.16203303635120392,
          0.16201291978359222,
          0.16183891892433167,
          0.16201472282409668,
          0.16182252764701843,
          0.16175463795661926,
          0.16153694689273834,
          0.16154414415359497,
          0.1613084077835083,
          0.16167140007019043,
          0.1612986922264099,
          0.16148336231708527,
          0.16124862432479858,
          0.16120745241641998,
          0.16102783381938934,
          0.1610206514596939,
          0.16096754372119904,
          0.1610700488090515,
          0.16116328537464142,
          0.1611594408750534,
          0.16080939769744873,
          0.16065017879009247,
          0.1609679013490677,
          0.16058124601840973,
          0.1603243350982666,
          0.16039550304412842,
          0.16013312339782715,
          0.16039110720157623,
          0.16008979082107544,
          0.1600187122821808,
          0.16037048399448395,
          0.16015948355197906,
          0.15999628603458405,
          0.1601363718509674,
          0.15996509790420532,
          0.15993726253509521,
          0.15991316735744476,
          0.1603505164384842,
          0.15957388281822205,
          0.15944337844848633,
          0.15966901183128357,
          0.1594139039516449,
          0.15968401730060577,
          0.15944130718708038,
          0.15922006964683533,
          0.15946190059185028,
          0.15921799838542938,
          0.15913830697536469,
          0.158959299325943,
          0.15905773639678955,
          0.15900114178657532,
          0.15860971808433533,
          0.15915678441524506,
          0.1588667929172516,
          0.15855634212493896,
          0.15879088640213013,
          0.15917129814624786,
          0.15844516456127167,
          0.158264622092247,
          0.15828979015350342,
          0.1581616997718811,
          0.15852303802967072,
          0.15829749405384064,
          0.15794260799884796,
          0.15821091830730438,
          0.1578335165977478,
          0.15792492032051086,
          0.1580127626657486,
          0.1579100787639618,
          0.15739089250564575,
          0.15765894949436188,
          0.157626211643219,
          0.1574912667274475,
          0.15746378898620605,
          0.1573757380247116,
          0.15752415359020233,
          0.15723221004009247,
          0.1573297679424286,
          0.15704941749572754,
          0.15717533230781555,
          0.1571519821882248,
          0.15691740810871124,
          0.15712600946426392,
          0.15682870149612427,
          0.15702198445796967,
          0.1569102257490158,
          0.1567975878715515,
          0.15652529895305634,
          0.1567307412624359,
          0.15653066337108612,
          0.1567862182855606,
          0.1567029058933258,
          0.1562308520078659,
          0.15615706145763397,
          0.15615151822566986,
          0.15612676739692688,
          0.15633811056613922,
          0.15600787103176117,
          0.15593820810317993,
          0.1559845209121704,
          0.15595164895057678,
          0.15585747361183167,
          0.15588517487049103,
          0.15578971803188324,
          0.1558264046907425,
          0.15547935664653778,
          0.1556883305311203,
          0.15557123720645905,
          0.15532135963439941,
          0.15558841824531555,
          0.15551875531673431,
          0.15498118102550507,
          0.15504716336727142,
          0.15519990026950836,
          0.1549094319343567,
          0.15505696833133698,
          0.15516971051692963,
          0.15509502589702606,
          0.1547745168209076,
          0.15473036468029022,
          0.15478163957595825,
          0.15446069836616516,
          0.1543402522802353,
          0.15459825098514557,
          0.1543092280626297,
          0.15421031415462494,
          0.15461936593055725,
          0.15421442687511444,
          0.15382902324199677,
          0.15420690178871155,
          0.15402768552303314,
          0.15384595096111298,
          0.15383155643939972,
          0.15383386611938477,
          0.15379633009433746,
          0.1535693109035492,
          0.15385696291923523,
          0.15338799357414246,
          0.15359167754650116,
          0.15327778458595276,
          0.1536683887243271,
          0.15326125919818878,
          0.15341641008853912,
          0.15327177941799164,
          0.15326808393001556,
          0.15289144217967987,
          0.15312086045742035,
          0.1531042456626892,
          0.15285399556159973,
          0.15300431847572327,
          0.15273334085941315,
          0.15270355343818665,
          0.15279358625411987,
          0.15256616473197937,
          0.15274159610271454,
          0.1524246782064438,
          0.15252244472503662,
          0.15258276462554932,
          0.15235592424869537,
          0.15230073034763336,
          0.1522679328918457,
          0.15210209786891937,
          0.15224537253379822,
          0.151901975274086,
          0.15195852518081665,
          0.15172676742076874,
          0.15218093991279602,
          0.15180718898773193,
          0.1522233784198761,
          0.15187184512615204,
          0.15154294669628143,
          0.15150876343250275,
          0.15148861706256866,
          0.15151919424533844,
          0.15135075151920319,
          0.15125074982643127,
          0.15106995403766632,
          0.15114443004131317,
          0.1509869247674942,
          0.15105602145195007,
          0.1512599140405655,
          0.15081341564655304,
          0.15075275301933289,
          0.1507379710674286,
          0.15062730014324188,
          0.15087902545928955,
          0.15057942271232605,
          0.1503882110118866,
          0.15068548917770386,
          0.15026356279850006,
          0.15029814839363098,
          0.1501450538635254,
          0.1500505656003952,
          0.15013419091701508,
          0.1503695845603943,
          0.15021494030952454,
          0.14998121559619904,
          0.15007150173187256,
          0.1497681736946106,
          0.1495905965566635,
          0.14954350888729095,
          0.14959165453910828,
          0.14932884275913239,
          0.1496625691652298,
          0.14956645667552948,
          0.14971579611301422,
          0.14930926263332367,
          0.1492491513490677,
          0.14928464591503143,
          0.14947690069675446,
          0.14950443804264069,
          0.14922308921813965,
          0.1490982472896576,
          0.14922937750816345,
          0.14893418550491333,
          0.14888811111450195,
          0.14881983399391174,
          0.14898259937763214,
          0.14864638447761536,
          0.14839857816696167,
          0.1485881805419922,
          0.1485302597284317,
          0.14824064075946808,
          0.14854855835437775,
          0.14849728345870972,
          0.14796428382396698,
          0.14835897088050842,
          0.14776702225208282,
          0.14829084277153015,
          0.14798825979232788,
          0.1479315608739853,
          0.14754439890384674,
          0.1475783884525299,
          0.14792658388614655,
          0.14758169651031494,
          0.14744889736175537,
          0.14748859405517578,
          0.14743132889270782,
          0.147638738155365,
          0.14735053479671478,
          0.1472131758928299,
          0.1472020298242569,
          0.14727292954921722,
          0.14750900864601135,
          0.1469714641571045,
          0.1468198299407959,
          0.14695045351982117,
          0.14694470167160034,
          0.1466933786869049,
          0.1469741016626358,
          0.1466091126203537,
          0.1467895656824112
         ],
         "type": "scatter"
        },
        {
         "mode": "lines",
         "name": "Validation Loss",
         "y": [
          0.5577536225318909,
          0.339071124792099,
          0.31009599566459656,
          0.2999083399772644,
          0.29371178150177,
          0.2896368205547333,
          0.28482308983802795,
          0.28102415800094604,
          0.2784905433654785,
          0.27612775564193726,
          0.275163859128952,
          0.2744704484939575,
          0.27167797088623047,
          0.27264052629470825,
          0.26943784952163696,
          0.26973479986190796,
          0.2677547335624695,
          0.26812833547592163,
          0.26692086458206177,
          0.2670416533946991,
          0.2653546631336212,
          0.2646435797214508,
          0.2645893096923828,
          0.26397332549095154,
          0.2652488052845001,
          0.26499509811401367,
          0.26308777928352356,
          0.26314204931259155,
          0.2629534900188446,
          0.2615281641483307,
          0.2610929310321808,
          0.2612029016017914,
          0.2606884241104126,
          0.26010429859161377,
          0.2602769732475281,
          0.2599680423736572,
          0.25950679183006287,
          0.2611534297466278,
          0.26054784655570984,
          0.2592347264289856,
          0.2588599622249603,
          0.2591306269168854,
          0.2586840093135834,
          0.25847774744033813,
          0.2580113708972931,
          0.25807586312294006,
          0.25852957367897034,
          0.2582063674926758,
          0.25683894753456116,
          0.25752732157707214,
          0.25750306248664856,
          0.2570020258426666,
          0.25698140263557434,
          0.255844384431839,
          0.25575685501098633,
          0.25563111901283264,
          0.2557520568370819,
          0.2574913501739502,
          0.25633305311203003,
          0.25421613454818726,
          0.2545816898345947,
          0.2541423738002777,
          0.2538098096847534,
          0.25504791736602783,
          0.25371432304382324,
          0.25309938192367554,
          0.2534714937210083,
          0.25527599453926086,
          0.2526927888393402,
          0.25235673785209656,
          0.25197044014930725,
          0.25240272283554077,
          0.25192075967788696,
          0.25291284918785095,
          0.2517518401145935,
          0.2520715296268463,
          0.2506338357925415,
          0.25069886445999146,
          0.2510135769844055,
          0.25010624527931213,
          0.2505759000778198,
          0.25047799944877625,
          0.24998745322227478,
          0.250413179397583,
          0.2499881386756897,
          0.2514103651046753,
          0.2500973641872406,
          0.24966177344322205,
          0.25152525305747986,
          0.250232458114624,
          0.2487337291240692,
          0.2487909495830536,
          0.2499522566795349,
          0.24888025224208832,
          0.2483479380607605,
          0.24907124042510986,
          0.25026068091392517,
          0.24829572439193726,
          0.2477368265390396,
          0.24761323630809784,
          0.24743668735027313,
          0.24752260744571686,
          0.248231440782547,
          0.24701088666915894,
          0.24776911735534668,
          0.24831274151802063,
          0.24769584834575653,
          0.2466607391834259,
          0.24683232605457306,
          0.24698711931705475,
          0.24762828648090363,
          0.24755145609378815,
          0.24712687730789185,
          0.2470189481973648,
          0.2465014010667801,
          0.24655219912528992,
          0.24767786264419556,
          0.245757058262825,
          0.24741235375404358,
          0.24599404633045197,
          0.24654099345207214,
          0.24644581973552704,
          0.24670842289924622,
          0.24632132053375244,
          0.24672117829322815,
          0.24587473273277283,
          0.2457422912120819,
          0.245361790060997,
          0.24574777483940125,
          0.2454267293214798,
          0.24620582163333893,
          0.24579790234565735,
          0.24566388130187988,
          0.24565698206424713,
          0.246151864528656,
          0.2457714080810547,
          0.24702996015548706,
          0.24523764848709106,
          0.2458050698041916,
          0.2469864785671234,
          0.2457498013973236,
          0.24463191628456116,
          0.24633200466632843,
          0.24479511380195618,
          0.2449924200773239,
          0.24483300745487213,
          0.24523940682411194,
          0.24623502790927887,
          0.24528995156288147,
          0.24436385929584503,
          0.24489673972129822,
          0.2453327476978302,
          0.24487905204296112,
          0.24482373893260956,
          0.24471880495548248,
          0.24495624005794525,
          0.244581937789917,
          0.24531202018260956,
          0.2446054071187973,
          0.24576760828495026,
          0.24459008872509003,
          0.24582354724407196,
          0.2443810999393463,
          0.2441498190164566,
          0.24371567368507385,
          0.24422387778759003,
          0.24643610417842865,
          0.24401411414146423,
          0.24600127339363098,
          0.24465720355510712,
          0.24627891182899475,
          0.24422036111354828,
          0.24400566518306732,
          0.24413082003593445,
          0.2435741126537323,
          0.24340158700942993,
          0.2437686175107956,
          0.2445620745420456,
          0.2435784488916397,
          0.24391727149486542,
          0.2433479130268097,
          0.24358561635017395,
          0.24330423772335052,
          0.24358497560024261,
          0.24408414959907532,
          0.24307961761951447,
          0.24306568503379822,
          0.24419209361076355,
          0.245030015707016,
          0.24365182220935822,
          0.24452219903469086,
          0.24307896196842194,
          0.2435062825679779,
          0.24312105774879456,
          0.24321489036083221,
          0.24377281963825226,
          0.24339699745178223,
          0.2437281310558319,
          0.2448597252368927,
          0.2432638257741928,
          0.24248145520687103,
          0.24308450520038605,
          0.2428227663040161,
          0.24385347962379456,
          0.24473580718040466,
          0.24293500185012817,
          0.24265719950199127,
          0.24266746640205383,
          0.24365904927253723,
          0.24216699600219727,
          0.24206523597240448,
          0.24279473721981049,
          0.24221231043338776,
          0.24733348190784454,
          0.24247358739376068,
          0.24278593063354492,
          0.2434200644493103,
          0.24467821419239044,
          0.24291422963142395,
          0.24285855889320374,
          0.2430879920721054,
          0.2428017109632492,
          0.24383795261383057,
          0.2430219203233719,
          0.24242065846920013,
          0.24299225211143494,
          0.24347741901874542,
          0.24204513430595398,
          0.2419922798871994,
          0.24334979057312012,
          0.2438368797302246,
          0.24249157309532166,
          0.24296218156814575,
          0.24269166588783264,
          0.24203450977802277,
          0.24192261695861816,
          0.24267858266830444,
          0.24205684661865234,
          0.24116648733615875,
          0.2421131283044815,
          0.24165184795856476,
          0.24194905161857605,
          0.24145552515983582,
          0.2434113621711731,
          0.24210363626480103,
          0.2428509145975113,
          0.24127385020256042,
          0.2430378496646881,
          0.24195829033851624,
          0.24209651350975037,
          0.24222539365291595,
          0.24328117072582245,
          0.24239514768123627,
          0.24143870174884796,
          0.24151454865932465,
          0.24137639999389648,
          0.2426658272743225,
          0.2412714809179306,
          0.24149179458618164,
          0.241859570145607,
          0.24546599388122559,
          0.2421339601278305,
          0.24240241944789886,
          0.24186739325523376,
          0.2419823855161667,
          0.24313701689243317,
          0.2420908659696579,
          0.2421714961528778,
          0.24124696850776672,
          0.24173149466514587,
          0.2414630800485611,
          0.24292060732841492,
          0.24246858060359955,
          0.2410895973443985,
          0.24130530655384064,
          0.24356724321842194,
          0.24180415272712708,
          0.24130591750144958,
          0.24111513793468475,
          0.24101583659648895,
          0.24053561687469482,
          0.24113401770591736,
          0.241253063082695,
          0.24167346954345703,
          0.24209509789943695,
          0.24144814908504486,
          0.24070365726947784,
          0.2409166842699051,
          0.24190174043178558,
          0.24195098876953125,
          0.24044981598854065,
          0.2439405471086502,
          0.24235132336616516,
          0.2420675903558731,
          0.24195130169391632,
          0.2418700009584427,
          0.24217082560062408,
          0.24151037633419037,
          0.24052585661411285,
          0.2404003143310547,
          0.24093180894851685,
          0.24059027433395386,
          0.2415626049041748,
          0.2405877709388733,
          0.24071115255355835,
          0.24128976464271545,
          0.24137833714485168,
          0.24173414707183838,
          0.24012257158756256,
          0.24215765297412872,
          0.24051354825496674,
          0.2407982051372528,
          0.24065405130386353,
          0.24067969620227814,
          0.2404695451259613,
          0.24065642058849335,
          0.24284297227859497,
          0.24077163636684418,
          0.24060234427452087,
          0.24146927893161774,
          0.24285198748111725,
          0.2416309267282486,
          0.24086102843284607,
          0.24142451584339142,
          0.2404392808675766,
          0.24011468887329102,
          0.24132190644741058,
          0.24045632779598236,
          0.2409052848815918,
          0.2410467565059662,
          0.2405320256948471,
          0.2403862327337265,
          0.24004323780536652,
          0.24132037162780762,
          0.2406395524740219,
          0.24039143323898315,
          0.24062024056911469,
          0.24129687249660492,
          0.241151824593544,
          0.2405523806810379,
          0.24150019884109497,
          0.24024589359760284,
          0.24009642004966736,
          0.23992809653282166,
          0.24086907505989075,
          0.24052874743938446,
          0.24233099818229675,
          0.2408612072467804,
          0.24055072665214539,
          0.23954711854457855,
          0.24019891023635864,
          0.24040888249874115,
          0.24089692533016205,
          0.23938457667827606,
          0.23958103358745575,
          0.23949922621250153,
          0.24104513227939606,
          0.24120770394802094,
          0.24098895490169525,
          0.2394506335258484,
          0.2400081902742386,
          0.23961184918880463,
          0.23944731056690216,
          0.24026460945606232,
          0.24012236297130585,
          0.23963041603565216,
          0.2399541735649109,
          0.23998992145061493,
          0.2397262156009674,
          0.23901888728141785,
          0.23938828706741333,
          0.24118976294994354,
          0.2399146556854248,
          0.24014189839363098,
          0.23989540338516235,
          0.24033121764659882,
          0.23981864750385284,
          0.24029874801635742,
          0.23989535868167877,
          0.2407832145690918,
          0.23930144309997559,
          0.239353209733963,
          0.2391655296087265,
          0.23888681828975677,
          0.23914740979671478,
          0.24016651511192322,
          0.23938411474227905,
          0.23932552337646484,
          0.24006414413452148,
          0.23926380276679993,
          0.2396022081375122,
          0.24102260172367096,
          0.23958639800548553,
          0.23954463005065918,
          0.23894482851028442,
          0.23953381180763245,
          0.2387588620185852,
          0.24042776226997375,
          0.24154958128929138,
          0.23999282717704773,
          0.2394588440656662,
          0.23891581594944,
          0.2387709766626358,
          0.2395986169576645,
          0.2410506308078766,
          0.238785982131958,
          0.239436075091362,
          0.23907113075256348,
          0.23939429223537445,
          0.23850958049297333,
          0.24197149276733398,
          0.23904640972614288,
          0.23938482999801636,
          0.2382088154554367,
          0.2398093342781067,
          0.23828299343585968,
          0.23899920284748077,
          0.2400258630514145,
          0.24063321948051453,
          0.2385208010673523,
          0.24079415202140808,
          0.23962825536727905,
          0.23895494639873505,
          0.23880384862422943,
          0.23905117809772491,
          0.23916175961494446,
          0.2407066524028778,
          0.2391543686389923,
          0.23919783532619476,
          0.23877589404582977,
          0.23839814960956573,
          0.23840957880020142,
          0.23905713856220245,
          0.239014133810997,
          0.2406785786151886,
          0.2379690259695053,
          0.23861761391162872,
          0.23898302018642426,
          0.24004653096199036,
          0.23897841572761536,
          0.23889388144016266,
          0.23876185715198517,
          0.2394254207611084,
          0.2385905385017395,
          0.23887354135513306,
          0.23860791325569153,
          0.23833677172660828,
          0.2378377765417099,
          0.23962615430355072,
          0.23842287063598633,
          0.2390638291835785,
          0.23859573900699615,
          0.23766668140888214,
          0.23853112757205963,
          0.23825062811374664,
          0.239517942070961,
          0.2384650856256485,
          0.23890230059623718,
          0.23872146010398865,
          0.237869992852211,
          0.23810112476348877,
          0.23799309134483337,
          0.23960751295089722,
          0.2390974909067154,
          0.23836538195610046,
          0.23938818275928497,
          0.23898382484912872,
          0.23788663744926453,
          0.23982936143875122,
          0.23866625130176544,
          0.2380237877368927,
          0.23846128582954407,
          0.2387348860502243,
          0.23802006244659424,
          0.23828889429569244,
          0.23868519067764282,
          0.23964187502861023,
          0.23849721252918243,
          0.23938466608524323,
          0.23953314125537872,
          0.2384748011827469,
          0.2409011870622635,
          0.2381715029478073,
          0.23832572996616364,
          0.23988394439220428,
          0.23912273347377777,
          0.23856429755687714,
          0.24025991559028625,
          0.23813846707344055,
          0.23769359290599823,
          0.2393598109483719,
          0.23837028443813324,
          0.2386389970779419,
          0.23757433891296387,
          0.23813998699188232,
          0.2404414564371109,
          0.2386179268360138,
          0.23867666721343994,
          0.23810389637947083,
          0.23906287550926208,
          0.23788082599639893,
          0.2378867119550705,
          0.23856203258037567,
          0.23992425203323364,
          0.239182248711586,
          0.2378186732530594,
          0.23856204748153687,
          0.23850083351135254,
          0.2382819801568985,
          0.23803380131721497,
          0.23819397389888763,
          0.2391485869884491,
          0.23810306191444397,
          0.23811644315719604,
          0.24078811705112457,
          0.23967048525810242,
          0.23866736888885498,
          0.23806244134902954,
          0.2395155280828476,
          0.23838865756988525,
          0.23855605721473694,
          0.23873844742774963,
          0.23915258049964905,
          0.23866897821426392,
          0.24025490880012512,
          0.23872019350528717,
          0.23818844556808472,
          0.2391604483127594,
          0.237920880317688,
          0.23976567387580872,
          0.23859727382659912,
          0.24132120609283447,
          0.238157719373703,
          0.2391894906759262,
          0.2392142117023468,
          0.2391425520181656,
          0.23812490701675415,
          0.23844611644744873,
          0.23942060768604279,
          0.23810744285583496,
          0.2383359968662262,
          0.2401413470506668,
          0.23850573599338531,
          0.23836758732795715,
          0.23824982345104218,
          0.2385208010673523,
          0.23992235958576202,
          0.24007533490657806,
          0.23900754749774933,
          0.23975679278373718,
          0.2390984743833542,
          0.2384175807237625,
          0.23964282870292664,
          0.238986074924469,
          0.23921936750411987,
          0.23887750506401062,
          0.23986931145191193,
          0.23865164816379547,
          0.23827716708183289,
          0.23899033665657043,
          0.23892009258270264,
          0.23956161737442017,
          0.23839512467384338,
          0.2394135743379593,
          0.23954303562641144,
          0.23921088874340057,
          0.23901456594467163,
          0.23918978869915009,
          0.23865583539009094,
          0.2389678806066513,
          0.24020755290985107,
          0.238961860537529,
          0.23827305436134338,
          0.2401258945465088,
          0.24033674597740173,
          0.23937760293483734,
          0.240304633975029,
          0.24037519097328186,
          0.23912861943244934,
          0.23860688507556915,
          0.23901459574699402,
          0.24083511531352997,
          0.23893332481384277,
          0.23886443674564362,
          0.23966552317142487,
          0.2393822968006134,
          0.23897433280944824,
          0.23872247338294983,
          0.23865917325019836,
          0.24124397337436676,
          0.24316607415676117,
          0.2395317256450653,
          0.2397853285074234,
          0.2404181808233261,
          0.23884262144565582,
          0.23919637501239777,
          0.23940862715244293,
          0.2398606240749359,
          0.23910431563854218,
          0.23949739336967468,
          0.23976901173591614,
          0.24042947590351105,
          0.23943594098091125,
          0.24092529714107513,
          0.23956513404846191,
          0.23938366770744324,
          0.23912325501441956,
          0.23962515592575073,
          0.23967665433883667,
          0.23943768441677094,
          0.23971857130527496,
          0.24105817079544067,
          0.24194633960723877,
          0.24063752591609955,
          0.2396215796470642,
          0.2404865026473999,
          0.23918092250823975,
          0.23984505236148834,
          0.2411993145942688,
          0.2402898520231247,
          0.23925621807575226,
          0.24331405758857727,
          0.24113960564136505,
          0.2426200807094574,
          0.24049517512321472,
          0.23988595604896545,
          0.2401292324066162,
          0.23961426317691803,
          0.2397828996181488,
          0.241205632686615,
          0.24080123007297516,
          0.23934856057167053,
          0.23918721079826355,
          0.23973830044269562,
          0.2410411387681961,
          0.239839106798172,
          0.23994088172912598,
          0.24518871307373047,
          0.24113717675209045,
          0.24078680574893951,
          0.23963458836078644,
          0.24093227088451385,
          0.24129270017147064,
          0.2399071753025055,
          0.24059203267097473,
          0.2416418194770813,
          0.2442413866519928,
          0.24050268530845642,
          0.23994430899620056,
          0.2410353273153305,
          0.24096794426441193,
          0.24123039841651917,
          0.24092699587345123,
          0.23970915377140045,
          0.24052435159683228,
          0.2407061755657196,
          0.2407325804233551,
          0.24065889418125153,
          0.24117465317249298,
          0.23988142609596252,
          0.24136118590831757,
          0.24216516315937042,
          0.24448058009147644,
          0.24255460500717163,
          0.2411852478981018,
          0.24043625593185425,
          0.24080432951450348,
          0.24045813083648682,
          0.240189790725708,
          0.24033507704734802,
          0.24096807837486267,
          0.24123576283454895,
          0.24276119470596313,
          0.2409759759902954,
          0.24071262776851654,
          0.24100913107395172,
          0.24103768169879913,
          0.24073193967342377,
          0.24045054614543915,
          0.24405911564826965,
          0.2412816882133484,
          0.2409832775592804,
          0.24264931678771973,
          0.2410782277584076,
          0.24190832674503326,
          0.2409372180700302,
          0.24153394997119904,
          0.2423323392868042,
          0.24168610572814941,
          0.2414589524269104,
          0.24211932718753815,
          0.2411230355501175,
          0.24168281257152557,
          0.24210649728775024,
          0.24149492383003235,
          0.24157509207725525,
          0.2424977719783783,
          0.24243824183940887,
          0.24359837174415588,
          0.24168317019939423,
          0.24218201637268066,
          0.24301107227802277,
          0.24496741592884064,
          0.2414759248495102,
          0.24135154485702515,
          0.24410435557365417,
          0.2417585849761963,
          0.24339252710342407,
          0.24271062016487122,
          0.2456691563129425,
          0.24331411719322205,
          0.24212947487831116,
          0.24390916526317596,
          0.24252495169639587,
          0.2416965812444687,
          0.2446591556072235,
          0.2418009489774704,
          0.2423953115940094,
          0.2427394688129425,
          0.2436048686504364,
          0.24370023608207703,
          0.24203167855739594,
          0.24252130091190338,
          0.24226441979408264,
          0.2431265413761139,
          0.2441667914390564,
          0.24230295419692993,
          0.24338935315608978,
          0.242658331990242,
          0.24341198801994324,
          0.2427951693534851,
          0.24295054376125336,
          0.24297566711902618,
          0.24325428903102875,
          0.2432418018579483,
          0.2440008968114853,
          0.2433483898639679,
          0.2440231442451477,
          0.243186816573143,
          0.24209263920783997,
          0.2434150129556656,
          0.24347500503063202,
          0.2444855123758316,
          0.24432659149169922,
          0.24308931827545166,
          0.24302205443382263,
          0.24389688670635223,
          0.24337556958198547,
          0.2437436431646347,
          0.24409177899360657,
          0.24434712529182434,
          0.24298924207687378,
          0.24398857355117798,
          0.2436162680387497,
          0.24392259120941162,
          0.24612002074718475,
          0.24442429840564728,
          0.24402691423892975,
          0.2437027245759964,
          0.24309474229812622,
          0.24348324537277222,
          0.24509648978710175,
          0.24332958459854126,
          0.24419769644737244,
          0.24446512758731842,
          0.24541975557804108,
          0.2447851598262787,
          0.24333758652210236,
          0.2440384328365326,
          0.24349650740623474,
          0.24455943703651428,
          0.2445807009935379,
          0.2441442757844925,
          0.24503254890441895,
          0.24430084228515625,
          0.24475274980068207,
          0.24573543667793274,
          0.24411100149154663,
          0.24487726390361786,
          0.24475575983524323,
          0.2442418336868286,
          0.2456880658864975,
          0.2443339228630066,
          0.24571789801120758,
          0.24575483798980713,
          0.2466883659362793,
          0.24443431198596954,
          0.24531759321689606,
          0.24561211466789246,
          0.24678590893745422,
          0.2445676326751709,
          0.2446695864200592,
          0.24523146450519562,
          0.24493880569934845,
          0.24957256019115448,
          0.24556829035282135,
          0.2460683435201645,
          0.24600084125995636,
          0.24591541290283203,
          0.24490581452846527,
          0.24510449171066284,
          0.24732455611228943,
          0.24634134769439697,
          0.24513225257396698,
          0.24711422622203827,
          0.24609333276748657,
          0.24913455545902252,
          0.24585865437984467,
          0.24608929455280304,
          0.24696789681911469,
          0.2455751597881317,
          0.24616405367851257,
          0.24716824293136597,
          0.24600021541118622,
          0.24606391787528992,
          0.24791952967643738,
          0.24617215991020203,
          0.24665719270706177,
          0.24799078702926636,
          0.24614176154136658,
          0.24613237380981445,
          0.2459438592195511,
          0.2470976710319519,
          0.24738094210624695,
          0.24888643622398376,
          0.2463371306657791,
          0.2464291900396347,
          0.2475089579820633,
          0.249895840883255,
          0.24662868678569794,
          0.24726566672325134,
          0.2485475093126297,
          0.24680717289447784,
          0.24706214666366577,
          0.24875323474407196,
          0.2507478594779968,
          0.2472953498363495,
          0.24794675409793854,
          0.24709446728229523,
          0.24795907735824585,
          0.24831785261631012,
          0.24842283129692078,
          0.2470754086971283,
          0.24900507926940918,
          0.24752987921237946,
          0.24740903079509735,
          0.24758373200893402,
          0.2488732784986496,
          0.24733561277389526,
          0.24737384915351868,
          0.24778713285923004,
          0.24784809350967407,
          0.24777598679065704,
          0.2475471794605255,
          0.24806128442287445,
          0.2494315654039383,
          0.24930979311466217,
          0.24839729070663452,
          0.2484506219625473,
          0.24789918959140778,
          0.2502194046974182,
          0.24926480650901794,
          0.24838602542877197,
          0.2535361647605896,
          0.2507619261741638,
          0.25008925795555115,
          0.2513226270675659,
          0.248714879155159,
          0.24859163165092468,
          0.24888883531093597,
          0.24835015833377838,
          0.2497567981481552,
          0.2490910440683365,
          0.24885547161102295,
          0.24915388226509094,
          0.24963271617889404,
          0.2495395541191101,
          0.2500424087047577,
          0.2489042580127716,
          0.25070154666900635,
          0.24909931421279907,
          0.2501700520515442,
          0.25012680888175964,
          0.2506726086139679,
          0.2505781352519989,
          0.25038769841194153,
          0.2498146891593933,
          0.2494087517261505,
          0.24984465539455414,
          0.24985718727111816,
          0.2499251663684845,
          0.2507200539112091,
          0.24967019259929657,
          0.2503044307231903,
          0.2502292990684509,
          0.25009381771087646,
          0.2511865794658661,
          0.25141623616218567,
          0.25107109546661377,
          0.2512296140193939,
          0.2524254322052002,
          0.2531741261482239,
          0.250820517539978,
          0.2522872984409332,
          0.2522749900817871,
          0.2521803677082062,
          0.2510448098182678,
          0.2510363459587097,
          0.25127270817756653,
          0.25297221541404724,
          0.2516314685344696,
          0.25155210494995117,
          0.2517569065093994,
          0.25187304615974426,
          0.25534364581108093,
          0.2550453841686249,
          0.25131863355636597,
          0.2516668736934662,
          0.25228992104530334,
          0.25295281410217285,
          0.25166720151901245,
          0.25201642513275146,
          0.25450772047042847,
          0.2524239122867584,
          0.2530165910720825,
          0.2524925172328949,
          0.253106951713562,
          0.2532344460487366,
          0.25429272651672363,
          0.25261420011520386,
          0.2533627152442932,
          0.25373077392578125,
          0.2531139850616455,
          0.25319570302963257,
          0.25310811400413513,
          0.25422149896621704,
          0.25437745451927185,
          0.25346022844314575,
          0.2539040148258209,
          0.2527652084827423,
          0.25678667426109314,
          0.25332731008529663,
          0.25358009338378906,
          0.253669410943985,
          0.2530786395072937,
          0.2537527084350586,
          0.25494855642318726,
          0.2538590431213379,
          0.25341662764549255,
          0.25364577770233154,
          0.25515055656433105,
          0.2554667294025421,
          0.2542749047279358,
          0.2535427510738373,
          0.2544473707675934,
          0.2542342245578766,
          0.2536994218826294,
          0.25400957465171814,
          0.255510538816452,
          0.25884658098220825,
          0.25409454107284546,
          0.2547062039375305,
          0.25512415170669556,
          0.25396186113357544,
          0.25646457076072693,
          0.2563977837562561,
          0.2552054226398468,
          0.25611937046051025,
          0.2550778388977051,
          0.2556599974632263,
          0.2550783157348633,
          0.25601592659950256,
          0.2562010884284973,
          0.25724390149116516,
          0.2557392716407776,
          0.2562582790851593,
          0.25545307993888855,
          0.2550686001777649,
          0.25559014081954956,
          0.2569798231124878,
          0.2564099133014679,
          0.25625988841056824,
          0.2566433250904083,
          0.2565341591835022,
          0.256538063287735,
          0.25666919350624084,
          0.256816029548645,
          0.25722038745880127,
          0.2573208808898926,
          0.25699853897094727,
          0.2576637268066406,
          0.25679731369018555,
          0.25685298442840576,
          0.2568259537220001,
          0.25812745094299316,
          0.2574571371078491,
          0.25820431113243103,
          0.2576677203178406,
          0.25799116492271423,
          0.25835198163986206
         ],
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "bgcolor": "rgb(17,17,17)",
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "rgb(17,17,17)",
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "subunitcolor": "#506784",
           "showland": true,
           "showlakes": true,
           "lakecolor": "rgb(17,17,17)"
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "borderwidth": 1,
           "bordercolor": "rgb(17,17,17)",
           "tickwidth": 0
          },
          "mapbox": {
           "style": "dark"
          }
         }
        },
        "title": {
         "text": "Training Loss"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"ed27e127-5764-42ed-b8ce-316493e50195\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ed27e127-5764-42ed-b8ce-316493e50195\")) {                    Plotly.newPlot(                        \"ed27e127-5764-42ed-b8ce-316493e50195\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"y\":[0.8221728205680847,0.4059525728225708,0.31078198552131653,0.29186147451400757,0.28236639499664307,0.2759021818637848,0.2707113027572632,0.26674535870552063,0.26342669129371643,0.26063841581344604,0.25824013352394104,0.2561529278755188,0.254269003868103,0.25274550914764404,0.2511916756629944,0.2498110681772232,0.24846859276294708,0.24731658399105072,0.24639160931110382,0.2454918771982193,0.24434448778629303,0.2434232085943222,0.2427108883857727,0.24181587994098663,0.24127411842346191,0.24056899547576904,0.23981860280036926,0.2391965687274933,0.23865613341331482,0.23805108666419983,0.237343892455101,0.236976757645607,0.23643182218074799,0.236032173037529,0.23537977039813995,0.23486967384815216,0.2343938946723938,0.23413637280464172,0.23374223709106445,0.23343409597873688,0.23298580944538116,0.23239447176456451,0.23203301429748535,0.2317318320274353,0.2313278168439865,0.23083806037902832,0.230381578207016,0.2299567312002182,0.22986143827438354,0.2291860729455948,0.22892990708351135,0.22849000990390778,0.22834400832653046,0.22786585986614227,0.22734853625297546,0.2270345836877823,0.22693973779678345,0.22661158442497253,0.22615006566047668,0.22569778561592102,0.2253211885690689,0.22505679726600647,0.22453247010707855,0.22456566989421844,0.2240161895751953,0.22362029552459717,0.22340315580368042,0.22332081198692322,0.22278481721878052,0.22241441905498505,0.22195777297019958,0.22162143886089325,0.22121110558509827,0.22082926332950592,0.22072385251522064,0.2205130010843277,0.22003690898418427,0.22012782096862793,0.21970795094966888,0.21915145218372345,0.2193886637687683,0.21870042383670807,0.21863853931427002,0.2180650383234024,0.21770597994327545,0.2173834592103958,0.21744653582572937,0.2170257568359375,0.2169177085161209,0.21659225225448608,0.21655331552028656,0.2160089612007141,0.2163783609867096,0.21575085818767548,0.2156670242547989,0.21502315998077393,0.2150454968214035,0.2147701233625412,0.21444790065288544,0.2143961638212204,0.21431979537010193,0.21402588486671448,0.21376082301139832,0.21335890889167786,0.21325618028640747,0.21300272643566132,0.21285808086395264,0.2127775251865387,0.21265809237957,0.2125522643327713,0.2120668590068817,0.2121511846780777,0.21196569502353668,0.21155212819576263,0.211497500538826,0.21121129393577576,0.21119947731494904,0.2110278159379959,0.21080586314201355,0.21087238192558289,0.21068710088729858,0.21007849276065826,0.21018356084823608,0.2098621129989624,0.20987896621227264,0.20943163335323334,0.20947134494781494,0.20934981107711792,0.20928551256656647,0.2088765650987625,0.20881035923957825,0.20873546600341797,0.20862329006195068,0.2082928866147995,0.2080586850643158,0.20812851190567017,0.20797109603881836,0.20798443257808685,0.20757798850536346,0.2074957937002182,0.20741324126720428,0.20697933435440063,0.20715513825416565,0.20713622868061066,0.206980362534523,0.20673242211341858,0.2065526247024536,0.2067699283361435,0.20649421215057373,0.20622608065605164,0.20614954829216003,0.2059309184551239,0.2058289796113968,0.20581229031085968,0.2058461755514145,0.20551341772079468,0.2053167074918747,0.20517660677433014,0.20484896004199982,0.2049075961112976,0.2050129473209381,0.2048403024673462,0.20465676486492157,0.20440416038036346,0.20421089231967926,0.20442815124988556,0.20379303395748138,0.2044626921415329,0.20386071503162384,0.20391640067100525,0.20358014106750488,0.20353640615940094,0.20345494151115417,0.20334498584270477,0.20348885655403137,0.20303848385810852,0.20304645597934723,0.20277175307273865,0.20264045894145966,0.20282170176506042,0.20277801156044006,0.2026168256998062,0.20250782370567322,0.20211873948574066,0.20236998796463013,0.20196323096752167,0.20185397565364838,0.2019326090812683,0.20163534581661224,0.201531320810318,0.2014874666929245,0.20148572325706482,0.2013835310935974,0.20125772058963776,0.20100848376750946,0.20100805163383484,0.20091544091701508,0.20089936256408691,0.20062975585460663,0.20068712532520294,0.2005259245634079,0.2003924399614334,0.20035569369792938,0.20026451349258423,0.19998575747013092,0.2000523954629898,0.1998215615749359,0.1997440755367279,0.19987133145332336,0.19949087500572205,0.19927164912223816,0.19951428472995758,0.19926513731479645,0.19926360249519348,0.199395090341568,0.1987391859292984,0.19883251190185547,0.1988038718700409,0.19878461956977844,0.19877523183822632,0.19870589673519135,0.19838587939739227,0.19834265112876892,0.19843333959579468,0.198016956448555,0.19811813533306122,0.1979479342699051,0.19819138944149017,0.19765225052833557,0.19787028431892395,0.1976456493139267,0.19763614237308502,0.1973777562379837,0.1973324418067932,0.1972142904996872,0.19708067178726196,0.19721689820289612,0.1970321387052536,0.19712430238723755,0.19677898287773132,0.1967153698205948,0.19675768911838531,0.1966724395751953,0.1966055929660797,0.1964256763458252,0.19617055356502533,0.19610510766506195,0.19619694352149963,0.19618044793605804,0.19603079557418823,0.19573445618152618,0.1959293633699417,0.19633205235004425,0.19600066542625427,0.19551897048950195,0.19550757110118866,0.1956358551979065,0.19517236948013306,0.19516348838806152,0.19533678889274597,0.19496695697307587,0.19544151425361633,0.19495247304439545,0.19477921724319458,0.19468531012535095,0.19460278749465942,0.19488316774368286,0.19454476237297058,0.1944044679403305,0.19447064399719238,0.19434453547000885,0.1942211389541626,0.19415751099586487,0.19415099918842316,0.1940993219614029,0.1939845085144043,0.1936611831188202,0.19386574625968933,0.19349785149097443,0.1935928761959076,0.1932675987482071,0.19344347715377808,0.19331413507461548,0.19324949383735657,0.19315950572490692,0.19300174713134766,0.19303205609321594,0.19337943196296692,0.19289329648017883,0.19288547337055206,0.1927497833967209,0.19266827404499054,0.19261611998081207,0.19246919453144073,0.19250603020191193,0.19245688617229462,0.1922166645526886,0.19258105754852295,0.19209612905979156,0.19198009371757507,0.19205673038959503,0.1920361965894699,0.1918594241142273,0.19161513447761536,0.19168715178966522,0.1916830986738205,0.19148816168308258,0.19143186509609222,0.19144009053707123,0.19145594537258148,0.1914013773202896,0.19108176231384277,0.19106657803058624,0.19108663499355316,0.19095470011234283,0.19083911180496216,0.1908699870109558,0.19068214297294617,0.19077008962631226,0.1907690465450287,0.19046732783317566,0.19052454829216003,0.19035464525222778,0.19017529487609863,0.19033144414424896,0.19018302857875824,0.18995048105716705,0.1901429444551468,0.18973688781261444,0.18990841507911682,0.18985465168952942,0.1897931694984436,0.1894555240869522,0.18934376537799835,0.18945254385471344,0.18961548805236816,0.18925373256206512,0.1894080638885498,0.1893010437488556,0.18961898982524872,0.1891530603170395,0.1888938546180725,0.1890168935060501,0.188785120844841,0.188979372382164,0.18881867825984955,0.18887656927108765,0.18855087459087372,0.18854784965515137,0.18847769498825073,0.188505157828331,0.18831993639469147,0.1880989670753479,0.18817193806171417,0.18812139332294464,0.188202902674675,0.187850683927536,0.18784616887569427,0.1878642737865448,0.18769419193267822,0.18767580389976501,0.18744401633739471,0.1874568611383438,0.18729446828365326,0.18754151463508606,0.1875758171081543,0.18730683624744415,0.18692496418952942,0.18730799853801727,0.1869579702615738,0.18705397844314575,0.18686193227767944,0.18675154447555542,0.1866496503353119,0.1865449845790863,0.18664765357971191,0.18627652525901794,0.1864786297082901,0.18650849163532257,0.18611837923526764,0.1861928105354309,0.18637089431285858,0.18598659336566925,0.18589146435260773,0.18631619215011597,0.1859777569770813,0.18561099469661713,0.18565733730793,0.185951828956604,0.18562549352645874,0.1854996383190155,0.18543867766857147,0.18540287017822266,0.18533003330230713,0.18522456288337708,0.18546706438064575,0.18505345284938812,0.18477050960063934,0.1851121038198471,0.18480221927165985,0.184715136885643,0.18458892405033112,0.18475934863090515,0.18475094437599182,0.18446004390716553,0.18435533344745636,0.18424679338932037,0.18408890068531036,0.18418550491333008,0.1840265542268753,0.18422330915927887,0.18403054773807526,0.18385159969329834,0.18358014523983002,0.18386730551719666,0.18350443243980408,0.18357045948505402,0.18363109230995178,0.18318741023540497,0.18354560434818268,0.1833462417125702,0.18355512619018555,0.18332205712795258,0.18322525918483734,0.18316683173179626,0.183038592338562,0.1831560879945755,0.18313919007778168,0.18301613628864288,0.18263964354991913,0.1827450543642044,0.18244077265262604,0.1824265718460083,0.18242600560188293,0.18226441740989685,0.18220610916614532,0.1819526106119156,0.18210436403751373,0.18192598223686218,0.18196256458759308,0.18195036053657532,0.1818268895149231,0.18193843960762024,0.1816374659538269,0.18170198798179626,0.18157000839710236,0.1814054697751999,0.18168655037879944,0.1813826709985733,0.18136627972126007,0.1810387670993805,0.18098187446594238,0.18105614185333252,0.18082834780216217,0.1811251938343048,0.18084374070167542,0.1807972937822342,0.18053437769412994,0.18049339950084686,0.18061193823814392,0.18030104041099548,0.1803983896970749,0.18017776310443878,0.1801966279745102,0.18041323125362396,0.18016429245471954,0.1801329106092453,0.1798977106809616,0.17995218932628632,0.179923415184021,0.17990167438983917,0.17968407273292542,0.1798962652683258,0.17972812056541443,0.17939677834510803,0.1795443892478943,0.17934221029281616,0.17937947809696198,0.17930862307548523,0.179496631026268,0.17937350273132324,0.17938651144504547,0.179181769490242,0.17898455262184143,0.17879317700862885,0.17870379984378815,0.17885945737361908,0.17871378362178802,0.17857462167739868,0.17827706038951874,0.17859168350696564,0.1784277856349945,0.17870068550109863,0.17812201380729675,0.17795541882514954,0.17813953757286072,0.1782209426164627,0.17802003026008606,0.17802925407886505,0.17778785526752472,0.17772795259952545,0.177851140499115,0.17752374708652496,0.17732536792755127,0.1776110678911209,0.17761681973934174,0.17719754576683044,0.1771892011165619,0.1773756891489029,0.17711776494979858,0.17716072499752045,0.1770150512456894,0.1769539713859558,0.1767461895942688,0.176866814494133,0.17658811807632446,0.17652693390846252,0.17661452293395996,0.1764616221189499,0.17658086121082306,0.1764504462480545,0.176239475607872,0.17636610567569733,0.17613449692726135,0.17618831992149353,0.17614296078681946,0.1760607212781906,0.1760086566209793,0.1757209599018097,0.1759290248155594,0.17614957690238953,0.17563802003860474,0.17566394805908203,0.17552322149276733,0.17544914782047272,0.17540563642978668,0.17539571225643158,0.17523059248924255,0.17499473690986633,0.17524434626102448,0.1757640540599823,0.1753702163696289,0.17520463466644287,0.17480389773845673,0.1748666763305664,0.17470122873783112,0.17446769773960114,0.1745785027742386,0.17441348731517792,0.17436803877353668,0.17441149055957794,0.17449802160263062,0.1743655651807785,0.1745399683713913,0.17418022453784943,0.17442020773887634,0.17409034073352814,0.17395292222499847,0.1742236316204071,0.17379987239837646,0.1739835888147354,0.1737849861383438,0.17362095415592194,0.1735178828239441,0.1734132617712021,0.17356754839420319,0.1737482100725174,0.17342615127563477,0.1729782223701477,0.17358991503715515,0.17323759198188782,0.17307166755199432,0.17331823706626892,0.17288614809513092,0.17294053733348846,0.17304790019989014,0.17283940315246582,0.17275750637054443,0.17283740639686584,0.17275846004486084,0.17247748374938965,0.1723453551530838,0.17228588461875916,0.17261871695518494,0.1721222847700119,0.17214316129684448,0.17199446260929108,0.1720491647720337,0.17224282026290894,0.1718662977218628,0.1723165065050125,0.17186017334461212,0.17185981571674347,0.17153732478618622,0.1716652810573578,0.17157208919525146,0.1716357171535492,0.1713705211877823,0.17159639298915863,0.17137227952480316,0.171129509806633,0.17112493515014648,0.17125585675239563,0.171095609664917,0.1710016131401062,0.17088624835014343,0.17089784145355225,0.17086394131183624,0.17088617384433746,0.17060807347297668,0.1706649512052536,0.17064817249774933,0.17041893303394318,0.17052792012691498,0.17042690515518188,0.17068374156951904,0.17029353976249695,0.16998641192913055,0.17020753026008606,0.16996529698371887,0.1698819249868393,0.16980686783790588,0.16992877423763275,0.169857919216156,0.16984374821186066,0.16976255178451538,0.16946722567081451,0.1696070283651352,0.16949225962162018,0.16935376822948456,0.16981105506420135,0.16932623088359833,0.16907106339931488,0.1689741611480713,0.16891105473041534,0.1689406782388687,0.16900798678398132,0.16901203989982605,0.168913334608078,0.16896510124206543,0.16891339421272278,0.1686224788427353,0.16867870092391968,0.16849714517593384,0.1687769889831543,0.16818644106388092,0.16838820278644562,0.16835272312164307,0.16811947524547577,0.16813446581363678,0.16816070675849915,0.16800183057785034,0.16808383166790009,0.16792498528957367,0.16781528294086456,0.16783419251441956,0.1674661785364151,0.16778546571731567,0.16781963407993317,0.16769178211688995,0.167479008436203,0.16754098236560822,0.16734036803245544,0.1674221158027649,0.16761894524097443,0.16728292405605316,0.16720545291900635,0.1667855679988861,0.1669044941663742,0.16694410145282745,0.1667417585849762,0.16685177385807037,0.16649824380874634,0.16685369610786438,0.16654245555400848,0.16665492951869965,0.16653381288051605,0.166391059756279,0.1663346141576767,0.1662931889295578,0.16618163883686066,0.16624991595745087,0.16619402170181274,0.16618938744068146,0.16594982147216797,0.166133850812912,0.16594743728637695,0.16582027077674866,0.1656438410282135,0.16543704271316528,0.16551344096660614,0.16558541357517242,0.16547800600528717,0.1652754694223404,0.16534923017024994,0.16535329818725586,0.16521289944648743,0.16519634425640106,0.16530193388462067,0.16536936163902283,0.16500604152679443,0.1648981273174286,0.16481854021549225,0.16476672887802124,0.16458339989185333,0.16472987830638885,0.16464580595493317,0.16452105343341827,0.16453230381011963,0.1642839014530182,0.16428923606872559,0.16473719477653503,0.16427624225616455,0.16438350081443787,0.1641593724489212,0.1641572117805481,0.16409815847873688,0.16394779086112976,0.1638767272233963,0.16375377774238586,0.1636190116405487,0.16355954110622406,0.1639067828655243,0.16361860930919647,0.16347958147525787,0.16363027691841125,0.163334921002388,0.16329242289066315,0.16347916424274445,0.16322094202041626,0.16302764415740967,0.1629590094089508,0.16301245987415314,0.16277636587619781,0.1628677099943161,0.16268238425254822,0.16274255514144897,0.16262957453727722,0.1625186651945114,0.1627340316772461,0.16255952417850494,0.16236983239650726,0.162686288356781,0.16200415790081024,0.16252875328063965,0.16234491765499115,0.1621490865945816,0.16213232278823853,0.16203303635120392,0.16201291978359222,0.16183891892433167,0.16201472282409668,0.16182252764701843,0.16175463795661926,0.16153694689273834,0.16154414415359497,0.1613084077835083,0.16167140007019043,0.1612986922264099,0.16148336231708527,0.16124862432479858,0.16120745241641998,0.16102783381938934,0.1610206514596939,0.16096754372119904,0.1610700488090515,0.16116328537464142,0.1611594408750534,0.16080939769744873,0.16065017879009247,0.1609679013490677,0.16058124601840973,0.1603243350982666,0.16039550304412842,0.16013312339782715,0.16039110720157623,0.16008979082107544,0.1600187122821808,0.16037048399448395,0.16015948355197906,0.15999628603458405,0.1601363718509674,0.15996509790420532,0.15993726253509521,0.15991316735744476,0.1603505164384842,0.15957388281822205,0.15944337844848633,0.15966901183128357,0.1594139039516449,0.15968401730060577,0.15944130718708038,0.15922006964683533,0.15946190059185028,0.15921799838542938,0.15913830697536469,0.158959299325943,0.15905773639678955,0.15900114178657532,0.15860971808433533,0.15915678441524506,0.1588667929172516,0.15855634212493896,0.15879088640213013,0.15917129814624786,0.15844516456127167,0.158264622092247,0.15828979015350342,0.1581616997718811,0.15852303802967072,0.15829749405384064,0.15794260799884796,0.15821091830730438,0.1578335165977478,0.15792492032051086,0.1580127626657486,0.1579100787639618,0.15739089250564575,0.15765894949436188,0.157626211643219,0.1574912667274475,0.15746378898620605,0.1573757380247116,0.15752415359020233,0.15723221004009247,0.1573297679424286,0.15704941749572754,0.15717533230781555,0.1571519821882248,0.15691740810871124,0.15712600946426392,0.15682870149612427,0.15702198445796967,0.1569102257490158,0.1567975878715515,0.15652529895305634,0.1567307412624359,0.15653066337108612,0.1567862182855606,0.1567029058933258,0.1562308520078659,0.15615706145763397,0.15615151822566986,0.15612676739692688,0.15633811056613922,0.15600787103176117,0.15593820810317993,0.1559845209121704,0.15595164895057678,0.15585747361183167,0.15588517487049103,0.15578971803188324,0.1558264046907425,0.15547935664653778,0.1556883305311203,0.15557123720645905,0.15532135963439941,0.15558841824531555,0.15551875531673431,0.15498118102550507,0.15504716336727142,0.15519990026950836,0.1549094319343567,0.15505696833133698,0.15516971051692963,0.15509502589702606,0.1547745168209076,0.15473036468029022,0.15478163957595825,0.15446069836616516,0.1543402522802353,0.15459825098514557,0.1543092280626297,0.15421031415462494,0.15461936593055725,0.15421442687511444,0.15382902324199677,0.15420690178871155,0.15402768552303314,0.15384595096111298,0.15383155643939972,0.15383386611938477,0.15379633009433746,0.1535693109035492,0.15385696291923523,0.15338799357414246,0.15359167754650116,0.15327778458595276,0.1536683887243271,0.15326125919818878,0.15341641008853912,0.15327177941799164,0.15326808393001556,0.15289144217967987,0.15312086045742035,0.1531042456626892,0.15285399556159973,0.15300431847572327,0.15273334085941315,0.15270355343818665,0.15279358625411987,0.15256616473197937,0.15274159610271454,0.1524246782064438,0.15252244472503662,0.15258276462554932,0.15235592424869537,0.15230073034763336,0.1522679328918457,0.15210209786891937,0.15224537253379822,0.151901975274086,0.15195852518081665,0.15172676742076874,0.15218093991279602,0.15180718898773193,0.1522233784198761,0.15187184512615204,0.15154294669628143,0.15150876343250275,0.15148861706256866,0.15151919424533844,0.15135075151920319,0.15125074982643127,0.15106995403766632,0.15114443004131317,0.1509869247674942,0.15105602145195007,0.1512599140405655,0.15081341564655304,0.15075275301933289,0.1507379710674286,0.15062730014324188,0.15087902545928955,0.15057942271232605,0.1503882110118866,0.15068548917770386,0.15026356279850006,0.15029814839363098,0.1501450538635254,0.1500505656003952,0.15013419091701508,0.1503695845603943,0.15021494030952454,0.14998121559619904,0.15007150173187256,0.1497681736946106,0.1495905965566635,0.14954350888729095,0.14959165453910828,0.14932884275913239,0.1496625691652298,0.14956645667552948,0.14971579611301422,0.14930926263332367,0.1492491513490677,0.14928464591503143,0.14947690069675446,0.14950443804264069,0.14922308921813965,0.1490982472896576,0.14922937750816345,0.14893418550491333,0.14888811111450195,0.14881983399391174,0.14898259937763214,0.14864638447761536,0.14839857816696167,0.1485881805419922,0.1485302597284317,0.14824064075946808,0.14854855835437775,0.14849728345870972,0.14796428382396698,0.14835897088050842,0.14776702225208282,0.14829084277153015,0.14798825979232788,0.1479315608739853,0.14754439890384674,0.1475783884525299,0.14792658388614655,0.14758169651031494,0.14744889736175537,0.14748859405517578,0.14743132889270782,0.147638738155365,0.14735053479671478,0.1472131758928299,0.1472020298242569,0.14727292954921722,0.14750900864601135,0.1469714641571045,0.1468198299407959,0.14695045351982117,0.14694470167160034,0.1466933786869049,0.1469741016626358,0.1466091126203537,0.1467895656824112],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Loss\",\"y\":[0.5577536225318909,0.339071124792099,0.31009599566459656,0.2999083399772644,0.29371178150177,0.2896368205547333,0.28482308983802795,0.28102415800094604,0.2784905433654785,0.27612775564193726,0.275163859128952,0.2744704484939575,0.27167797088623047,0.27264052629470825,0.26943784952163696,0.26973479986190796,0.2677547335624695,0.26812833547592163,0.26692086458206177,0.2670416533946991,0.2653546631336212,0.2646435797214508,0.2645893096923828,0.26397332549095154,0.2652488052845001,0.26499509811401367,0.26308777928352356,0.26314204931259155,0.2629534900188446,0.2615281641483307,0.2610929310321808,0.2612029016017914,0.2606884241104126,0.26010429859161377,0.2602769732475281,0.2599680423736572,0.25950679183006287,0.2611534297466278,0.26054784655570984,0.2592347264289856,0.2588599622249603,0.2591306269168854,0.2586840093135834,0.25847774744033813,0.2580113708972931,0.25807586312294006,0.25852957367897034,0.2582063674926758,0.25683894753456116,0.25752732157707214,0.25750306248664856,0.2570020258426666,0.25698140263557434,0.255844384431839,0.25575685501098633,0.25563111901283264,0.2557520568370819,0.2574913501739502,0.25633305311203003,0.25421613454818726,0.2545816898345947,0.2541423738002777,0.2538098096847534,0.25504791736602783,0.25371432304382324,0.25309938192367554,0.2534714937210083,0.25527599453926086,0.2526927888393402,0.25235673785209656,0.25197044014930725,0.25240272283554077,0.25192075967788696,0.25291284918785095,0.2517518401145935,0.2520715296268463,0.2506338357925415,0.25069886445999146,0.2510135769844055,0.25010624527931213,0.2505759000778198,0.25047799944877625,0.24998745322227478,0.250413179397583,0.2499881386756897,0.2514103651046753,0.2500973641872406,0.24966177344322205,0.25152525305747986,0.250232458114624,0.2487337291240692,0.2487909495830536,0.2499522566795349,0.24888025224208832,0.2483479380607605,0.24907124042510986,0.25026068091392517,0.24829572439193726,0.2477368265390396,0.24761323630809784,0.24743668735027313,0.24752260744571686,0.248231440782547,0.24701088666915894,0.24776911735534668,0.24831274151802063,0.24769584834575653,0.2466607391834259,0.24683232605457306,0.24698711931705475,0.24762828648090363,0.24755145609378815,0.24712687730789185,0.2470189481973648,0.2465014010667801,0.24655219912528992,0.24767786264419556,0.245757058262825,0.24741235375404358,0.24599404633045197,0.24654099345207214,0.24644581973552704,0.24670842289924622,0.24632132053375244,0.24672117829322815,0.24587473273277283,0.2457422912120819,0.245361790060997,0.24574777483940125,0.2454267293214798,0.24620582163333893,0.24579790234565735,0.24566388130187988,0.24565698206424713,0.246151864528656,0.2457714080810547,0.24702996015548706,0.24523764848709106,0.2458050698041916,0.2469864785671234,0.2457498013973236,0.24463191628456116,0.24633200466632843,0.24479511380195618,0.2449924200773239,0.24483300745487213,0.24523940682411194,0.24623502790927887,0.24528995156288147,0.24436385929584503,0.24489673972129822,0.2453327476978302,0.24487905204296112,0.24482373893260956,0.24471880495548248,0.24495624005794525,0.244581937789917,0.24531202018260956,0.2446054071187973,0.24576760828495026,0.24459008872509003,0.24582354724407196,0.2443810999393463,0.2441498190164566,0.24371567368507385,0.24422387778759003,0.24643610417842865,0.24401411414146423,0.24600127339363098,0.24465720355510712,0.24627891182899475,0.24422036111354828,0.24400566518306732,0.24413082003593445,0.2435741126537323,0.24340158700942993,0.2437686175107956,0.2445620745420456,0.2435784488916397,0.24391727149486542,0.2433479130268097,0.24358561635017395,0.24330423772335052,0.24358497560024261,0.24408414959907532,0.24307961761951447,0.24306568503379822,0.24419209361076355,0.245030015707016,0.24365182220935822,0.24452219903469086,0.24307896196842194,0.2435062825679779,0.24312105774879456,0.24321489036083221,0.24377281963825226,0.24339699745178223,0.2437281310558319,0.2448597252368927,0.2432638257741928,0.24248145520687103,0.24308450520038605,0.2428227663040161,0.24385347962379456,0.24473580718040466,0.24293500185012817,0.24265719950199127,0.24266746640205383,0.24365904927253723,0.24216699600219727,0.24206523597240448,0.24279473721981049,0.24221231043338776,0.24733348190784454,0.24247358739376068,0.24278593063354492,0.2434200644493103,0.24467821419239044,0.24291422963142395,0.24285855889320374,0.2430879920721054,0.2428017109632492,0.24383795261383057,0.2430219203233719,0.24242065846920013,0.24299225211143494,0.24347741901874542,0.24204513430595398,0.2419922798871994,0.24334979057312012,0.2438368797302246,0.24249157309532166,0.24296218156814575,0.24269166588783264,0.24203450977802277,0.24192261695861816,0.24267858266830444,0.24205684661865234,0.24116648733615875,0.2421131283044815,0.24165184795856476,0.24194905161857605,0.24145552515983582,0.2434113621711731,0.24210363626480103,0.2428509145975113,0.24127385020256042,0.2430378496646881,0.24195829033851624,0.24209651350975037,0.24222539365291595,0.24328117072582245,0.24239514768123627,0.24143870174884796,0.24151454865932465,0.24137639999389648,0.2426658272743225,0.2412714809179306,0.24149179458618164,0.241859570145607,0.24546599388122559,0.2421339601278305,0.24240241944789886,0.24186739325523376,0.2419823855161667,0.24313701689243317,0.2420908659696579,0.2421714961528778,0.24124696850776672,0.24173149466514587,0.2414630800485611,0.24292060732841492,0.24246858060359955,0.2410895973443985,0.24130530655384064,0.24356724321842194,0.24180415272712708,0.24130591750144958,0.24111513793468475,0.24101583659648895,0.24053561687469482,0.24113401770591736,0.241253063082695,0.24167346954345703,0.24209509789943695,0.24144814908504486,0.24070365726947784,0.2409166842699051,0.24190174043178558,0.24195098876953125,0.24044981598854065,0.2439405471086502,0.24235132336616516,0.2420675903558731,0.24195130169391632,0.2418700009584427,0.24217082560062408,0.24151037633419037,0.24052585661411285,0.2404003143310547,0.24093180894851685,0.24059027433395386,0.2415626049041748,0.2405877709388733,0.24071115255355835,0.24128976464271545,0.24137833714485168,0.24173414707183838,0.24012257158756256,0.24215765297412872,0.24051354825496674,0.2407982051372528,0.24065405130386353,0.24067969620227814,0.2404695451259613,0.24065642058849335,0.24284297227859497,0.24077163636684418,0.24060234427452087,0.24146927893161774,0.24285198748111725,0.2416309267282486,0.24086102843284607,0.24142451584339142,0.2404392808675766,0.24011468887329102,0.24132190644741058,0.24045632779598236,0.2409052848815918,0.2410467565059662,0.2405320256948471,0.2403862327337265,0.24004323780536652,0.24132037162780762,0.2406395524740219,0.24039143323898315,0.24062024056911469,0.24129687249660492,0.241151824593544,0.2405523806810379,0.24150019884109497,0.24024589359760284,0.24009642004966736,0.23992809653282166,0.24086907505989075,0.24052874743938446,0.24233099818229675,0.2408612072467804,0.24055072665214539,0.23954711854457855,0.24019891023635864,0.24040888249874115,0.24089692533016205,0.23938457667827606,0.23958103358745575,0.23949922621250153,0.24104513227939606,0.24120770394802094,0.24098895490169525,0.2394506335258484,0.2400081902742386,0.23961184918880463,0.23944731056690216,0.24026460945606232,0.24012236297130585,0.23963041603565216,0.2399541735649109,0.23998992145061493,0.2397262156009674,0.23901888728141785,0.23938828706741333,0.24118976294994354,0.2399146556854248,0.24014189839363098,0.23989540338516235,0.24033121764659882,0.23981864750385284,0.24029874801635742,0.23989535868167877,0.2407832145690918,0.23930144309997559,0.239353209733963,0.2391655296087265,0.23888681828975677,0.23914740979671478,0.24016651511192322,0.23938411474227905,0.23932552337646484,0.24006414413452148,0.23926380276679993,0.2396022081375122,0.24102260172367096,0.23958639800548553,0.23954463005065918,0.23894482851028442,0.23953381180763245,0.2387588620185852,0.24042776226997375,0.24154958128929138,0.23999282717704773,0.2394588440656662,0.23891581594944,0.2387709766626358,0.2395986169576645,0.2410506308078766,0.238785982131958,0.239436075091362,0.23907113075256348,0.23939429223537445,0.23850958049297333,0.24197149276733398,0.23904640972614288,0.23938482999801636,0.2382088154554367,0.2398093342781067,0.23828299343585968,0.23899920284748077,0.2400258630514145,0.24063321948051453,0.2385208010673523,0.24079415202140808,0.23962825536727905,0.23895494639873505,0.23880384862422943,0.23905117809772491,0.23916175961494446,0.2407066524028778,0.2391543686389923,0.23919783532619476,0.23877589404582977,0.23839814960956573,0.23840957880020142,0.23905713856220245,0.239014133810997,0.2406785786151886,0.2379690259695053,0.23861761391162872,0.23898302018642426,0.24004653096199036,0.23897841572761536,0.23889388144016266,0.23876185715198517,0.2394254207611084,0.2385905385017395,0.23887354135513306,0.23860791325569153,0.23833677172660828,0.2378377765417099,0.23962615430355072,0.23842287063598633,0.2390638291835785,0.23859573900699615,0.23766668140888214,0.23853112757205963,0.23825062811374664,0.239517942070961,0.2384650856256485,0.23890230059623718,0.23872146010398865,0.237869992852211,0.23810112476348877,0.23799309134483337,0.23960751295089722,0.2390974909067154,0.23836538195610046,0.23938818275928497,0.23898382484912872,0.23788663744926453,0.23982936143875122,0.23866625130176544,0.2380237877368927,0.23846128582954407,0.2387348860502243,0.23802006244659424,0.23828889429569244,0.23868519067764282,0.23964187502861023,0.23849721252918243,0.23938466608524323,0.23953314125537872,0.2384748011827469,0.2409011870622635,0.2381715029478073,0.23832572996616364,0.23988394439220428,0.23912273347377777,0.23856429755687714,0.24025991559028625,0.23813846707344055,0.23769359290599823,0.2393598109483719,0.23837028443813324,0.2386389970779419,0.23757433891296387,0.23813998699188232,0.2404414564371109,0.2386179268360138,0.23867666721343994,0.23810389637947083,0.23906287550926208,0.23788082599639893,0.2378867119550705,0.23856203258037567,0.23992425203323364,0.239182248711586,0.2378186732530594,0.23856204748153687,0.23850083351135254,0.2382819801568985,0.23803380131721497,0.23819397389888763,0.2391485869884491,0.23810306191444397,0.23811644315719604,0.24078811705112457,0.23967048525810242,0.23866736888885498,0.23806244134902954,0.2395155280828476,0.23838865756988525,0.23855605721473694,0.23873844742774963,0.23915258049964905,0.23866897821426392,0.24025490880012512,0.23872019350528717,0.23818844556808472,0.2391604483127594,0.237920880317688,0.23976567387580872,0.23859727382659912,0.24132120609283447,0.238157719373703,0.2391894906759262,0.2392142117023468,0.2391425520181656,0.23812490701675415,0.23844611644744873,0.23942060768604279,0.23810744285583496,0.2383359968662262,0.2401413470506668,0.23850573599338531,0.23836758732795715,0.23824982345104218,0.2385208010673523,0.23992235958576202,0.24007533490657806,0.23900754749774933,0.23975679278373718,0.2390984743833542,0.2384175807237625,0.23964282870292664,0.238986074924469,0.23921936750411987,0.23887750506401062,0.23986931145191193,0.23865164816379547,0.23827716708183289,0.23899033665657043,0.23892009258270264,0.23956161737442017,0.23839512467384338,0.2394135743379593,0.23954303562641144,0.23921088874340057,0.23901456594467163,0.23918978869915009,0.23865583539009094,0.2389678806066513,0.24020755290985107,0.238961860537529,0.23827305436134338,0.2401258945465088,0.24033674597740173,0.23937760293483734,0.240304633975029,0.24037519097328186,0.23912861943244934,0.23860688507556915,0.23901459574699402,0.24083511531352997,0.23893332481384277,0.23886443674564362,0.23966552317142487,0.2393822968006134,0.23897433280944824,0.23872247338294983,0.23865917325019836,0.24124397337436676,0.24316607415676117,0.2395317256450653,0.2397853285074234,0.2404181808233261,0.23884262144565582,0.23919637501239777,0.23940862715244293,0.2398606240749359,0.23910431563854218,0.23949739336967468,0.23976901173591614,0.24042947590351105,0.23943594098091125,0.24092529714107513,0.23956513404846191,0.23938366770744324,0.23912325501441956,0.23962515592575073,0.23967665433883667,0.23943768441677094,0.23971857130527496,0.24105817079544067,0.24194633960723877,0.24063752591609955,0.2396215796470642,0.2404865026473999,0.23918092250823975,0.23984505236148834,0.2411993145942688,0.2402898520231247,0.23925621807575226,0.24331405758857727,0.24113960564136505,0.2426200807094574,0.24049517512321472,0.23988595604896545,0.2401292324066162,0.23961426317691803,0.2397828996181488,0.241205632686615,0.24080123007297516,0.23934856057167053,0.23918721079826355,0.23973830044269562,0.2410411387681961,0.239839106798172,0.23994088172912598,0.24518871307373047,0.24113717675209045,0.24078680574893951,0.23963458836078644,0.24093227088451385,0.24129270017147064,0.2399071753025055,0.24059203267097473,0.2416418194770813,0.2442413866519928,0.24050268530845642,0.23994430899620056,0.2410353273153305,0.24096794426441193,0.24123039841651917,0.24092699587345123,0.23970915377140045,0.24052435159683228,0.2407061755657196,0.2407325804233551,0.24065889418125153,0.24117465317249298,0.23988142609596252,0.24136118590831757,0.24216516315937042,0.24448058009147644,0.24255460500717163,0.2411852478981018,0.24043625593185425,0.24080432951450348,0.24045813083648682,0.240189790725708,0.24033507704734802,0.24096807837486267,0.24123576283454895,0.24276119470596313,0.2409759759902954,0.24071262776851654,0.24100913107395172,0.24103768169879913,0.24073193967342377,0.24045054614543915,0.24405911564826965,0.2412816882133484,0.2409832775592804,0.24264931678771973,0.2410782277584076,0.24190832674503326,0.2409372180700302,0.24153394997119904,0.2423323392868042,0.24168610572814941,0.2414589524269104,0.24211932718753815,0.2411230355501175,0.24168281257152557,0.24210649728775024,0.24149492383003235,0.24157509207725525,0.2424977719783783,0.24243824183940887,0.24359837174415588,0.24168317019939423,0.24218201637268066,0.24301107227802277,0.24496741592884064,0.2414759248495102,0.24135154485702515,0.24410435557365417,0.2417585849761963,0.24339252710342407,0.24271062016487122,0.2456691563129425,0.24331411719322205,0.24212947487831116,0.24390916526317596,0.24252495169639587,0.2416965812444687,0.2446591556072235,0.2418009489774704,0.2423953115940094,0.2427394688129425,0.2436048686504364,0.24370023608207703,0.24203167855739594,0.24252130091190338,0.24226441979408264,0.2431265413761139,0.2441667914390564,0.24230295419692993,0.24338935315608978,0.242658331990242,0.24341198801994324,0.2427951693534851,0.24295054376125336,0.24297566711902618,0.24325428903102875,0.2432418018579483,0.2440008968114853,0.2433483898639679,0.2440231442451477,0.243186816573143,0.24209263920783997,0.2434150129556656,0.24347500503063202,0.2444855123758316,0.24432659149169922,0.24308931827545166,0.24302205443382263,0.24389688670635223,0.24337556958198547,0.2437436431646347,0.24409177899360657,0.24434712529182434,0.24298924207687378,0.24398857355117798,0.2436162680387497,0.24392259120941162,0.24612002074718475,0.24442429840564728,0.24402691423892975,0.2437027245759964,0.24309474229812622,0.24348324537277222,0.24509648978710175,0.24332958459854126,0.24419769644737244,0.24446512758731842,0.24541975557804108,0.2447851598262787,0.24333758652210236,0.2440384328365326,0.24349650740623474,0.24455943703651428,0.2445807009935379,0.2441442757844925,0.24503254890441895,0.24430084228515625,0.24475274980068207,0.24573543667793274,0.24411100149154663,0.24487726390361786,0.24475575983524323,0.2442418336868286,0.2456880658864975,0.2443339228630066,0.24571789801120758,0.24575483798980713,0.2466883659362793,0.24443431198596954,0.24531759321689606,0.24561211466789246,0.24678590893745422,0.2445676326751709,0.2446695864200592,0.24523146450519562,0.24493880569934845,0.24957256019115448,0.24556829035282135,0.2460683435201645,0.24600084125995636,0.24591541290283203,0.24490581452846527,0.24510449171066284,0.24732455611228943,0.24634134769439697,0.24513225257396698,0.24711422622203827,0.24609333276748657,0.24913455545902252,0.24585865437984467,0.24608929455280304,0.24696789681911469,0.2455751597881317,0.24616405367851257,0.24716824293136597,0.24600021541118622,0.24606391787528992,0.24791952967643738,0.24617215991020203,0.24665719270706177,0.24799078702926636,0.24614176154136658,0.24613237380981445,0.2459438592195511,0.2470976710319519,0.24738094210624695,0.24888643622398376,0.2463371306657791,0.2464291900396347,0.2475089579820633,0.249895840883255,0.24662868678569794,0.24726566672325134,0.2485475093126297,0.24680717289447784,0.24706214666366577,0.24875323474407196,0.2507478594779968,0.2472953498363495,0.24794675409793854,0.24709446728229523,0.24795907735824585,0.24831785261631012,0.24842283129692078,0.2470754086971283,0.24900507926940918,0.24752987921237946,0.24740903079509735,0.24758373200893402,0.2488732784986496,0.24733561277389526,0.24737384915351868,0.24778713285923004,0.24784809350967407,0.24777598679065704,0.2475471794605255,0.24806128442287445,0.2494315654039383,0.24930979311466217,0.24839729070663452,0.2484506219625473,0.24789918959140778,0.2502194046974182,0.24926480650901794,0.24838602542877197,0.2535361647605896,0.2507619261741638,0.25008925795555115,0.2513226270675659,0.248714879155159,0.24859163165092468,0.24888883531093597,0.24835015833377838,0.2497567981481552,0.2490910440683365,0.24885547161102295,0.24915388226509094,0.24963271617889404,0.2495395541191101,0.2500424087047577,0.2489042580127716,0.25070154666900635,0.24909931421279907,0.2501700520515442,0.25012680888175964,0.2506726086139679,0.2505781352519989,0.25038769841194153,0.2498146891593933,0.2494087517261505,0.24984465539455414,0.24985718727111816,0.2499251663684845,0.2507200539112091,0.24967019259929657,0.2503044307231903,0.2502292990684509,0.25009381771087646,0.2511865794658661,0.25141623616218567,0.25107109546661377,0.2512296140193939,0.2524254322052002,0.2531741261482239,0.250820517539978,0.2522872984409332,0.2522749900817871,0.2521803677082062,0.2510448098182678,0.2510363459587097,0.25127270817756653,0.25297221541404724,0.2516314685344696,0.25155210494995117,0.2517569065093994,0.25187304615974426,0.25534364581108093,0.2550453841686249,0.25131863355636597,0.2516668736934662,0.25228992104530334,0.25295281410217285,0.25166720151901245,0.25201642513275146,0.25450772047042847,0.2524239122867584,0.2530165910720825,0.2524925172328949,0.253106951713562,0.2532344460487366,0.25429272651672363,0.25261420011520386,0.2533627152442932,0.25373077392578125,0.2531139850616455,0.25319570302963257,0.25310811400413513,0.25422149896621704,0.25437745451927185,0.25346022844314575,0.2539040148258209,0.2527652084827423,0.25678667426109314,0.25332731008529663,0.25358009338378906,0.253669410943985,0.2530786395072937,0.2537527084350586,0.25494855642318726,0.2538590431213379,0.25341662764549255,0.25364577770233154,0.25515055656433105,0.2554667294025421,0.2542749047279358,0.2535427510738373,0.2544473707675934,0.2542342245578766,0.2536994218826294,0.25400957465171814,0.255510538816452,0.25884658098220825,0.25409454107284546,0.2547062039375305,0.25512415170669556,0.25396186113357544,0.25646457076072693,0.2563977837562561,0.2552054226398468,0.25611937046051025,0.2550778388977051,0.2556599974632263,0.2550783157348633,0.25601592659950256,0.2562010884284973,0.25724390149116516,0.2557392716407776,0.2562582790851593,0.25545307993888855,0.2550686001777649,0.25559014081954956,0.2569798231124878,0.2564099133014679,0.25625988841056824,0.2566433250904083,0.2565341591835022,0.256538063287735,0.25666919350624084,0.256816029548645,0.25722038745880127,0.2573208808898926,0.25699853897094727,0.2576637268066406,0.25679731369018555,0.25685298442840576,0.2568259537220001,0.25812745094299316,0.2574571371078491,0.25820431113243103,0.2576677203178406,0.25799116492271423,0.25835198163986206],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"bgcolor\":\"rgb(17,17,17)\",\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"rgb(17,17,17)\",\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"subunitcolor\":\"#506784\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"rgb(17,17,17)\"},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"borderwidth\":1,\"bordercolor\":\"rgb(17,17,17)\",\"tickwidth\":0},\"mapbox\":{\"style\":\"dark\"}}},\"title\":{\"text\":\"Training Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('ed27e127-5764-42ed-b8ce-316493e50195');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Train Accuracy",
         "y": [
          0.6261234879493713,
          0.8528341054916382,
          0.8771961331367493,
          0.8825708031654358,
          0.8861297965049744,
          0.888306736946106,
          0.8900952935218811,
          0.8916128277778625,
          0.893085241317749,
          0.8936362266540527,
          0.894467294216156,
          0.8956054449081421,
          0.8961926102638245,
          0.8968248963356018,
          0.8971139788627625,
          0.8982791900634766,
          0.8984056711196899,
          0.8988754153251648,
          0.8991915583610535,
          0.8995077013969421,
          0.8998780250549316,
          0.9003387093544006,
          0.9011065363883972,
          0.9012420177459717,
          0.9011426568031311,
          0.9016937017440796,
          0.9015672206878662,
          0.9021814465522766,
          0.9025518298149109,
          0.9025879502296448,
          0.9030938148498535,
          0.9029853940010071,
          0.9033015966415405,
          0.9033828377723694,
          0.9036177396774292,
          0.9037261009216309,
          0.9043042063713074,
          0.9040874242782593,
          0.9040332436561584,
          0.9041867852210999,
          0.9045752286911011,
          0.9051894545555115,
          0.9050178527832031,
          0.9050991535186768,
          0.9053159356117249,
          0.9057585597038269,
          0.905722439289093,
          0.9058127403259277,
          0.9056049585342407,
          0.9061018228530884,
          0.9063547253608704,
          0.9065715074539185,
          0.906670868396759,
          0.9067341089248657,
          0.9071134924888611,
          0.9069960713386536,
          0.9073212742805481,
          0.9070502519607544,
          0.9082335829734802,
          0.907818078994751,
          0.9078632593154907,
          0.907818078994751,
          0.9084594249725342,
          0.9082245826721191,
          0.9087665677070618,
          0.9086400866508484,
          0.9090465903282166,
          0.9089381694793701,
          0.908739447593689,
          0.909182071685791,
          0.9097511172294617,
          0.9097872972488403,
          0.9099227786064148,
          0.9096156358718872,
          0.9101576209068298,
          0.9100040793418884,
          0.9105280041694641,
          0.9102028012275696,
          0.9102118015289307,
          0.9107628464698792,
          0.9104376435279846,
          0.9112687110900879,
          0.910356342792511,
          0.9111874103546143,
          0.9112957715988159,
          0.9116751551628113,
          0.9113318920135498,
          0.9113951325416565,
          0.9122171401977539,
          0.911738395690918,
          0.9121358394622803,
          0.9120455384254456,
          0.9119913578033447,
          0.9121177792549133,
          0.9122262001037598,
          0.9124429821968079,
          0.9124791026115417,
          0.9125242829322815,
          0.9127410650253296,
          0.9129307866096497,
          0.9130843281745911,
          0.9125514030456543,
          0.9130843281745911,
          0.9134456515312195,
          0.9129217267036438,
          0.9132830500602722,
          0.9135088920593262,
          0.9134275913238525,
          0.9134817719459534,
          0.9137256741523743,
          0.9137076139450073,
          0.913581132888794,
          0.9139515161514282,
          0.9139785766601562,
          0.9144122004508972,
          0.9147283434867859,
          0.914258599281311,
          0.9141231179237366,
          0.9145205616950989,
          0.9145386219024658,
          0.9141050577163696,
          0.9146379828453064,
          0.9142676591873169,
          0.9145747423171997,
          0.9144302606582642,
          0.9151167273521423,
          0.9147554039955139,
          0.91471928358078,
          0.9149541854858398,
          0.9148457646369934,
          0.9150716066360474,
          0.9153064489364624,
          0.9151709675788879,
          0.9152160882949829,
          0.9155051708221436,
          0.9150716066360474,
          0.9149541854858398,
          0.9152251482009888,
          0.9152973890304565,
          0.9156225919723511,
          0.9157580733299255,
          0.9159026145935059,
          0.9156948924064636,
          0.9155774116516113,
          0.9158393740653992,
          0.9159206748008728,
          0.9159929752349854,
          0.9159567952156067,
          0.9157129526138306,
          0.9157038927078247,
          0.916327178478241,
          0.9160652160644531,
          0.9162097573280334,
          0.9163181185722351,
          0.9160471558570862,
          0.9160110354423523,
          0.9161465167999268,
          0.9163000583648682,
          0.9163813591003418,
          0.9163452386856079,
          0.9163000583648682,
          0.9165439605712891,
          0.9164717197418213,
          0.9166704416275024,
          0.9167336821556091,
          0.9164987802505493,
          0.9171401262283325,
          0.9165891408920288,
          0.9169143438339233,
          0.9170950055122375,
          0.9171311259269714,
          0.917248547077179,
          0.9170769453048706,
          0.9169775247573853,
          0.9173027276992798,
          0.9171853065490723,
          0.9170588254928589,
          0.9173117876052856,
          0.9175646901130676,
          0.9173659682273865,
          0.9173027276992798,
          0.9172846674919128,
          0.9178537726402283,
          0.9179350733757019,
          0.9180705547332764,
          0.9179711937904358,
          0.9181337952613831,
          0.9179531335830688,
          0.9181247353553772,
          0.9180614948272705,
          0.9180976748466492,
          0.9184318780899048,
          0.9180253744125366,
          0.9184951186180115,
          0.9184679985046387,
          0.918377697467804,
          0.9185041189193726,
          0.9180705547332764,
          0.9185402393341064,
          0.9185583591461182,
          0.9185492992401123,
          0.9186486601829529,
          0.9189106225967407,
          0.9193441867828369,
          0.9183505773544312,
          0.9185854196548462,
          0.9191274046897888,
          0.9194164872169495,
          0.9186577200889587,
          0.9190822243690491,
          0.9191093444824219,
          0.9187932014465332,
          0.9190371036529541,
          0.9190009236335754,
          0.9190009236335754,
          0.9192267656326294,
          0.9192628860473633,
          0.9192809462547302,
          0.9192267656326294,
          0.9189919233322144,
          0.9192177653312683,
          0.9195700287818909,
          0.9193171262741089,
          0.919786810874939,
          0.9196965098381042,
          0.919533908367157,
          0.9198591113090515,
          0.9193351864814758,
          0.9200307130813599,
          0.9196332693099976,
          0.9198861718177795,
          0.919994592666626,
          0.9198861718177795,
          0.9198952317237854,
          0.9201210141181946,
          0.9202204346656799,
          0.9201752543449402,
          0.9200578331947327,
          0.9202384948730469,
          0.920482337474823,
          0.9201661944389343,
          0.9205636382102966,
          0.9203016757965088,
          0.9201120138168335,
          0.9201481342315674,
          0.9203920364379883,
          0.9203739762306213,
          0.920897901058197,
          0.920437216758728,
          0.9206359386444092,
          0.9207804799079895,
          0.9205816984176636,
          0.9203829765319824,
          0.9206088185310364,
          0.9211327433586121,
          0.9210966229438782,
          0.9204462170600891,
          0.9213947057723999,
          0.9208256006240845,
          0.9205726981163025,
          0.9210605025291443,
          0.9208436608314514,
          0.9210514426231384,
          0.9211417436599731,
          0.9210514426231384,
          0.9215030670166016,
          0.9208075404167175,
          0.9216927886009216,
          0.9210243225097656,
          0.9213405251502991,
          0.921358585357666,
          0.9211146831512451,
          0.9216024279594421,
          0.9214488863945007,
          0.921385645866394,
          0.921376645565033,
          0.9215843677520752,
          0.9214940667152405,
          0.92186439037323,
          0.9217650294303894,
          0.9221805930137634,
          0.9216566681861877,
          0.9217379689216614,
          0.9220992922782898,
          0.9221263527870178,
          0.9218734502792358,
          0.9220450520515442,
          0.9217289090156555,
          0.9220360517501831,
          0.921846330165863,
          0.9219547510147095,
          0.9220179915428162,
          0.9217921495437622,
          0.922298014163971,
          0.9221354126930237,
          0.9219908714294434,
          0.9224244356155396,
          0.9222708940505981,
          0.9225509166717529,
          0.9222076535224915,
          0.9224154353141785,
          0.9225689768791199,
          0.9228128790855408,
          0.9226502776145935,
          0.9226322174072266,
          0.9226412773132324,
          0.9228309392929077,
          0.9228128790855408,
          0.9227135181427002,
          0.922767698764801,
          0.9228670597076416,
          0.9230387210845947,
          0.9231290221214294,
          0.9230929017066956,
          0.9232103228569031,
          0.9231019616127014,
          0.9231561422348022,
          0.9231019616127014,
          0.9229303002357483,
          0.9228851199150085,
          0.9234451651573181,
          0.9228038191795349,
          0.9235987663269043,
          0.9232645034790039,
          0.9227586984634399,
          0.9235807061195374,
          0.9234090447425842,
          0.9232103228569031,
          0.9231561422348022,
          0.9231742024421692,
          0.9234451651573181,
          0.9233548641204834,
          0.923454225063324,
          0.9236710071563721,
          0.923436164855957,
          0.9234813451766968,
          0.9235716462135315,
          0.9236529469490051,
          0.9238877892494202,
          0.9234632849693298,
          0.9237433075904846,
          0.9236800670623779,
          0.9240503907203674,
          0.9237523078918457,
          0.9238246083259583,
          0.9237613677978516,
          0.9235445261001587,
          0.9243394732475281,
          0.9239690899848938,
          0.9241588115692139,
          0.9238246083259583,
          0.9239781498908997,
          0.9242581725120544,
          0.923896849155426,
          0.924149751663208,
          0.9241768717765808,
          0.9240323305130005,
          0.9244207739830017,
          0.9242039918899536,
          0.9243304133415222,
          0.9241858720779419,
          0.9244478344917297,
          0.9243033528327942,
          0.924149751663208,
          0.924375593662262,
          0.9237251877784729,
          0.9247910976409912,
          0.9248091578483582,
          0.9245291352272034,
          0.9242852330207825,
          0.9244117140769958,
          0.9249175786972046,
          0.9247459173202515,
          0.9250801801681519,
          0.9241949319839478,
          0.9245110750198364,
          0.9250621199607849,
          0.9248995184898376,
          0.9247820973396301,
          0.9250891804695129,
          0.9248633980751038,
          0.9248272180557251,
          0.925053060054779,
          0.9252969622612,
          0.9244478344917297,
          0.925071120262146,
          0.9252698421478271,
          0.9253602027893066,
          0.9249898195266724,
          0.9250891804695129,
          0.9251524209976196,
          0.9253330826759338,
          0.9254866242408752,
          0.9253782629966736,
          0.9248995184898376,
          0.9252156615257263,
          0.925278902053833,
          0.9255860447883606,
          0.9251434206962585,
          0.9256311655044556,
          0.9252969622612,
          0.9259202480316162,
          0.9256944060325623,
          0.9254956841468811,
          0.925278902053833,
          0.9255498647689819,
          0.9254415035247803,
          0.9257847666740417,
          0.9258028268814087,
          0.9261731505393982,
          0.9255679249763489,
          0.9258660674095154,
          0.9254324436187744,
          0.9258028268814087,
          0.9260105490684509,
          0.9259202480316162,
          0.925757646560669,
          0.9256402254104614,
          0.9260918498039246,
          0.9257485866546631,
          0.9261099100112915,
          0.9261279702186584,
          0.9258660674095154,
          0.9257305264472961,
          0.926182210445404,
          0.9259563684463501,
          0.9256492257118225,
          0.925974428653717,
          0.9257034659385681,
          0.9263357520103455,
          0.9270312786102295,
          0.9266338348388672,
          0.9265435338020325,
          0.9263086318969727,
          0.9262544512748718,
          0.926453173160553,
          0.9266157746315002,
          0.9266699552536011,
          0.9264622330665588,
          0.9268506169319153,
          0.9264983534812927,
          0.9268686771392822,
          0.9262183308601379,
          0.926435112953186,
          0.9266157746315002,
          0.9268416166305542,
          0.9266338348388672,
          0.9268416166305542,
          0.9269229173660278,
          0.926913857460022,
          0.9268416166305542,
          0.9269590377807617,
          0.9272210001945496,
          0.9271125793457031,
          0.9268596768379211,
          0.9272210001945496,
          0.9270674586296082,
          0.9274829626083374,
          0.927356481552124,
          0.9271125793457031,
          0.927103579044342,
          0.9275190830230713,
          0.9275913238525391,
          0.9274377822875977,
          0.9272932410240173,
          0.9271577596664429,
          0.9272119402885437,
          0.927582323551178,
          0.9273655414581299,
          0.9271577596664429,
          0.9276275038719177,
          0.9271848797798157,
          0.9274287223815918,
          0.9277539253234863,
          0.927564263343811,
          0.9277629852294922,
          0.927564263343811,
          0.9276184439659119,
          0.9272119402885437,
          0.9275462031364441,
          0.9276275038719177,
          0.927374541759491,
          0.9279255867004395,
          0.9274829626083374,
          0.92783522605896,
          0.9279255867004395,
          0.9280520081520081,
          0.9277629852294922,
          0.9282146096229553,
          0.9279255867004395,
          0.9277268648147583,
          0.9277719855308533,
          0.9277539253234863,
          0.9281513690948486,
          0.9284404516220093,
          0.9281784892082214,
          0.9278894066810608,
          0.9283952713012695,
          0.9279165267944336,
          0.9285759329795837,
          0.9280971884727478,
          0.928295910358429,
          0.9286933541297913,
          0.9286211133003235,
          0.9287475943565369,
          0.9283139705657959,
          0.9285669326782227,
          0.928512692451477,
          0.9284133315086365,
          0.9287114143371582,
          0.9286482334136963,
          0.9287927150726318,
          0.928738534450531,
          0.9284585118293762,
          0.9287927150726318,
          0.9289372563362122,
          0.9291630983352661,
          0.9287837147712708,
          0.9290546774864197,
          0.928991436958313,
          0.9286030530929565,
          0.9289191961288452,
          0.9287837147712708,
          0.9293798804283142,
          0.9288107752799988,
          0.9288559556007385,
          0.928964376449585,
          0.92919921875,
          0.9293527603149414,
          0.9293166399002075,
          0.9288198351860046,
          0.9291450381278992,
          0.9295424818992615,
          0.9293979406356812,
          0.9291811585426331,
          0.9289553165435791,
          0.9293979406356812,
          0.9294160008430481,
          0.9294431209564209,
          0.9292172789573669,
          0.9292353391647339,
          0.9292714595794678,
          0.9295424818992615,
          0.9296237826347351,
          0.9297321438789368,
          0.929659903049469,
          0.9298225045204163,
          0.9296147227287292,
          0.9298496246337891,
          0.929885745048523,
          0.9299038052558899,
          0.9296960234642029,
          0.9300031661987305,
          0.9301296472549438,
          0.9300844669342041,
          0.9297502636909485,
          0.9299579858779907,
          0.9301296472549438,
          0.9297050833702087,
          0.9299851059913635,
          0.9298225045204163,
          0.9300754070281982,
          0.9302831888198853,
          0.9302560687065125,
          0.9303193092346191,
          0.930355429649353,
          0.9301838278770447,
          0.9301477074623108,
          0.9301928281784058,
          0.9305090308189392,
          0.9304006099700928,
          0.9306806325912476,
          0.9302651286125183,
          0.9307619333267212,
          0.9308974146842957,
          0.9309606552124023,
          0.9304728507995605,
          0.930834174156189,
          0.9303825497627258,
          0.9305903315544128,
          0.9307799935340881,
          0.9310510158538818,
          0.9305090308189392,
          0.9304999709129333,
          0.9309697151184082,
          0.9305903315544128,
          0.9309606552124023,
          0.9306354522705078,
          0.9309245347976685,
          0.9307528734207153,
          0.9306716322898865,
          0.9312406778335571,
          0.9309967756271362,
          0.9306535124778748,
          0.9309877753257751,
          0.9311051964759827,
          0.9309787154197693,
          0.9314484596252441,
          0.9311684370040894,
          0.9315478205680847,
          0.9314665198326111,
          0.9315117001533508,
          0.9315297603607178,
          0.9313310384750366,
          0.9314574599266052,
          0.9317826628684998,
          0.9318459033966064,
          0.9314665198326111,
          0.9317194223403931,
          0.9315748810768127,
          0.9316200613975525,
          0.9310871362686157,
          0.9315478205680847,
          0.9316020011901855,
          0.9316110610961914,
          0.9315207004547119,
          0.9318097829818726,
          0.9316652417182922,
          0.9318459033966064,
          0.9319000840187073,
          0.9319813847541809,
          0.9319723844528198,
          0.9319272041320801,
          0.9315478205680847,
          0.9317646026611328,
          0.9322704672813416,
          0.9318910837173462,
          0.9320988059043884,
          0.931945264339447,
          0.9324691891670227,
          0.9317465424537659,
          0.932198166847229,
          0.9323065876960754,
          0.9321439862251282,
          0.93263179063797,
          0.9321620464324951,
          0.9323698282241821,
          0.9321349263191223,
          0.9325143694877625,
          0.9321439862251282,
          0.9323155879974365,
          0.9323155879974365,
          0.9325414299964905,
          0.9326498508453369,
          0.932216227054596,
          0.9325233697891235,
          0.932405948638916,
          0.9327492117881775,
          0.9325143694877625,
          0.932884693145752,
          0.9328033924102783,
          0.933074414730072,
          0.9328395128250122,
          0.932424008846283,
          0.9331375956535339,
          0.9328033924102783,
          0.9326227307319641,
          0.9329840540885925,
          0.9327220916748047,
          0.9330562949180603,
          0.9329298734664917,
          0.9326949715614319,
          0.932866632938385,
          0.9329840540885925,
          0.9330472946166992,
          0.933562159538269,
          0.9332369565963745,
          0.9332640767097473,
          0.9334537982940674,
          0.9332008361816406,
          0.933092474937439,
          0.932866632938385,
          0.933535099029541,
          0.9334899187088013,
          0.9335260391235352,
          0.9331466555595398,
          0.9336163401603699,
          0.9333544373512268,
          0.9334808588027954,
          0.933562159538269,
          0.9333092570304871,
          0.9338060617446899,
          0.9338060617446899,
          0.9335079789161682,
          0.9331466555595398,
          0.9334899187088013,
          0.9336525201797485,
          0.9334537982940674,
          0.9339234828948975,
          0.9339234828948975,
          0.9339054226875305,
          0.933788001537323,
          0.9341673851013184,
          0.9340860843658447,
          0.934022843837738,
          0.9338963627815247,
          0.9344474077224731,
          0.9340680241584778,
          0.9338512420654297,
          0.9336344599723816,
          0.9339325428009033,
          0.934013843536377,
          0.934483528137207,
          0.9340951442718506,
          0.9342125654220581,
          0.9343209266662598,
          0.9344202876091003,
          0.9343209266662598,
          0.9340860843658447,
          0.9344022274017334,
          0.9345106482505798,
          0.934013843536377,
          0.9343661069869995,
          0.9345828890800476,
          0.9343932271003723,
          0.9344202876091003,
          0.9345648288726807,
          0.9346822500228882,
          0.9346822500228882,
          0.9349170923233032,
          0.9350706934928894,
          0.9346190094947815,
          0.9346551895141602,
          0.9347454905509949,
          0.9347545504570007,
          0.9348900318145752,
          0.9347454905509949,
          0.9349983930587769,
          0.9346732497215271,
          0.9349352121353149,
          0.9349983930587769,
          0.9352332949638367,
          0.9348267912864685,
          0.9349893927574158,
          0.9355133175849915,
          0.9351881146430969,
          0.9354500770568848,
          0.935404896736145,
          0.9350074529647827,
          0.9347454905509949,
          0.9350526332855225,
          0.9352874755859375,
          0.9352152347564697,
          0.9353235960006714,
          0.9351971745491028,
          0.9351429343223572,
          0.9356759190559387,
          0.9353777766227722,
          0.9356216788291931,
          0.9354952573776245,
          0.9355133175849915,
          0.9352061748504639,
          0.9352965354919434,
          0.9356939792633057,
          0.9353958964347839,
          0.935612678527832,
          0.9359559416770935,
          0.9357390999794006,
          0.9360553026199341,
          0.9355403780937195,
          0.9358294606208801,
          0.9361456036567688,
          0.9356668591499329,
          0.9359197616577148,
          0.9360553026199341,
          0.9359559416770935,
          0.9357481598854065,
          0.9358023405075073,
          0.9358746409416199,
          0.9360553026199341,
          0.9360823631286621,
          0.9363172650337219,
          0.9361275434494019,
          0.9360823631286621,
          0.936118483543396,
          0.9358927011489868,
          0.9366153478622437,
          0.936579167842865,
          0.9361366033554077,
          0.9358023405075073,
          0.936100423336029,
          0.9358565807342529,
          0.9359197616577148,
          0.9364979267120361,
          0.9366153478622437,
          0.9362810850143433,
          0.936073362827301,
          0.9366514682769775,
          0.9362179040908813,
          0.9367327690124512,
          0.9371392726898193,
          0.936786949634552,
          0.9363353252410889,
          0.9363353252410889,
          0.9362810850143433,
          0.9367598295211792,
          0.9368321299552917,
          0.936579167842865,
          0.9362720847129822,
          0.9371934533119202,
          0.9363985657691956,
          0.9368230700492859,
          0.9373469948768616,
          0.9368773102760315,
          0.9362991452217102,
          0.9368682503700256,
          0.9371573328971863,
          0.9371302127838135,
          0.9369856715202332,
          0.9365701675415039,
          0.9368321299552917,
          0.9371030926704407,
          0.9370669722557068,
          0.937247633934021,
          0.9374373555183411,
          0.9370850324630737,
          0.9371753931045532,
          0.9375908970832825,
          0.9373469948768616,
          0.9373650550842285,
          0.9374915361404419,
          0.9373650550842285,
          0.9375637769699097,
          0.9375547766685486,
          0.937482476234436,
          0.9375818371772766,
          0.9378618597984314,
          0.9378076791763306,
          0.937961220741272,
          0.9378437995910645,
          0.9375367164611816,
          0.9374192953109741,
          0.9377805590629578,
          0.9376270174980164,
          0.9375276565551758,
          0.9381780624389648,
          0.9378167390823364,
          0.9375728368759155,
          0.9374102354049683,
          0.938150942325592,
          0.9379974007606506,
          0.9381780624389648,
          0.9381870627403259,
          0.9377444386482239,
          0.9376811981201172,
          0.9376180171966553,
          0.9383135437965393,
          0.937916100025177,
          0.9382774233818054,
          0.938169002532959,
          0.9381057620048523,
          0.9380064010620117,
          0.9382412433624268,
          0.9384400248527527,
          0.9384580850601196,
          0.9380606412887573,
          0.9378889799118042,
          0.9383316040039062,
          0.9383857846260071,
          0.9384942054748535,
          0.9381870627403259,
          0.9382412433624268,
          0.9388013482093811,
          0.9383316040039062,
          0.9385303258895874,
          0.9382593631744385,
          0.9380064010620117,
          0.9388284087181091,
          0.9382954835891724,
          0.9392349123954773,
          0.93888258934021,
          0.938611626625061,
          0.9385122656822205,
          0.9387109875679016,
          0.9382593631744385,
          0.9389910101890564,
          0.9389007091522217,
          0.9386929273605347,
          0.9389819502830505,
          0.9390632510185242,
          0.9391897320747375,
          0.9389187693595886,
          0.9389819502830505,
          0.9393613934516907,
          0.9387651681900024,
          0.9391626119613647,
          0.9396684765815735,
          0.9397045969963074,
          0.9387742280960083,
          0.9394155740737915,
          0.9392619729042053,
          0.9392529726028442,
          0.9389548897743225,
          0.9399304389953613,
          0.9389458298683167,
          0.9391174912452698,
          0.9397136569023132,
          0.9390181303024292,
          0.9393703937530518,
          0.9391355514526367,
          0.9395058751106262,
          0.9396414160728455,
          0.9393884539604187,
          0.939758837223053,
          0.9396504163742065,
          0.9393794536590576,
          0.9392619729042053,
          0.9396955966949463,
          0.9395420551300049,
          0.9396955966949463,
          0.9396775364875793,
          0.9400388598442078,
          0.9400297999382019,
          0.9401924014091492,
          0.9397949576377869,
          0.9397497773170471,
          0.939325213432312,
          0.9401382207870483,
          0.9396504163742065,
          0.939758837223053,
          0.9397678375244141,
          0.9400930404663086,
          0.9398401379585266,
          0.9402646422386169,
          0.9401562809944153,
          0.9401021003723145,
          0.9403369426727295,
          0.9398310780525208,
          0.9401021003723145,
          0.9405356645584106,
          0.9401021003723145,
          0.940219521522522,
          0.9407795667648315,
          0.9400388598442078,
          0.9405266046524048,
          0.9402556419372559,
          0.9404633641242981,
          0.9402917623519897,
          0.9408698678016663,
          0.9403730630874634,
          0.9403730630874634,
          0.9405176043510437,
          0.9404995441436768,
          0.9410866498947144,
          0.9405898451805115,
          0.9405085444450378,
          0.9411770105361938,
          0.9408608675003052,
          0.9408879280090332,
          0.9407343864440918,
          0.9408428072929382,
          0.9408608675003052,
          0.9405627846717834,
          0.9410324692726135,
          0.9408337473869324,
          0.9410957098007202,
          0.9409782886505127,
          0.9406350255012512,
          0.9407253265380859,
          0.9406530857086182,
          0.9411589503288269,
          0.9411589503288269,
          0.9410866498947144,
          0.9412583112716675,
          0.9411318302154541,
          0.9410505294799805,
          0.9412492513656616,
          0.9413396120071411,
          0.9415112137794495,
          0.9409963488578796,
          0.9417551159858704,
          0.9413847327232361,
          0.9415925145149231,
          0.9411137700080872,
          0.9412944316864014,
          0.9415112137794495,
          0.9412673115730286,
          0.9413396120071411,
          0.9415112137794495,
          0.941628634929657,
          0.9416738152503967,
          0.9415653944015503,
          0.9417189955711365,
          0.9415563941001892,
          0.9415292739868164,
          0.9417731761932373,
          0.941836416721344,
          0.9418815970420837,
          0.9416557550430298,
          0.941818356513977,
          0.942062258720398,
          0.9414209127426147,
          0.9417822360992432,
          0.9421254396438599,
          0.9420170783996582,
          0.9416738152503967,
          0.9417641758918762,
          0.9418002963066101,
          0.9421616196632385,
          0.9419177174568176
         ],
         "type": "scatter"
        },
        {
         "mode": "lines",
         "name": "Validation Accuracy",
         "y": [
          0.8108803629875183,
          0.8647674322128296,
          0.8735198378562927,
          0.8772953748703003,
          0.8808992505073547,
          0.8803844451904297,
          0.884503185749054,
          0.8855328559875488,
          0.887420654296875,
          0.8887935280799866,
          0.8881070613861084,
          0.8882787227630615,
          0.8894799947738647,
          0.8896515965461731,
          0.8903380632400513,
          0.8901664614677429,
          0.8906813263893127,
          0.8906813263893127,
          0.8901664614677429,
          0.8903380632400513,
          0.8905097246170044,
          0.8905097246170044,
          0.8913677930831909,
          0.8901664614677429,
          0.8910245299339294,
          0.8920542597770691,
          0.8915393948554993,
          0.8911961317062378,
          0.8915393948554993,
          0.8927406668663025,
          0.8923974633216858,
          0.8920542597770691,
          0.8925690650939941,
          0.8939419984817505,
          0.8937703967094421,
          0.8927406668663025,
          0.8923974633216858,
          0.8925690650939941,
          0.8922258615493774,
          0.8949716687202454,
          0.8951432704925537,
          0.893598735332489,
          0.8944568634033203,
          0.8944568634033203,
          0.8939419984817505,
          0.893598735332489,
          0.8922258615493774,
          0.8925690650939941,
          0.8956581354141235,
          0.8941136002540588,
          0.8951432704925537,
          0.8939419984817505,
          0.8927406668663025,
          0.8941136002540588,
          0.8951432704925537,
          0.8961730003356934,
          0.896001398563385,
          0.8951432704925537,
          0.8937703967094421,
          0.8956581354141235,
          0.894800066947937,
          0.894800066947937,
          0.8963446021080017,
          0.8939419984817505,
          0.8941136002540588,
          0.8963446021080017,
          0.8944568634033203,
          0.8954865336418152,
          0.8966878056526184,
          0.8963446021080017,
          0.8965162038803101,
          0.8958297371864319,
          0.8972026705741882,
          0.8925690650939941,
          0.8956581354141235,
          0.8939419984817505,
          0.8972026705741882,
          0.8970310688018799,
          0.8972026705741882,
          0.896001398563385,
          0.8961730003356934,
          0.8961730003356934,
          0.8970310688018799,
          0.8972026705741882,
          0.8968594670295715,
          0.896001398563385,
          0.8975459337234497,
          0.8980607390403748,
          0.8968594670295715,
          0.8951432704925537,
          0.8977175354957581,
          0.8980607390403748,
          0.8970310688018799,
          0.8977175354957581,
          0.8982323408126831,
          0.8966878056526184,
          0.8963446021080017,
          0.8978891372680664,
          0.8980607390403748,
          0.8982323408126831,
          0.8985756039619446,
          0.8982323408126831,
          0.8989188075065613,
          0.8989188075065613,
          0.8985756039619446,
          0.8973742723464966,
          0.8996052742004395,
          0.8990904688835144,
          0.8990904688835144,
          0.8985756039619446,
          0.8978891372680664,
          0.8978891372680664,
          0.8980607390403748,
          0.8982323408126831,
          0.8999485373497009,
          0.8989188075065613,
          0.8982323408126831,
          0.8994336724281311,
          0.8985756039619446,
          0.8987472057342529,
          0.8992620706558228,
          0.8994336724281311,
          0.8989188075065613,
          0.8990904688835144,
          0.9001201391220093,
          0.8985756039619446,
          0.8994336724281311,
          0.900463342666626,
          0.9006350040435791,
          0.8985756039619446,
          0.8985756039619446,
          0.8985756039619446,
          0.8992620706558228,
          0.9001201391220093,
          0.8994336724281311,
          0.8994336724281311,
          0.8985756039619446,
          0.8994336724281311,
          0.8997768759727478,
          0.8996052742004395,
          0.8994336724281311,
          0.9006350040435791,
          0.8989188075065613,
          0.8996052742004395,
          0.8992620706558228,
          0.9006350040435791,
          0.9006350040435791,
          0.8987472057342529,
          0.9002917408943176,
          0.900463342666626,
          0.9009782075881958,
          0.9014930725097656,
          0.8994336724281311,
          0.9006350040435791,
          0.8984040021896362,
          0.8999485373497009,
          0.9013214111328125,
          0.8994336724281311,
          0.9006350040435791,
          0.9006350040435791,
          0.9013214111328125,
          0.9001201391220093,
          0.9013214111328125,
          0.8997768759727478,
          0.9009782075881958,
          0.9006350040435791,
          0.8997768759727478,
          0.8996052742004395,
          0.8996052742004395,
          0.8987472057342529,
          0.8999485373497009,
          0.8997768759727478,
          0.8996052742004395,
          0.8999485373497009,
          0.9011498093605042,
          0.9013214111328125,
          0.8987472057342529,
          0.8996052742004395,
          0.9011498093605042,
          0.9009782075881958,
          0.900463342666626,
          0.900463342666626,
          0.901664674282074,
          0.9001201391220093,
          0.8990904688835144,
          0.900463342666626,
          0.9001201391220093,
          0.9006350040435791,
          0.9001201391220093,
          0.900463342666626,
          0.9014930725097656,
          0.9009782075881958,
          0.9001201391220093,
          0.900463342666626,
          0.9001201391220093,
          0.9002917408943176,
          0.900463342666626,
          0.9002917408943176,
          0.9006350040435791,
          0.900463342666626,
          0.9006350040435791,
          0.9014930725097656,
          0.9011498093605042,
          0.9014930725097656,
          0.9006350040435791,
          0.9002917408943176,
          0.9014930725097656,
          0.9011498093605042,
          0.9008066058158875,
          0.9011498093605042,
          0.9020078778266907,
          0.9014930725097656,
          0.9014930725097656,
          0.8982323408126831,
          0.9006350040435791,
          0.9001201391220093,
          0.9006350040435791,
          0.8997768759727478,
          0.9018362760543823,
          0.9011498093605042,
          0.9011498093605042,
          0.901664674282074,
          0.901664674282074,
          0.9013214111328125,
          0.9009782075881958,
          0.9025227427482605,
          0.902179479598999,
          0.9035524129867554,
          0.9026943445205688,
          0.902179479598999,
          0.9011498093605042,
          0.9018362760543823,
          0.9025227427482605,
          0.9026943445205688,
          0.9011498093605042,
          0.9026943445205688,
          0.9001201391220093,
          0.9025227427482605,
          0.9028659462928772,
          0.9026943445205688,
          0.9023511409759521,
          0.9030376076698303,
          0.9018362760543823,
          0.9001201391220093,
          0.9020078778266907,
          0.902179479598999,
          0.9020078778266907,
          0.8999485373497009,
          0.9020078778266907,
          0.902179479598999,
          0.9028659462928772,
          0.9014930725097656,
          0.9023511409759521,
          0.9026943445205688,
          0.9013214111328125,
          0.902179479598999,
          0.9009782075881958,
          0.903380811214447,
          0.9023511409759521,
          0.9013214111328125,
          0.8996052742004395,
          0.9008066058158875,
          0.9014930725097656,
          0.9006350040435791,
          0.902179479598999,
          0.9025227427482605,
          0.9023511409759521,
          0.900463342666626,
          0.9023511409759521,
          0.9025227427482605,
          0.9023511409759521,
          0.9009782075881958,
          0.9011498093605042,
          0.9030376076698303,
          0.9020078778266907,
          0.9030376076698303,
          0.9030376076698303,
          0.902179479598999,
          0.9035524129867554,
          0.9025227427482605,
          0.9026943445205688,
          0.901664674282074,
          0.901664674282074,
          0.9020078778266907,
          0.901664674282074,
          0.901664674282074,
          0.9030376076698303,
          0.9026943445205688,
          0.9018362760543823,
          0.9020078778266907,
          0.9018362760543823,
          0.9008066058158875,
          0.9023511409759521,
          0.9014930725097656,
          0.9013214111328125,
          0.9020078778266907,
          0.901664674282074,
          0.9018362760543823,
          0.902179479598999,
          0.9028659462928772,
          0.902179479598999,
          0.9025227427482605,
          0.900463342666626,
          0.9025227427482605,
          0.902179479598999,
          0.9026943445205688,
          0.9032092094421387,
          0.902179479598999,
          0.902179479598999,
          0.9023511409759521,
          0.9037240147590637,
          0.9030376076698303,
          0.9020078778266907,
          0.902179479598999,
          0.9030376076698303,
          0.9028659462928772,
          0.8987472057342529,
          0.9044104814529419,
          0.9023511409759521,
          0.9018362760543823,
          0.9018362760543823,
          0.9006350040435791,
          0.9025227427482605,
          0.9018362760543823,
          0.9035524129867554,
          0.9030376076698303,
          0.9032092094421387,
          0.9028659462928772,
          0.9032092094421387,
          0.9025227427482605,
          0.9028659462928772,
          0.9040672779083252,
          0.9026943445205688,
          0.9023511409759521,
          0.9042388796806335,
          0.9035524129867554,
          0.9037240147590637,
          0.9023511409759521,
          0.9026943445205688,
          0.9037240147590637,
          0.9028659462928772,
          0.904582142829895,
          0.9030376076698303,
          0.9032092094421387,
          0.9013214111328125,
          0.9023511409759521,
          0.9009782075881958,
          0.9028659462928772,
          0.9037240147590637,
          0.904582142829895,
          0.903380811214447,
          0.903380811214447,
          0.9023511409759521,
          0.9030376076698303,
          0.9032092094421387,
          0.9040672779083252,
          0.9025227427482605,
          0.9018362760543823,
          0.9026943445205688,
          0.9032092094421387,
          0.903380811214447,
          0.904582142829895,
          0.9040672779083252,
          0.9028659462928772,
          0.9042388796806335,
          0.9047537446022034,
          0.9038956761360168,
          0.9030376076698303,
          0.9040672779083252,
          0.9044104814529419,
          0.9038956761360168,
          0.9028659462928772,
          0.9047537446022034,
          0.9038956761360168,
          0.904582142829895,
          0.9020078778266907,
          0.9049253463745117,
          0.9032092094421387,
          0.9040672779083252,
          0.9038956761360168,
          0.9035524129867554,
          0.9040672779083252,
          0.904582142829895,
          0.9042388796806335,
          0.904582142829895,
          0.9032092094421387,
          0.9030376076698303,
          0.9047537446022034,
          0.9038956761360168,
          0.904582142829895,
          0.904582142829895,
          0.9026943445205688,
          0.9038956761360168,
          0.9037240147590637,
          0.9047537446022034,
          0.9035524129867554,
          0.9040672779083252,
          0.9032092094421387,
          0.9042388796806335,
          0.9047537446022034,
          0.9044104814529419,
          0.9040672779083252,
          0.904582142829895,
          0.9049253463745117,
          0.9030376076698303,
          0.9047537446022034,
          0.9042388796806335,
          0.903380811214447,
          0.9035524129867554,
          0.9056118130683899,
          0.9044104814529419,
          0.9038956761360168,
          0.9040672779083252,
          0.9049253463745117,
          0.9042388796806335,
          0.9050969481468201,
          0.9054402112960815,
          0.9042388796806335,
          0.9020078778266907,
          0.9056118130683899,
          0.9014930725097656,
          0.9049253463745117,
          0.9057834148406982,
          0.9052685499191284,
          0.9056118130683899,
          0.9062982797622681,
          0.903380811214447,
          0.9049253463745117,
          0.9037240147590637,
          0.9050969481468201,
          0.904582142829895,
          0.9061266779899597,
          0.9044104814529419,
          0.9047537446022034,
          0.9025227427482605,
          0.9052685499191284,
          0.9050969481468201,
          0.9052685499191284,
          0.9040672779083252,
          0.9044104814529419,
          0.9044104814529419,
          0.9050969481468201,
          0.9050969481468201,
          0.9064698815345764,
          0.9052685499191284,
          0.9050969481468201,
          0.9057834148406982,
          0.9056118130683899,
          0.904582142829895,
          0.904582142829895,
          0.9047537446022034,
          0.9050969481468201,
          0.9062982797622681,
          0.9044104814529419,
          0.9056118130683899,
          0.9032092094421387,
          0.9052685499191284,
          0.9047537446022034,
          0.9062982797622681,
          0.904582142829895,
          0.9052685499191284,
          0.9059550166130066,
          0.9057834148406982,
          0.9049253463745117,
          0.9044104814529419,
          0.904582142829895,
          0.9042388796806335,
          0.9050969481468201,
          0.9044104814529419,
          0.9061266779899597,
          0.9047537446022034,
          0.9047537446022034,
          0.9030376076698303,
          0.9059550166130066,
          0.9064698815345764,
          0.9047537446022034,
          0.9050969481468201,
          0.9057834148406982,
          0.9044104814529419,
          0.9059550166130066,
          0.9057834148406982,
          0.9059550166130066,
          0.9047537446022034,
          0.9064698815345764,
          0.9049253463745117,
          0.9052685499191284,
          0.9049253463745117,
          0.9026943445205688,
          0.9054402112960815,
          0.9056118130683899,
          0.9044104814529419,
          0.9069847464561462,
          0.9050969481468201,
          0.904582142829895,
          0.9068130850791931,
          0.9035524129867554,
          0.9062982797622681,
          0.9059550166130066,
          0.9042388796806335,
          0.9052685499191284,
          0.9050969481468201,
          0.9059550166130066,
          0.9040672779083252,
          0.9014930725097656,
          0.9052685499191284,
          0.9042388796806335,
          0.9068130850791931,
          0.9069847464561462,
          0.9061266779899597,
          0.9064698815345764,
          0.9071563482284546,
          0.9049253463745117,
          0.9049253463745117,
          0.9057834148406982,
          0.9014930725097656,
          0.904582142829895,
          0.9042388796806335,
          0.9044104814529419,
          0.9038956761360168,
          0.9049253463745117,
          0.9062982797622681,
          0.9059550166130066,
          0.9050969481468201,
          0.9059550166130066,
          0.9037240147590637,
          0.904582142829895,
          0.9052685499191284,
          0.904582142829895,
          0.9050969481468201,
          0.9047537446022034,
          0.904582142829895,
          0.9047537446022034,
          0.9062982797622681,
          0.9044104814529419,
          0.9061266779899597,
          0.9062982797622681,
          0.9056118130683899,
          0.9061266779899597,
          0.9054402112960815,
          0.9062982797622681,
          0.9056118130683899,
          0.9059550166130066,
          0.9061266779899597,
          0.9061266779899597,
          0.9052685499191284,
          0.9057834148406982,
          0.9056118130683899,
          0.9052685499191284,
          0.904582142829895,
          0.9061266779899597,
          0.9059550166130066,
          0.9052685499191284,
          0.9047537446022034,
          0.9054402112960815,
          0.9049253463745117,
          0.9064698815345764,
          0.904582142829895,
          0.904582142829895,
          0.9059550166130066,
          0.9069847464561462,
          0.9059550166130066,
          0.9052685499191284,
          0.9069847464561462,
          0.9032092094421387,
          0.904582142829895,
          0.9038956761360168,
          0.9049253463745117,
          0.9057834148406982,
          0.9049253463745117,
          0.9047537446022034,
          0.9042388796806335,
          0.9054402112960815,
          0.9071563482284546,
          0.9040672779083252,
          0.9059550166130066,
          0.904582142829895,
          0.9071563482284546,
          0.9074995517730713,
          0.9050969481468201,
          0.9059550166130066,
          0.9038956761360168,
          0.9056118130683899,
          0.9059550166130066,
          0.9056118130683899,
          0.9042388796806335,
          0.9071563482284546,
          0.9057834148406982,
          0.9059550166130066,
          0.9056118130683899,
          0.903380811214447,
          0.9025227427482605,
          0.9064698815345764,
          0.9040672779083252,
          0.9068130850791931,
          0.9064698815345764,
          0.9047537446022034,
          0.9061266779899597,
          0.9061266779899597,
          0.9050969481468201,
          0.9061266779899597,
          0.9050969481468201,
          0.9074995517730713,
          0.9059550166130066,
          0.9078428149223328,
          0.9052685499191284,
          0.9047537446022034,
          0.904582142829895,
          0.9062982797622681,
          0.9056118130683899,
          0.9064698815345764,
          0.9050969481468201,
          0.9026943445205688,
          0.9047537446022034,
          0.9061266779899597,
          0.9074995517730713,
          0.903380811214447,
          0.9057834148406982,
          0.9076712131500244,
          0.9073279500007629,
          0.9052685499191284,
          0.9056118130683899,
          0.9056118130683899,
          0.9054402112960815,
          0.9037240147590637,
          0.904582142829895,
          0.9049253463745117,
          0.9047537446022034,
          0.9064698815345764,
          0.9061266779899597,
          0.904582142829895,
          0.9074995517730713,
          0.9078428149223328,
          0.9062982797622681,
          0.9047537446022034,
          0.9049253463745117,
          0.9057834148406982,
          0.9050969481468201,
          0.9042388796806335,
          0.9049253463745117,
          0.9054402112960815,
          0.9068130850791931,
          0.9059550166130066,
          0.9047537446022034,
          0.9073279500007629,
          0.9049253463745117,
          0.903380811214447,
          0.9044104814529419,
          0.9062982797622681,
          0.9071563482284546,
          0.9052685499191284,
          0.9068130850791931,
          0.904582142829895,
          0.9057834148406982,
          0.9052685499191284,
          0.9061266779899597,
          0.9047537446022034,
          0.9054402112960815,
          0.9057834148406982,
          0.9049253463745117,
          0.9044104814529419,
          0.9050969481468201,
          0.9064698815345764,
          0.9040672779083252,
          0.9056118130683899,
          0.9037240147590637,
          0.9066414833068848,
          0.9059550166130066,
          0.9049253463745117,
          0.9071563482284546,
          0.9074995517730713,
          0.9068130850791931,
          0.9042388796806335,
          0.9059550166130066,
          0.9057834148406982,
          0.9054402112960815,
          0.9056118130683899,
          0.9068130850791931,
          0.904582142829895,
          0.9059550166130066,
          0.9061266779899597,
          0.9056118130683899,
          0.9050969481468201,
          0.9061266779899597,
          0.9038956761360168,
          0.9068130850791931,
          0.9062982797622681,
          0.9073279500007629,
          0.9047537446022034,
          0.904582142829895,
          0.9057834148406982,
          0.9044104814529419,
          0.9056118130683899,
          0.9049253463745117,
          0.9044104814529419,
          0.9068130850791931,
          0.9059550166130066,
          0.9069847464561462,
          0.9050969481468201,
          0.9059550166130066,
          0.9047537446022034,
          0.9066414833068848,
          0.9037240147590637,
          0.9071563482284546,
          0.9052685499191284,
          0.9044104814529419,
          0.9054402112960815,
          0.9054402112960815,
          0.9049253463745117,
          0.9068130850791931,
          0.9049253463745117,
          0.9050969481468201,
          0.9064698815345764,
          0.9049253463745117,
          0.9068130850791931,
          0.9074995517730713,
          0.9052685499191284,
          0.9062982797622681,
          0.9054402112960815,
          0.9062982797622681,
          0.9056118130683899,
          0.9069847464561462,
          0.9061266779899597,
          0.9062982797622681,
          0.9042388796806335,
          0.9052685499191284,
          0.9059550166130066,
          0.904582142829895,
          0.9050969481468201,
          0.9047537446022034,
          0.9050969481468201,
          0.9059550166130066,
          0.9054402112960815,
          0.9059550166130066,
          0.904582142829895,
          0.9059550166130066,
          0.9044104814529419,
          0.9061266779899597,
          0.9059550166130066,
          0.9052685499191284,
          0.9054402112960815,
          0.9050969481468201,
          0.9056118130683899,
          0.9049253463745117,
          0.9049253463745117,
          0.9056118130683899,
          0.9061266779899597,
          0.9049253463745117,
          0.9047537446022034,
          0.9059550166130066,
          0.9069847464561462,
          0.9040672779083252,
          0.9054402112960815,
          0.9062982797622681,
          0.9076712131500244,
          0.9047537446022034,
          0.903380811214447,
          0.9056118130683899,
          0.9054402112960815,
          0.9054402112960815,
          0.9061266779899597,
          0.9066414833068848,
          0.9056118130683899,
          0.9062982797622681,
          0.9047537446022034,
          0.9061266779899597,
          0.9068130850791931,
          0.9052685499191284,
          0.9044104814529419,
          0.9064698815345764,
          0.9064698815345764,
          0.9049253463745117,
          0.9042388796806335,
          0.9064698815345764,
          0.9044104814529419,
          0.9059550166130066,
          0.9066414833068848,
          0.9056118130683899,
          0.9061266779899597,
          0.9038956761360168,
          0.9064698815345764,
          0.9064698815345764,
          0.9056118130683899,
          0.9052685499191284,
          0.9061266779899597,
          0.9061266779899597,
          0.9052685499191284,
          0.9054402112960815,
          0.9052685499191284,
          0.9054402112960815,
          0.9061266779899597,
          0.9054402112960815,
          0.9056118130683899,
          0.9050969481468201,
          0.9050969481468201,
          0.9052685499191284,
          0.9059550166130066,
          0.9061266779899597,
          0.9054402112960815,
          0.9050969481468201,
          0.9047537446022034,
          0.9059550166130066,
          0.9056118130683899,
          0.9042388796806335,
          0.9059550166130066,
          0.9047537446022034,
          0.9059550166130066,
          0.9050969481468201,
          0.9066414833068848,
          0.9061266779899597,
          0.9028659462928772,
          0.9054402112960815,
          0.9054402112960815,
          0.904582142829895,
          0.9049253463745117,
          0.9056118130683899,
          0.9042388796806335,
          0.9064698815345764,
          0.9052685499191284,
          0.9069847464561462,
          0.9057834148406982,
          0.9061266779899597,
          0.9062982797622681,
          0.9042388796806335,
          0.9062982797622681,
          0.9040672779083252,
          0.9054402112960815,
          0.9047537446022034,
          0.9050969481468201,
          0.9047537446022034,
          0.9054402112960815,
          0.9049253463745117,
          0.9056118130683899,
          0.9052685499191284,
          0.9059550166130066,
          0.904582142829895,
          0.9044104814529419,
          0.9064698815345764,
          0.9057834148406982,
          0.9050969481468201,
          0.9044104814529419,
          0.9059550166130066,
          0.9038956761360168,
          0.9050969481468201,
          0.9066414833068848,
          0.9056118130683899,
          0.9068130850791931,
          0.9057834148406982,
          0.9040672779083252,
          0.9064698815345764,
          0.9056118130683899,
          0.9064698815345764,
          0.9069847464561462,
          0.9049253463745117,
          0.9057834148406982,
          0.9054402112960815,
          0.9049253463745117,
          0.9047537446022034,
          0.9059550166130066,
          0.9054402112960815,
          0.9057834148406982,
          0.9062982797622681,
          0.9059550166130066,
          0.9057834148406982,
          0.9044104814529419,
          0.9050969481468201,
          0.9050969481468201,
          0.9035524129867554,
          0.9057834148406982,
          0.9061266779899597,
          0.9052685499191284,
          0.9061266779899597,
          0.9047537446022034,
          0.9049253463745117,
          0.9062982797622681,
          0.9062982797622681,
          0.9062982797622681,
          0.9042388796806335,
          0.9050969481468201,
          0.9061266779899597,
          0.9052685499191284,
          0.9050969481468201,
          0.9071563482284546,
          0.9050969481468201,
          0.9054402112960815,
          0.9047537446022034,
          0.9049253463745117,
          0.9050969481468201,
          0.9050969481468201,
          0.9054402112960815,
          0.9054402112960815,
          0.9061266779899597,
          0.9056118130683899,
          0.9054402112960815,
          0.9054402112960815,
          0.9059550166130066,
          0.9061266779899597,
          0.9052685499191284,
          0.9052685499191284,
          0.9044104814529419,
          0.9068130850791931,
          0.9042388796806335,
          0.9026943445205688,
          0.9052685499191284,
          0.9056118130683899,
          0.9059550166130066,
          0.9044104814529419,
          0.904582142829895,
          0.9054402112960815,
          0.9052685499191284,
          0.903380811214447,
          0.9073279500007629,
          0.9064698815345764,
          0.9066414833068848,
          0.9059550166130066,
          0.9020078778266907,
          0.9064698815345764,
          0.9052685499191284,
          0.904582142829895,
          0.9064698815345764,
          0.9052685499191284,
          0.9061266779899597,
          0.9059550166130066,
          0.9050969481468201,
          0.903380811214447,
          0.9054402112960815,
          0.9047537446022034,
          0.9049253463745117,
          0.9066414833068848,
          0.9044104814529419,
          0.9049253463745117,
          0.9052685499191284,
          0.9056118130683899,
          0.9054402112960815,
          0.9064698815345764,
          0.9049253463745117,
          0.9052685499191284,
          0.9037240147590637,
          0.9059550166130066,
          0.9057834148406982,
          0.9056118130683899,
          0.9023511409759521,
          0.9061266779899597,
          0.9056118130683899,
          0.9054402112960815,
          0.9062982797622681,
          0.9054402112960815,
          0.9044104814529419,
          0.9050969481468201,
          0.9056118130683899,
          0.9050969481468201,
          0.9052685499191284,
          0.9037240147590637,
          0.9049253463745117,
          0.9062982797622681,
          0.9068130850791931,
          0.9040672779083252,
          0.9054402112960815,
          0.9040672779083252,
          0.9038956761360168,
          0.9028659462928772,
          0.904582142829895,
          0.9044104814529419,
          0.9050969481468201,
          0.9049253463745117,
          0.9037240147590637,
          0.9037240147590637,
          0.9054402112960815,
          0.9049253463745117,
          0.9054402112960815,
          0.904582142829895,
          0.9040672779083252,
          0.9052685499191284,
          0.9059550166130066,
          0.9035524129867554,
          0.9050969481468201,
          0.9042388796806335,
          0.9047537446022034,
          0.9050969481468201,
          0.9054402112960815,
          0.904582142829895,
          0.9059550166130066,
          0.9050969481468201,
          0.9050969481468201,
          0.904582142829895,
          0.9047537446022034,
          0.9054402112960815,
          0.9038956761360168,
          0.9038956761360168,
          0.9042388796806335,
          0.903380811214447,
          0.9062982797622681,
          0.9054402112960815,
          0.904582142829895,
          0.9047537446022034,
          0.9038956761360168,
          0.9047537446022034,
          0.9047537446022034,
          0.9056118130683899,
          0.9050969481468201,
          0.9059550166130066
         ],
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "bgcolor": "rgb(17,17,17)",
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "rgb(17,17,17)",
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "subunitcolor": "#506784",
           "showland": true,
           "showlakes": true,
           "lakecolor": "rgb(17,17,17)"
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "borderwidth": 1,
           "bordercolor": "rgb(17,17,17)",
           "tickwidth": 0
          },
          "mapbox": {
           "style": "dark"
          }
         }
        },
        "title": {
         "text": "Training Accuracy"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"31422381-e0e7-4789-8e92-e9194ea4e751\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"31422381-e0e7-4789-8e92-e9194ea4e751\")) {                    Plotly.newPlot(                        \"31422381-e0e7-4789-8e92-e9194ea4e751\",                        [{\"mode\":\"lines\",\"name\":\"Train Accuracy\",\"y\":[0.6261234879493713,0.8528341054916382,0.8771961331367493,0.8825708031654358,0.8861297965049744,0.888306736946106,0.8900952935218811,0.8916128277778625,0.893085241317749,0.8936362266540527,0.894467294216156,0.8956054449081421,0.8961926102638245,0.8968248963356018,0.8971139788627625,0.8982791900634766,0.8984056711196899,0.8988754153251648,0.8991915583610535,0.8995077013969421,0.8998780250549316,0.9003387093544006,0.9011065363883972,0.9012420177459717,0.9011426568031311,0.9016937017440796,0.9015672206878662,0.9021814465522766,0.9025518298149109,0.9025879502296448,0.9030938148498535,0.9029853940010071,0.9033015966415405,0.9033828377723694,0.9036177396774292,0.9037261009216309,0.9043042063713074,0.9040874242782593,0.9040332436561584,0.9041867852210999,0.9045752286911011,0.9051894545555115,0.9050178527832031,0.9050991535186768,0.9053159356117249,0.9057585597038269,0.905722439289093,0.9058127403259277,0.9056049585342407,0.9061018228530884,0.9063547253608704,0.9065715074539185,0.906670868396759,0.9067341089248657,0.9071134924888611,0.9069960713386536,0.9073212742805481,0.9070502519607544,0.9082335829734802,0.907818078994751,0.9078632593154907,0.907818078994751,0.9084594249725342,0.9082245826721191,0.9087665677070618,0.9086400866508484,0.9090465903282166,0.9089381694793701,0.908739447593689,0.909182071685791,0.9097511172294617,0.9097872972488403,0.9099227786064148,0.9096156358718872,0.9101576209068298,0.9100040793418884,0.9105280041694641,0.9102028012275696,0.9102118015289307,0.9107628464698792,0.9104376435279846,0.9112687110900879,0.910356342792511,0.9111874103546143,0.9112957715988159,0.9116751551628113,0.9113318920135498,0.9113951325416565,0.9122171401977539,0.911738395690918,0.9121358394622803,0.9120455384254456,0.9119913578033447,0.9121177792549133,0.9122262001037598,0.9124429821968079,0.9124791026115417,0.9125242829322815,0.9127410650253296,0.9129307866096497,0.9130843281745911,0.9125514030456543,0.9130843281745911,0.9134456515312195,0.9129217267036438,0.9132830500602722,0.9135088920593262,0.9134275913238525,0.9134817719459534,0.9137256741523743,0.9137076139450073,0.913581132888794,0.9139515161514282,0.9139785766601562,0.9144122004508972,0.9147283434867859,0.914258599281311,0.9141231179237366,0.9145205616950989,0.9145386219024658,0.9141050577163696,0.9146379828453064,0.9142676591873169,0.9145747423171997,0.9144302606582642,0.9151167273521423,0.9147554039955139,0.91471928358078,0.9149541854858398,0.9148457646369934,0.9150716066360474,0.9153064489364624,0.9151709675788879,0.9152160882949829,0.9155051708221436,0.9150716066360474,0.9149541854858398,0.9152251482009888,0.9152973890304565,0.9156225919723511,0.9157580733299255,0.9159026145935059,0.9156948924064636,0.9155774116516113,0.9158393740653992,0.9159206748008728,0.9159929752349854,0.9159567952156067,0.9157129526138306,0.9157038927078247,0.916327178478241,0.9160652160644531,0.9162097573280334,0.9163181185722351,0.9160471558570862,0.9160110354423523,0.9161465167999268,0.9163000583648682,0.9163813591003418,0.9163452386856079,0.9163000583648682,0.9165439605712891,0.9164717197418213,0.9166704416275024,0.9167336821556091,0.9164987802505493,0.9171401262283325,0.9165891408920288,0.9169143438339233,0.9170950055122375,0.9171311259269714,0.917248547077179,0.9170769453048706,0.9169775247573853,0.9173027276992798,0.9171853065490723,0.9170588254928589,0.9173117876052856,0.9175646901130676,0.9173659682273865,0.9173027276992798,0.9172846674919128,0.9178537726402283,0.9179350733757019,0.9180705547332764,0.9179711937904358,0.9181337952613831,0.9179531335830688,0.9181247353553772,0.9180614948272705,0.9180976748466492,0.9184318780899048,0.9180253744125366,0.9184951186180115,0.9184679985046387,0.918377697467804,0.9185041189193726,0.9180705547332764,0.9185402393341064,0.9185583591461182,0.9185492992401123,0.9186486601829529,0.9189106225967407,0.9193441867828369,0.9183505773544312,0.9185854196548462,0.9191274046897888,0.9194164872169495,0.9186577200889587,0.9190822243690491,0.9191093444824219,0.9187932014465332,0.9190371036529541,0.9190009236335754,0.9190009236335754,0.9192267656326294,0.9192628860473633,0.9192809462547302,0.9192267656326294,0.9189919233322144,0.9192177653312683,0.9195700287818909,0.9193171262741089,0.919786810874939,0.9196965098381042,0.919533908367157,0.9198591113090515,0.9193351864814758,0.9200307130813599,0.9196332693099976,0.9198861718177795,0.919994592666626,0.9198861718177795,0.9198952317237854,0.9201210141181946,0.9202204346656799,0.9201752543449402,0.9200578331947327,0.9202384948730469,0.920482337474823,0.9201661944389343,0.9205636382102966,0.9203016757965088,0.9201120138168335,0.9201481342315674,0.9203920364379883,0.9203739762306213,0.920897901058197,0.920437216758728,0.9206359386444092,0.9207804799079895,0.9205816984176636,0.9203829765319824,0.9206088185310364,0.9211327433586121,0.9210966229438782,0.9204462170600891,0.9213947057723999,0.9208256006240845,0.9205726981163025,0.9210605025291443,0.9208436608314514,0.9210514426231384,0.9211417436599731,0.9210514426231384,0.9215030670166016,0.9208075404167175,0.9216927886009216,0.9210243225097656,0.9213405251502991,0.921358585357666,0.9211146831512451,0.9216024279594421,0.9214488863945007,0.921385645866394,0.921376645565033,0.9215843677520752,0.9214940667152405,0.92186439037323,0.9217650294303894,0.9221805930137634,0.9216566681861877,0.9217379689216614,0.9220992922782898,0.9221263527870178,0.9218734502792358,0.9220450520515442,0.9217289090156555,0.9220360517501831,0.921846330165863,0.9219547510147095,0.9220179915428162,0.9217921495437622,0.922298014163971,0.9221354126930237,0.9219908714294434,0.9224244356155396,0.9222708940505981,0.9225509166717529,0.9222076535224915,0.9224154353141785,0.9225689768791199,0.9228128790855408,0.9226502776145935,0.9226322174072266,0.9226412773132324,0.9228309392929077,0.9228128790855408,0.9227135181427002,0.922767698764801,0.9228670597076416,0.9230387210845947,0.9231290221214294,0.9230929017066956,0.9232103228569031,0.9231019616127014,0.9231561422348022,0.9231019616127014,0.9229303002357483,0.9228851199150085,0.9234451651573181,0.9228038191795349,0.9235987663269043,0.9232645034790039,0.9227586984634399,0.9235807061195374,0.9234090447425842,0.9232103228569031,0.9231561422348022,0.9231742024421692,0.9234451651573181,0.9233548641204834,0.923454225063324,0.9236710071563721,0.923436164855957,0.9234813451766968,0.9235716462135315,0.9236529469490051,0.9238877892494202,0.9234632849693298,0.9237433075904846,0.9236800670623779,0.9240503907203674,0.9237523078918457,0.9238246083259583,0.9237613677978516,0.9235445261001587,0.9243394732475281,0.9239690899848938,0.9241588115692139,0.9238246083259583,0.9239781498908997,0.9242581725120544,0.923896849155426,0.924149751663208,0.9241768717765808,0.9240323305130005,0.9244207739830017,0.9242039918899536,0.9243304133415222,0.9241858720779419,0.9244478344917297,0.9243033528327942,0.924149751663208,0.924375593662262,0.9237251877784729,0.9247910976409912,0.9248091578483582,0.9245291352272034,0.9242852330207825,0.9244117140769958,0.9249175786972046,0.9247459173202515,0.9250801801681519,0.9241949319839478,0.9245110750198364,0.9250621199607849,0.9248995184898376,0.9247820973396301,0.9250891804695129,0.9248633980751038,0.9248272180557251,0.925053060054779,0.9252969622612,0.9244478344917297,0.925071120262146,0.9252698421478271,0.9253602027893066,0.9249898195266724,0.9250891804695129,0.9251524209976196,0.9253330826759338,0.9254866242408752,0.9253782629966736,0.9248995184898376,0.9252156615257263,0.925278902053833,0.9255860447883606,0.9251434206962585,0.9256311655044556,0.9252969622612,0.9259202480316162,0.9256944060325623,0.9254956841468811,0.925278902053833,0.9255498647689819,0.9254415035247803,0.9257847666740417,0.9258028268814087,0.9261731505393982,0.9255679249763489,0.9258660674095154,0.9254324436187744,0.9258028268814087,0.9260105490684509,0.9259202480316162,0.925757646560669,0.9256402254104614,0.9260918498039246,0.9257485866546631,0.9261099100112915,0.9261279702186584,0.9258660674095154,0.9257305264472961,0.926182210445404,0.9259563684463501,0.9256492257118225,0.925974428653717,0.9257034659385681,0.9263357520103455,0.9270312786102295,0.9266338348388672,0.9265435338020325,0.9263086318969727,0.9262544512748718,0.926453173160553,0.9266157746315002,0.9266699552536011,0.9264622330665588,0.9268506169319153,0.9264983534812927,0.9268686771392822,0.9262183308601379,0.926435112953186,0.9266157746315002,0.9268416166305542,0.9266338348388672,0.9268416166305542,0.9269229173660278,0.926913857460022,0.9268416166305542,0.9269590377807617,0.9272210001945496,0.9271125793457031,0.9268596768379211,0.9272210001945496,0.9270674586296082,0.9274829626083374,0.927356481552124,0.9271125793457031,0.927103579044342,0.9275190830230713,0.9275913238525391,0.9274377822875977,0.9272932410240173,0.9271577596664429,0.9272119402885437,0.927582323551178,0.9273655414581299,0.9271577596664429,0.9276275038719177,0.9271848797798157,0.9274287223815918,0.9277539253234863,0.927564263343811,0.9277629852294922,0.927564263343811,0.9276184439659119,0.9272119402885437,0.9275462031364441,0.9276275038719177,0.927374541759491,0.9279255867004395,0.9274829626083374,0.92783522605896,0.9279255867004395,0.9280520081520081,0.9277629852294922,0.9282146096229553,0.9279255867004395,0.9277268648147583,0.9277719855308533,0.9277539253234863,0.9281513690948486,0.9284404516220093,0.9281784892082214,0.9278894066810608,0.9283952713012695,0.9279165267944336,0.9285759329795837,0.9280971884727478,0.928295910358429,0.9286933541297913,0.9286211133003235,0.9287475943565369,0.9283139705657959,0.9285669326782227,0.928512692451477,0.9284133315086365,0.9287114143371582,0.9286482334136963,0.9287927150726318,0.928738534450531,0.9284585118293762,0.9287927150726318,0.9289372563362122,0.9291630983352661,0.9287837147712708,0.9290546774864197,0.928991436958313,0.9286030530929565,0.9289191961288452,0.9287837147712708,0.9293798804283142,0.9288107752799988,0.9288559556007385,0.928964376449585,0.92919921875,0.9293527603149414,0.9293166399002075,0.9288198351860046,0.9291450381278992,0.9295424818992615,0.9293979406356812,0.9291811585426331,0.9289553165435791,0.9293979406356812,0.9294160008430481,0.9294431209564209,0.9292172789573669,0.9292353391647339,0.9292714595794678,0.9295424818992615,0.9296237826347351,0.9297321438789368,0.929659903049469,0.9298225045204163,0.9296147227287292,0.9298496246337891,0.929885745048523,0.9299038052558899,0.9296960234642029,0.9300031661987305,0.9301296472549438,0.9300844669342041,0.9297502636909485,0.9299579858779907,0.9301296472549438,0.9297050833702087,0.9299851059913635,0.9298225045204163,0.9300754070281982,0.9302831888198853,0.9302560687065125,0.9303193092346191,0.930355429649353,0.9301838278770447,0.9301477074623108,0.9301928281784058,0.9305090308189392,0.9304006099700928,0.9306806325912476,0.9302651286125183,0.9307619333267212,0.9308974146842957,0.9309606552124023,0.9304728507995605,0.930834174156189,0.9303825497627258,0.9305903315544128,0.9307799935340881,0.9310510158538818,0.9305090308189392,0.9304999709129333,0.9309697151184082,0.9305903315544128,0.9309606552124023,0.9306354522705078,0.9309245347976685,0.9307528734207153,0.9306716322898865,0.9312406778335571,0.9309967756271362,0.9306535124778748,0.9309877753257751,0.9311051964759827,0.9309787154197693,0.9314484596252441,0.9311684370040894,0.9315478205680847,0.9314665198326111,0.9315117001533508,0.9315297603607178,0.9313310384750366,0.9314574599266052,0.9317826628684998,0.9318459033966064,0.9314665198326111,0.9317194223403931,0.9315748810768127,0.9316200613975525,0.9310871362686157,0.9315478205680847,0.9316020011901855,0.9316110610961914,0.9315207004547119,0.9318097829818726,0.9316652417182922,0.9318459033966064,0.9319000840187073,0.9319813847541809,0.9319723844528198,0.9319272041320801,0.9315478205680847,0.9317646026611328,0.9322704672813416,0.9318910837173462,0.9320988059043884,0.931945264339447,0.9324691891670227,0.9317465424537659,0.932198166847229,0.9323065876960754,0.9321439862251282,0.93263179063797,0.9321620464324951,0.9323698282241821,0.9321349263191223,0.9325143694877625,0.9321439862251282,0.9323155879974365,0.9323155879974365,0.9325414299964905,0.9326498508453369,0.932216227054596,0.9325233697891235,0.932405948638916,0.9327492117881775,0.9325143694877625,0.932884693145752,0.9328033924102783,0.933074414730072,0.9328395128250122,0.932424008846283,0.9331375956535339,0.9328033924102783,0.9326227307319641,0.9329840540885925,0.9327220916748047,0.9330562949180603,0.9329298734664917,0.9326949715614319,0.932866632938385,0.9329840540885925,0.9330472946166992,0.933562159538269,0.9332369565963745,0.9332640767097473,0.9334537982940674,0.9332008361816406,0.933092474937439,0.932866632938385,0.933535099029541,0.9334899187088013,0.9335260391235352,0.9331466555595398,0.9336163401603699,0.9333544373512268,0.9334808588027954,0.933562159538269,0.9333092570304871,0.9338060617446899,0.9338060617446899,0.9335079789161682,0.9331466555595398,0.9334899187088013,0.9336525201797485,0.9334537982940674,0.9339234828948975,0.9339234828948975,0.9339054226875305,0.933788001537323,0.9341673851013184,0.9340860843658447,0.934022843837738,0.9338963627815247,0.9344474077224731,0.9340680241584778,0.9338512420654297,0.9336344599723816,0.9339325428009033,0.934013843536377,0.934483528137207,0.9340951442718506,0.9342125654220581,0.9343209266662598,0.9344202876091003,0.9343209266662598,0.9340860843658447,0.9344022274017334,0.9345106482505798,0.934013843536377,0.9343661069869995,0.9345828890800476,0.9343932271003723,0.9344202876091003,0.9345648288726807,0.9346822500228882,0.9346822500228882,0.9349170923233032,0.9350706934928894,0.9346190094947815,0.9346551895141602,0.9347454905509949,0.9347545504570007,0.9348900318145752,0.9347454905509949,0.9349983930587769,0.9346732497215271,0.9349352121353149,0.9349983930587769,0.9352332949638367,0.9348267912864685,0.9349893927574158,0.9355133175849915,0.9351881146430969,0.9354500770568848,0.935404896736145,0.9350074529647827,0.9347454905509949,0.9350526332855225,0.9352874755859375,0.9352152347564697,0.9353235960006714,0.9351971745491028,0.9351429343223572,0.9356759190559387,0.9353777766227722,0.9356216788291931,0.9354952573776245,0.9355133175849915,0.9352061748504639,0.9352965354919434,0.9356939792633057,0.9353958964347839,0.935612678527832,0.9359559416770935,0.9357390999794006,0.9360553026199341,0.9355403780937195,0.9358294606208801,0.9361456036567688,0.9356668591499329,0.9359197616577148,0.9360553026199341,0.9359559416770935,0.9357481598854065,0.9358023405075073,0.9358746409416199,0.9360553026199341,0.9360823631286621,0.9363172650337219,0.9361275434494019,0.9360823631286621,0.936118483543396,0.9358927011489868,0.9366153478622437,0.936579167842865,0.9361366033554077,0.9358023405075073,0.936100423336029,0.9358565807342529,0.9359197616577148,0.9364979267120361,0.9366153478622437,0.9362810850143433,0.936073362827301,0.9366514682769775,0.9362179040908813,0.9367327690124512,0.9371392726898193,0.936786949634552,0.9363353252410889,0.9363353252410889,0.9362810850143433,0.9367598295211792,0.9368321299552917,0.936579167842865,0.9362720847129822,0.9371934533119202,0.9363985657691956,0.9368230700492859,0.9373469948768616,0.9368773102760315,0.9362991452217102,0.9368682503700256,0.9371573328971863,0.9371302127838135,0.9369856715202332,0.9365701675415039,0.9368321299552917,0.9371030926704407,0.9370669722557068,0.937247633934021,0.9374373555183411,0.9370850324630737,0.9371753931045532,0.9375908970832825,0.9373469948768616,0.9373650550842285,0.9374915361404419,0.9373650550842285,0.9375637769699097,0.9375547766685486,0.937482476234436,0.9375818371772766,0.9378618597984314,0.9378076791763306,0.937961220741272,0.9378437995910645,0.9375367164611816,0.9374192953109741,0.9377805590629578,0.9376270174980164,0.9375276565551758,0.9381780624389648,0.9378167390823364,0.9375728368759155,0.9374102354049683,0.938150942325592,0.9379974007606506,0.9381780624389648,0.9381870627403259,0.9377444386482239,0.9376811981201172,0.9376180171966553,0.9383135437965393,0.937916100025177,0.9382774233818054,0.938169002532959,0.9381057620048523,0.9380064010620117,0.9382412433624268,0.9384400248527527,0.9384580850601196,0.9380606412887573,0.9378889799118042,0.9383316040039062,0.9383857846260071,0.9384942054748535,0.9381870627403259,0.9382412433624268,0.9388013482093811,0.9383316040039062,0.9385303258895874,0.9382593631744385,0.9380064010620117,0.9388284087181091,0.9382954835891724,0.9392349123954773,0.93888258934021,0.938611626625061,0.9385122656822205,0.9387109875679016,0.9382593631744385,0.9389910101890564,0.9389007091522217,0.9386929273605347,0.9389819502830505,0.9390632510185242,0.9391897320747375,0.9389187693595886,0.9389819502830505,0.9393613934516907,0.9387651681900024,0.9391626119613647,0.9396684765815735,0.9397045969963074,0.9387742280960083,0.9394155740737915,0.9392619729042053,0.9392529726028442,0.9389548897743225,0.9399304389953613,0.9389458298683167,0.9391174912452698,0.9397136569023132,0.9390181303024292,0.9393703937530518,0.9391355514526367,0.9395058751106262,0.9396414160728455,0.9393884539604187,0.939758837223053,0.9396504163742065,0.9393794536590576,0.9392619729042053,0.9396955966949463,0.9395420551300049,0.9396955966949463,0.9396775364875793,0.9400388598442078,0.9400297999382019,0.9401924014091492,0.9397949576377869,0.9397497773170471,0.939325213432312,0.9401382207870483,0.9396504163742065,0.939758837223053,0.9397678375244141,0.9400930404663086,0.9398401379585266,0.9402646422386169,0.9401562809944153,0.9401021003723145,0.9403369426727295,0.9398310780525208,0.9401021003723145,0.9405356645584106,0.9401021003723145,0.940219521522522,0.9407795667648315,0.9400388598442078,0.9405266046524048,0.9402556419372559,0.9404633641242981,0.9402917623519897,0.9408698678016663,0.9403730630874634,0.9403730630874634,0.9405176043510437,0.9404995441436768,0.9410866498947144,0.9405898451805115,0.9405085444450378,0.9411770105361938,0.9408608675003052,0.9408879280090332,0.9407343864440918,0.9408428072929382,0.9408608675003052,0.9405627846717834,0.9410324692726135,0.9408337473869324,0.9410957098007202,0.9409782886505127,0.9406350255012512,0.9407253265380859,0.9406530857086182,0.9411589503288269,0.9411589503288269,0.9410866498947144,0.9412583112716675,0.9411318302154541,0.9410505294799805,0.9412492513656616,0.9413396120071411,0.9415112137794495,0.9409963488578796,0.9417551159858704,0.9413847327232361,0.9415925145149231,0.9411137700080872,0.9412944316864014,0.9415112137794495,0.9412673115730286,0.9413396120071411,0.9415112137794495,0.941628634929657,0.9416738152503967,0.9415653944015503,0.9417189955711365,0.9415563941001892,0.9415292739868164,0.9417731761932373,0.941836416721344,0.9418815970420837,0.9416557550430298,0.941818356513977,0.942062258720398,0.9414209127426147,0.9417822360992432,0.9421254396438599,0.9420170783996582,0.9416738152503967,0.9417641758918762,0.9418002963066101,0.9421616196632385,0.9419177174568176],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Accuracy\",\"y\":[0.8108803629875183,0.8647674322128296,0.8735198378562927,0.8772953748703003,0.8808992505073547,0.8803844451904297,0.884503185749054,0.8855328559875488,0.887420654296875,0.8887935280799866,0.8881070613861084,0.8882787227630615,0.8894799947738647,0.8896515965461731,0.8903380632400513,0.8901664614677429,0.8906813263893127,0.8906813263893127,0.8901664614677429,0.8903380632400513,0.8905097246170044,0.8905097246170044,0.8913677930831909,0.8901664614677429,0.8910245299339294,0.8920542597770691,0.8915393948554993,0.8911961317062378,0.8915393948554993,0.8927406668663025,0.8923974633216858,0.8920542597770691,0.8925690650939941,0.8939419984817505,0.8937703967094421,0.8927406668663025,0.8923974633216858,0.8925690650939941,0.8922258615493774,0.8949716687202454,0.8951432704925537,0.893598735332489,0.8944568634033203,0.8944568634033203,0.8939419984817505,0.893598735332489,0.8922258615493774,0.8925690650939941,0.8956581354141235,0.8941136002540588,0.8951432704925537,0.8939419984817505,0.8927406668663025,0.8941136002540588,0.8951432704925537,0.8961730003356934,0.896001398563385,0.8951432704925537,0.8937703967094421,0.8956581354141235,0.894800066947937,0.894800066947937,0.8963446021080017,0.8939419984817505,0.8941136002540588,0.8963446021080017,0.8944568634033203,0.8954865336418152,0.8966878056526184,0.8963446021080017,0.8965162038803101,0.8958297371864319,0.8972026705741882,0.8925690650939941,0.8956581354141235,0.8939419984817505,0.8972026705741882,0.8970310688018799,0.8972026705741882,0.896001398563385,0.8961730003356934,0.8961730003356934,0.8970310688018799,0.8972026705741882,0.8968594670295715,0.896001398563385,0.8975459337234497,0.8980607390403748,0.8968594670295715,0.8951432704925537,0.8977175354957581,0.8980607390403748,0.8970310688018799,0.8977175354957581,0.8982323408126831,0.8966878056526184,0.8963446021080017,0.8978891372680664,0.8980607390403748,0.8982323408126831,0.8985756039619446,0.8982323408126831,0.8989188075065613,0.8989188075065613,0.8985756039619446,0.8973742723464966,0.8996052742004395,0.8990904688835144,0.8990904688835144,0.8985756039619446,0.8978891372680664,0.8978891372680664,0.8980607390403748,0.8982323408126831,0.8999485373497009,0.8989188075065613,0.8982323408126831,0.8994336724281311,0.8985756039619446,0.8987472057342529,0.8992620706558228,0.8994336724281311,0.8989188075065613,0.8990904688835144,0.9001201391220093,0.8985756039619446,0.8994336724281311,0.900463342666626,0.9006350040435791,0.8985756039619446,0.8985756039619446,0.8985756039619446,0.8992620706558228,0.9001201391220093,0.8994336724281311,0.8994336724281311,0.8985756039619446,0.8994336724281311,0.8997768759727478,0.8996052742004395,0.8994336724281311,0.9006350040435791,0.8989188075065613,0.8996052742004395,0.8992620706558228,0.9006350040435791,0.9006350040435791,0.8987472057342529,0.9002917408943176,0.900463342666626,0.9009782075881958,0.9014930725097656,0.8994336724281311,0.9006350040435791,0.8984040021896362,0.8999485373497009,0.9013214111328125,0.8994336724281311,0.9006350040435791,0.9006350040435791,0.9013214111328125,0.9001201391220093,0.9013214111328125,0.8997768759727478,0.9009782075881958,0.9006350040435791,0.8997768759727478,0.8996052742004395,0.8996052742004395,0.8987472057342529,0.8999485373497009,0.8997768759727478,0.8996052742004395,0.8999485373497009,0.9011498093605042,0.9013214111328125,0.8987472057342529,0.8996052742004395,0.9011498093605042,0.9009782075881958,0.900463342666626,0.900463342666626,0.901664674282074,0.9001201391220093,0.8990904688835144,0.900463342666626,0.9001201391220093,0.9006350040435791,0.9001201391220093,0.900463342666626,0.9014930725097656,0.9009782075881958,0.9001201391220093,0.900463342666626,0.9001201391220093,0.9002917408943176,0.900463342666626,0.9002917408943176,0.9006350040435791,0.900463342666626,0.9006350040435791,0.9014930725097656,0.9011498093605042,0.9014930725097656,0.9006350040435791,0.9002917408943176,0.9014930725097656,0.9011498093605042,0.9008066058158875,0.9011498093605042,0.9020078778266907,0.9014930725097656,0.9014930725097656,0.8982323408126831,0.9006350040435791,0.9001201391220093,0.9006350040435791,0.8997768759727478,0.9018362760543823,0.9011498093605042,0.9011498093605042,0.901664674282074,0.901664674282074,0.9013214111328125,0.9009782075881958,0.9025227427482605,0.902179479598999,0.9035524129867554,0.9026943445205688,0.902179479598999,0.9011498093605042,0.9018362760543823,0.9025227427482605,0.9026943445205688,0.9011498093605042,0.9026943445205688,0.9001201391220093,0.9025227427482605,0.9028659462928772,0.9026943445205688,0.9023511409759521,0.9030376076698303,0.9018362760543823,0.9001201391220093,0.9020078778266907,0.902179479598999,0.9020078778266907,0.8999485373497009,0.9020078778266907,0.902179479598999,0.9028659462928772,0.9014930725097656,0.9023511409759521,0.9026943445205688,0.9013214111328125,0.902179479598999,0.9009782075881958,0.903380811214447,0.9023511409759521,0.9013214111328125,0.8996052742004395,0.9008066058158875,0.9014930725097656,0.9006350040435791,0.902179479598999,0.9025227427482605,0.9023511409759521,0.900463342666626,0.9023511409759521,0.9025227427482605,0.9023511409759521,0.9009782075881958,0.9011498093605042,0.9030376076698303,0.9020078778266907,0.9030376076698303,0.9030376076698303,0.902179479598999,0.9035524129867554,0.9025227427482605,0.9026943445205688,0.901664674282074,0.901664674282074,0.9020078778266907,0.901664674282074,0.901664674282074,0.9030376076698303,0.9026943445205688,0.9018362760543823,0.9020078778266907,0.9018362760543823,0.9008066058158875,0.9023511409759521,0.9014930725097656,0.9013214111328125,0.9020078778266907,0.901664674282074,0.9018362760543823,0.902179479598999,0.9028659462928772,0.902179479598999,0.9025227427482605,0.900463342666626,0.9025227427482605,0.902179479598999,0.9026943445205688,0.9032092094421387,0.902179479598999,0.902179479598999,0.9023511409759521,0.9037240147590637,0.9030376076698303,0.9020078778266907,0.902179479598999,0.9030376076698303,0.9028659462928772,0.8987472057342529,0.9044104814529419,0.9023511409759521,0.9018362760543823,0.9018362760543823,0.9006350040435791,0.9025227427482605,0.9018362760543823,0.9035524129867554,0.9030376076698303,0.9032092094421387,0.9028659462928772,0.9032092094421387,0.9025227427482605,0.9028659462928772,0.9040672779083252,0.9026943445205688,0.9023511409759521,0.9042388796806335,0.9035524129867554,0.9037240147590637,0.9023511409759521,0.9026943445205688,0.9037240147590637,0.9028659462928772,0.904582142829895,0.9030376076698303,0.9032092094421387,0.9013214111328125,0.9023511409759521,0.9009782075881958,0.9028659462928772,0.9037240147590637,0.904582142829895,0.903380811214447,0.903380811214447,0.9023511409759521,0.9030376076698303,0.9032092094421387,0.9040672779083252,0.9025227427482605,0.9018362760543823,0.9026943445205688,0.9032092094421387,0.903380811214447,0.904582142829895,0.9040672779083252,0.9028659462928772,0.9042388796806335,0.9047537446022034,0.9038956761360168,0.9030376076698303,0.9040672779083252,0.9044104814529419,0.9038956761360168,0.9028659462928772,0.9047537446022034,0.9038956761360168,0.904582142829895,0.9020078778266907,0.9049253463745117,0.9032092094421387,0.9040672779083252,0.9038956761360168,0.9035524129867554,0.9040672779083252,0.904582142829895,0.9042388796806335,0.904582142829895,0.9032092094421387,0.9030376076698303,0.9047537446022034,0.9038956761360168,0.904582142829895,0.904582142829895,0.9026943445205688,0.9038956761360168,0.9037240147590637,0.9047537446022034,0.9035524129867554,0.9040672779083252,0.9032092094421387,0.9042388796806335,0.9047537446022034,0.9044104814529419,0.9040672779083252,0.904582142829895,0.9049253463745117,0.9030376076698303,0.9047537446022034,0.9042388796806335,0.903380811214447,0.9035524129867554,0.9056118130683899,0.9044104814529419,0.9038956761360168,0.9040672779083252,0.9049253463745117,0.9042388796806335,0.9050969481468201,0.9054402112960815,0.9042388796806335,0.9020078778266907,0.9056118130683899,0.9014930725097656,0.9049253463745117,0.9057834148406982,0.9052685499191284,0.9056118130683899,0.9062982797622681,0.903380811214447,0.9049253463745117,0.9037240147590637,0.9050969481468201,0.904582142829895,0.9061266779899597,0.9044104814529419,0.9047537446022034,0.9025227427482605,0.9052685499191284,0.9050969481468201,0.9052685499191284,0.9040672779083252,0.9044104814529419,0.9044104814529419,0.9050969481468201,0.9050969481468201,0.9064698815345764,0.9052685499191284,0.9050969481468201,0.9057834148406982,0.9056118130683899,0.904582142829895,0.904582142829895,0.9047537446022034,0.9050969481468201,0.9062982797622681,0.9044104814529419,0.9056118130683899,0.9032092094421387,0.9052685499191284,0.9047537446022034,0.9062982797622681,0.904582142829895,0.9052685499191284,0.9059550166130066,0.9057834148406982,0.9049253463745117,0.9044104814529419,0.904582142829895,0.9042388796806335,0.9050969481468201,0.9044104814529419,0.9061266779899597,0.9047537446022034,0.9047537446022034,0.9030376076698303,0.9059550166130066,0.9064698815345764,0.9047537446022034,0.9050969481468201,0.9057834148406982,0.9044104814529419,0.9059550166130066,0.9057834148406982,0.9059550166130066,0.9047537446022034,0.9064698815345764,0.9049253463745117,0.9052685499191284,0.9049253463745117,0.9026943445205688,0.9054402112960815,0.9056118130683899,0.9044104814529419,0.9069847464561462,0.9050969481468201,0.904582142829895,0.9068130850791931,0.9035524129867554,0.9062982797622681,0.9059550166130066,0.9042388796806335,0.9052685499191284,0.9050969481468201,0.9059550166130066,0.9040672779083252,0.9014930725097656,0.9052685499191284,0.9042388796806335,0.9068130850791931,0.9069847464561462,0.9061266779899597,0.9064698815345764,0.9071563482284546,0.9049253463745117,0.9049253463745117,0.9057834148406982,0.9014930725097656,0.904582142829895,0.9042388796806335,0.9044104814529419,0.9038956761360168,0.9049253463745117,0.9062982797622681,0.9059550166130066,0.9050969481468201,0.9059550166130066,0.9037240147590637,0.904582142829895,0.9052685499191284,0.904582142829895,0.9050969481468201,0.9047537446022034,0.904582142829895,0.9047537446022034,0.9062982797622681,0.9044104814529419,0.9061266779899597,0.9062982797622681,0.9056118130683899,0.9061266779899597,0.9054402112960815,0.9062982797622681,0.9056118130683899,0.9059550166130066,0.9061266779899597,0.9061266779899597,0.9052685499191284,0.9057834148406982,0.9056118130683899,0.9052685499191284,0.904582142829895,0.9061266779899597,0.9059550166130066,0.9052685499191284,0.9047537446022034,0.9054402112960815,0.9049253463745117,0.9064698815345764,0.904582142829895,0.904582142829895,0.9059550166130066,0.9069847464561462,0.9059550166130066,0.9052685499191284,0.9069847464561462,0.9032092094421387,0.904582142829895,0.9038956761360168,0.9049253463745117,0.9057834148406982,0.9049253463745117,0.9047537446022034,0.9042388796806335,0.9054402112960815,0.9071563482284546,0.9040672779083252,0.9059550166130066,0.904582142829895,0.9071563482284546,0.9074995517730713,0.9050969481468201,0.9059550166130066,0.9038956761360168,0.9056118130683899,0.9059550166130066,0.9056118130683899,0.9042388796806335,0.9071563482284546,0.9057834148406982,0.9059550166130066,0.9056118130683899,0.903380811214447,0.9025227427482605,0.9064698815345764,0.9040672779083252,0.9068130850791931,0.9064698815345764,0.9047537446022034,0.9061266779899597,0.9061266779899597,0.9050969481468201,0.9061266779899597,0.9050969481468201,0.9074995517730713,0.9059550166130066,0.9078428149223328,0.9052685499191284,0.9047537446022034,0.904582142829895,0.9062982797622681,0.9056118130683899,0.9064698815345764,0.9050969481468201,0.9026943445205688,0.9047537446022034,0.9061266779899597,0.9074995517730713,0.903380811214447,0.9057834148406982,0.9076712131500244,0.9073279500007629,0.9052685499191284,0.9056118130683899,0.9056118130683899,0.9054402112960815,0.9037240147590637,0.904582142829895,0.9049253463745117,0.9047537446022034,0.9064698815345764,0.9061266779899597,0.904582142829895,0.9074995517730713,0.9078428149223328,0.9062982797622681,0.9047537446022034,0.9049253463745117,0.9057834148406982,0.9050969481468201,0.9042388796806335,0.9049253463745117,0.9054402112960815,0.9068130850791931,0.9059550166130066,0.9047537446022034,0.9073279500007629,0.9049253463745117,0.903380811214447,0.9044104814529419,0.9062982797622681,0.9071563482284546,0.9052685499191284,0.9068130850791931,0.904582142829895,0.9057834148406982,0.9052685499191284,0.9061266779899597,0.9047537446022034,0.9054402112960815,0.9057834148406982,0.9049253463745117,0.9044104814529419,0.9050969481468201,0.9064698815345764,0.9040672779083252,0.9056118130683899,0.9037240147590637,0.9066414833068848,0.9059550166130066,0.9049253463745117,0.9071563482284546,0.9074995517730713,0.9068130850791931,0.9042388796806335,0.9059550166130066,0.9057834148406982,0.9054402112960815,0.9056118130683899,0.9068130850791931,0.904582142829895,0.9059550166130066,0.9061266779899597,0.9056118130683899,0.9050969481468201,0.9061266779899597,0.9038956761360168,0.9068130850791931,0.9062982797622681,0.9073279500007629,0.9047537446022034,0.904582142829895,0.9057834148406982,0.9044104814529419,0.9056118130683899,0.9049253463745117,0.9044104814529419,0.9068130850791931,0.9059550166130066,0.9069847464561462,0.9050969481468201,0.9059550166130066,0.9047537446022034,0.9066414833068848,0.9037240147590637,0.9071563482284546,0.9052685499191284,0.9044104814529419,0.9054402112960815,0.9054402112960815,0.9049253463745117,0.9068130850791931,0.9049253463745117,0.9050969481468201,0.9064698815345764,0.9049253463745117,0.9068130850791931,0.9074995517730713,0.9052685499191284,0.9062982797622681,0.9054402112960815,0.9062982797622681,0.9056118130683899,0.9069847464561462,0.9061266779899597,0.9062982797622681,0.9042388796806335,0.9052685499191284,0.9059550166130066,0.904582142829895,0.9050969481468201,0.9047537446022034,0.9050969481468201,0.9059550166130066,0.9054402112960815,0.9059550166130066,0.904582142829895,0.9059550166130066,0.9044104814529419,0.9061266779899597,0.9059550166130066,0.9052685499191284,0.9054402112960815,0.9050969481468201,0.9056118130683899,0.9049253463745117,0.9049253463745117,0.9056118130683899,0.9061266779899597,0.9049253463745117,0.9047537446022034,0.9059550166130066,0.9069847464561462,0.9040672779083252,0.9054402112960815,0.9062982797622681,0.9076712131500244,0.9047537446022034,0.903380811214447,0.9056118130683899,0.9054402112960815,0.9054402112960815,0.9061266779899597,0.9066414833068848,0.9056118130683899,0.9062982797622681,0.9047537446022034,0.9061266779899597,0.9068130850791931,0.9052685499191284,0.9044104814529419,0.9064698815345764,0.9064698815345764,0.9049253463745117,0.9042388796806335,0.9064698815345764,0.9044104814529419,0.9059550166130066,0.9066414833068848,0.9056118130683899,0.9061266779899597,0.9038956761360168,0.9064698815345764,0.9064698815345764,0.9056118130683899,0.9052685499191284,0.9061266779899597,0.9061266779899597,0.9052685499191284,0.9054402112960815,0.9052685499191284,0.9054402112960815,0.9061266779899597,0.9054402112960815,0.9056118130683899,0.9050969481468201,0.9050969481468201,0.9052685499191284,0.9059550166130066,0.9061266779899597,0.9054402112960815,0.9050969481468201,0.9047537446022034,0.9059550166130066,0.9056118130683899,0.9042388796806335,0.9059550166130066,0.9047537446022034,0.9059550166130066,0.9050969481468201,0.9066414833068848,0.9061266779899597,0.9028659462928772,0.9054402112960815,0.9054402112960815,0.904582142829895,0.9049253463745117,0.9056118130683899,0.9042388796806335,0.9064698815345764,0.9052685499191284,0.9069847464561462,0.9057834148406982,0.9061266779899597,0.9062982797622681,0.9042388796806335,0.9062982797622681,0.9040672779083252,0.9054402112960815,0.9047537446022034,0.9050969481468201,0.9047537446022034,0.9054402112960815,0.9049253463745117,0.9056118130683899,0.9052685499191284,0.9059550166130066,0.904582142829895,0.9044104814529419,0.9064698815345764,0.9057834148406982,0.9050969481468201,0.9044104814529419,0.9059550166130066,0.9038956761360168,0.9050969481468201,0.9066414833068848,0.9056118130683899,0.9068130850791931,0.9057834148406982,0.9040672779083252,0.9064698815345764,0.9056118130683899,0.9064698815345764,0.9069847464561462,0.9049253463745117,0.9057834148406982,0.9054402112960815,0.9049253463745117,0.9047537446022034,0.9059550166130066,0.9054402112960815,0.9057834148406982,0.9062982797622681,0.9059550166130066,0.9057834148406982,0.9044104814529419,0.9050969481468201,0.9050969481468201,0.9035524129867554,0.9057834148406982,0.9061266779899597,0.9052685499191284,0.9061266779899597,0.9047537446022034,0.9049253463745117,0.9062982797622681,0.9062982797622681,0.9062982797622681,0.9042388796806335,0.9050969481468201,0.9061266779899597,0.9052685499191284,0.9050969481468201,0.9071563482284546,0.9050969481468201,0.9054402112960815,0.9047537446022034,0.9049253463745117,0.9050969481468201,0.9050969481468201,0.9054402112960815,0.9054402112960815,0.9061266779899597,0.9056118130683899,0.9054402112960815,0.9054402112960815,0.9059550166130066,0.9061266779899597,0.9052685499191284,0.9052685499191284,0.9044104814529419,0.9068130850791931,0.9042388796806335,0.9026943445205688,0.9052685499191284,0.9056118130683899,0.9059550166130066,0.9044104814529419,0.904582142829895,0.9054402112960815,0.9052685499191284,0.903380811214447,0.9073279500007629,0.9064698815345764,0.9066414833068848,0.9059550166130066,0.9020078778266907,0.9064698815345764,0.9052685499191284,0.904582142829895,0.9064698815345764,0.9052685499191284,0.9061266779899597,0.9059550166130066,0.9050969481468201,0.903380811214447,0.9054402112960815,0.9047537446022034,0.9049253463745117,0.9066414833068848,0.9044104814529419,0.9049253463745117,0.9052685499191284,0.9056118130683899,0.9054402112960815,0.9064698815345764,0.9049253463745117,0.9052685499191284,0.9037240147590637,0.9059550166130066,0.9057834148406982,0.9056118130683899,0.9023511409759521,0.9061266779899597,0.9056118130683899,0.9054402112960815,0.9062982797622681,0.9054402112960815,0.9044104814529419,0.9050969481468201,0.9056118130683899,0.9050969481468201,0.9052685499191284,0.9037240147590637,0.9049253463745117,0.9062982797622681,0.9068130850791931,0.9040672779083252,0.9054402112960815,0.9040672779083252,0.9038956761360168,0.9028659462928772,0.904582142829895,0.9044104814529419,0.9050969481468201,0.9049253463745117,0.9037240147590637,0.9037240147590637,0.9054402112960815,0.9049253463745117,0.9054402112960815,0.904582142829895,0.9040672779083252,0.9052685499191284,0.9059550166130066,0.9035524129867554,0.9050969481468201,0.9042388796806335,0.9047537446022034,0.9050969481468201,0.9054402112960815,0.904582142829895,0.9059550166130066,0.9050969481468201,0.9050969481468201,0.904582142829895,0.9047537446022034,0.9054402112960815,0.9038956761360168,0.9038956761360168,0.9042388796806335,0.903380811214447,0.9062982797622681,0.9054402112960815,0.904582142829895,0.9047537446022034,0.9038956761360168,0.9047537446022034,0.9047537446022034,0.9056118130683899,0.9050969481468201,0.9059550166130066],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"bgcolor\":\"rgb(17,17,17)\",\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"rgb(17,17,17)\",\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"subunitcolor\":\"#506784\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"rgb(17,17,17)\"},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"borderwidth\":1,\"bordercolor\":\"rgb(17,17,17)\",\"tickwidth\":0},\"mapbox\":{\"style\":\"dark\"}}},\"title\":{\"text\":\"Training Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('31422381-e0e7-4789-8e92-e9194ea4e751');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train Loss'))\n",
    "fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "fig.update_layout(title='Training Loss', xaxis_title='Epoch', yaxis_title='Loss')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history.history['categorical_accuracy'], mode='lines', name='Train Accuracy'))\n",
    "fig.add_trace(go.Scatter(y=history.history['val_categorical_accuracy'], mode='lines', name='Validation Accuracy'))\n",
    "fig.update_layout(title='Training Accuracy', xaxis_title='Epoch', yaxis_title='Accuracy')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T07:36:30.967269Z",
     "end_time": "2023-10-06T07:36:31.118913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=(<tf.Tensor 'IteratorGetNext:0' shape=(None, 1536) dtype=float32>,). Consider rewriting this model with the Functional API.\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "NNmodel_inf = tf.keras.Sequential([NNmodel, tf.keras.layers.Softmax()])\n",
    "validate = np.hstack((np.round(NNmodel_inf.predict([X_tensor_test[:100, :]]), 2), np.array(y_test[:100]).reshape((-1, 1))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T03:19:26.205602Z",
     "end_time": "2023-09-25T03:19:26.359666Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "validate_tree = np.hstack((clf.predict_proba(X_test[:100]), np.array(y_test[:100]).reshape((-1, 1))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T20:45:53.420930Z",
     "end_time": "2023-09-24T20:45:53.648299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 2ms/step\n",
      "ANN Accuracy: 0.9104170241977003\n",
      "RF Accuracy: 0.9028659687660889\n"
     ]
    }
   ],
   "source": [
    "print('ANN Accuracy: ' + str(np.count_nonzero(np.argmax(NNmodel_inf.predict([X_tensor_test]), axis=1) == y_test)/len(y_test)))\n",
    "print('RF Accuracy: ' + str(np.count_nonzero(clf.predict(X_test) == y_test)/len(y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T03:19:30.974935Z",
     "end_time": "2023-09-25T03:19:38.741049Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 0, ..., 2, 2, 2], dtype=int64)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T15:56:22.941326Z",
     "end_time": "2023-09-20T15:56:26.672464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
